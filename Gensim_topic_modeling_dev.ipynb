{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from os import listdir, makedirs\n",
    "from os.path import join, isfile, isdir, exists\n",
    "import pandas as pd\n",
    "import gc\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models import CoherenceModel, TfidfModel, LdaModel, LdaMulticore\n",
    "from gensim.models.hdpmodel import HdpModel, HdpTopicFormatter\n",
    "from gensim.models.callbacks import CoherenceMetric, DiffMetric, PerplexityMetric, ConvergenceMetric\n",
    "from itertools import chain, islice\n",
    "from constants import (\n",
    "    FULL_PATH, ETL_PATH, NLP_PATH, SMPL_PATH, POS, NOUN, PROPN, TOKEN, HASH, SENT_IDX, PUNCT\n",
    ")\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "#pd.options.display.max_rows = 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_to_lists(token_series):\n",
    "    return tuple(token_series.tolist())\n",
    "\n",
    "def docs2corpora(documents, tfidf=True, stopwords=None, filter_below=5, filter_above=0.5,\n",
    "                split=False, max_test_size_rel=0.1, max_test_size_abs=5000):\n",
    "    dictionary = Dictionary(documents)\n",
    "    dictionary.filter_extremes(no_below=filter_below, no_above=filter_above)\n",
    "\n",
    "    # filter some noice (e.g. special characters)\n",
    "    if stopwords:\n",
    "        stopword_ids = [dictionary.token2id[token] for token in stopwords]\n",
    "        dictionary.filter_tokens(bad_ids=stopword_ids, good_ids=None)\n",
    "        \n",
    "    corpora = dict()\n",
    "    if split:\n",
    "        if length*max_test_size_rel < max_test_size_abs:\n",
    "            split1 = int(length*(1-(2*max_test_size_rel)))\n",
    "            split2 = int(length*(1-max_test_size_rel))\n",
    "        else:\n",
    "            split1 = length-(2*max_test_size_abs)\n",
    "            split2 = length-max_test_size_abs\n",
    "        training_texts = documents[:split1]\n",
    "        holdout_texts = documents[split1:split2]\n",
    "        test_texts = documents[split2:]\n",
    "        corpora['training_corpus'] = [dictionary.doc2bow(text) for text in training_texts]\n",
    "        corpora['holdout_corpus'] = [dictionary.doc2bow(text) for text in holdout_texts]\n",
    "        corpora['test_corpus'] = [dictionary.doc2bow(text) for text in test_texts]\n",
    "    else:\n",
    "        training_texts = documents\n",
    "        corpora['training_corpus'] = [dictionary.doc2bow(text) for text in training_texts]\n",
    "        corpora['holdout_corpus'], corpora['test_corpus'] = None, None\n",
    "\n",
    "    if tfidf:\n",
    "        for key, bow_corpus in corpora.items():\n",
    "            tfidf_model = TfidfModel(bow_corpus)\n",
    "            corpora[key] = tfidf_model[bow_corpus]\n",
    "    return corpora, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    'Europarl',\n",
    "    'FAZ_combined',\n",
    "    'FOCUS_cleansed',\n",
    "    'OnlineParticipation',\n",
    "    'PoliticalSpeeches',\n",
    "    'dewiki',\n",
    "    'dewac',\n",
    "]\n",
    "goodids = {\n",
    "    # filetered via some fixed rules and similarity measure to character distribution\n",
    "    'dewac': join(ETL_PATH, 'dewac_good_ids.pickle'),\n",
    "    'dewiki': join(ETL_PATH, 'dewiki_good_ids.pickle'),\n",
    "    # the samples contain only a small subset of all articles\n",
    "    # the reason for this is that the samples are roughly equal in size per category\n",
    "    # 'FAZ_combined': join(ETL_PATH, 'FAZ_document_sample3.pickle'),\n",
    "    # 'FOCUS_cleansed': join(ETL_PATH, 'FOCUS_document_sample3.pickle'),\n",
    "}\n",
    "bad_tokens = {\n",
    "    'Europarl': [\n",
    "        'E.', 'Kerr', 'The', 'la', 'ia', 'For', 'Ieke', 'the',\n",
    "    ],\n",
    "    'FAZ_combined': [\n",
    "        'S.', 'j.reinecke@faz.de', 'B.',\n",
    "    ],\n",
    "    'FOCUS_cleansed': [],\n",
    "    'OnlineParticipation': [\n",
    "        'Re', '@#1', '@#2', '@#3', '@#4', '@#5', '@#6', '@#7', '@#8', '@#9', '@#1.1', 'Für', 'Muss',\n",
    "        'etc', 'sorry', 'Ggf', 'u.a.', 'z.B.'\n",
    "        'B.', 'stimmt', ';-)', 'lieber', 'o.', 'Ja', 'Desweiteren',\n",
    "    ],\n",
    "    'PoliticalSpeeches': [],\n",
    "    'dewiki': [],\n",
    "    'dewac': [],\n",
    "}\n",
    "all_bad_tokens = set(chain(*bad_tokens.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: OnlineParticipation\n",
      "reading OnlineParticipation_simple_wiki_phrases.pickle\n",
      "initial number of words: 1661338\n",
      "reducing number of tokens\n",
      "final number of words: 375942\n",
      "number of documents: 25663\n"
     ]
    }
   ],
   "source": [
    "#for dataset in datasets[1:2]:\n",
    "dataset = datasets[3]\n",
    "print('dataset:', dataset)\n",
    "dir_path = join(SMPL_PATH, 'wiki_phrases')\n",
    "files = sorted([f for f in listdir(dir_path) if f.startswith(dataset)])\n",
    "for name in files:\n",
    "    full_path = join(dir_path, name)\n",
    "    if isdir(full_path):\n",
    "        subdir = sorted([join(name, f) for f in listdir(full_path) if f.startswith(dataset)])\n",
    "        files += subdir\n",
    "\n",
    "keepids = None\n",
    "if dataset in goodids:\n",
    "    keepids = pd.read_pickle(goodids[dataset])\n",
    "\n",
    "documents = []\n",
    "for name in files:\n",
    "    gc.collect()\n",
    "    full_path = join(dir_path, name)\n",
    "    if not isfile(full_path):\n",
    "        continue\n",
    "\n",
    "    print('reading', name)\n",
    "    df = pd.read_pickle(join(dir_path, name))\n",
    "    print('initial number of words:', len(df))\n",
    "    print('reducing number of tokens')\n",
    "    if keepids is not None:\n",
    "        # some datasets have already been filtered so you may not see a difference in any case\n",
    "        df = df[df.hash.isin(keepids.index)]\n",
    "\n",
    "    # fixing bad POS tagging\n",
    "    mask = df.token.isin(['[', ']', '<', '>', '/', '–', '%'])\n",
    "    df.loc[mask, POS] = PUNCT\n",
    "\n",
    "    # using only certain POS tags\n",
    "    df = df[df.POS.isin({NOUN, PROPN, 'NER', 'NPHRASE'})]\n",
    "    df[TOKEN] = df[TOKEN].map(lambda x: x.strip('-/'))\n",
    "    df = df[df.token.str.len() > 1]\n",
    "    df = df[~df.token.isin(all_bad_tokens)]\n",
    "    print('final number of words:', len(df))\n",
    "    # groupby sorts the documents by hash-id which is equal to shuffeling the dataset before building the model\n",
    "    df = df.groupby([HASH])[TOKEN].agg(docs_to_lists)\n",
    "    documents += df.values.tolist()\n",
    "\n",
    "length = len(documents)\n",
    "print('number of documents:', length)\n",
    "del keepids, files\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora, dictionary = docs2corpora(\n",
    "    documents, tfidf=False,\n",
    "    #stopwords=bad_tokens[dataset],  # stopword removale has been moved to the pandas preprocessing pipeline\n",
    "    filter_below=5, filter_above=0.5,\n",
    "    split=True,\n",
    ")\n",
    "training_corpus = corpora['training_corpus']\n",
    "holdout_corpus = corpora['holdout_corpus']\n",
    "test_corpus = corpora['test_corpus']\n",
    "\n",
    "def init_callbacks(viz_env=None, title_suffix=''):\n",
    "    # define perplexity callback for hold_out and test corpus\n",
    "    pl_holdout = PerplexityMetric(corpus=holdout_corpus, logger=\"visdom\", viz_env=viz_env, title=\"Perplexity (hold_out)\"+title_suffix)\n",
    "    pl_test = PerplexityMetric(corpus=test_corpus, logger=\"visdom\", viz_env=viz_env, title=\"Perplexity (test)\"+title_suffix)\n",
    "\n",
    "    # define other remaining metrics available\n",
    "    ch_umass = CoherenceMetric(corpus=training_corpus, coherence=\"u_mass\", topn=10, logger=\"visdom\", viz_env=viz_env, title=\"Coherence (u_mass)\"+title_suffix)\n",
    "    ch_cv = CoherenceMetric(corpus=training_corpus, texts=documents, coherence=\"c_v\", topn=10, logger=\"visdom\", viz_env=viz_env, title=\"Coherence (c_v)\"+title_suffix)\n",
    "    diff_kl = DiffMetric(distance=\"kullback_leibler\", logger=\"visdom\", viz_env=viz_env, title=\"Diff (kullback_leibler)\"+title_suffix)\n",
    "    convergence_kl = ConvergenceMetric(distance=\"jaccard\", logger=\"visdom\", viz_env=viz_env, title=\"Convergence (jaccard)\"+title_suffix)\n",
    "\n",
    "    return [pl_holdout, pl_test, ch_umass, ch_cv, diff_kl, convergence_kl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameterset(corpus, dictionary, callbacks=None, nbtopics=100, parametrization='a42', eval_every=None):\n",
    "    print(f'building LDA model \"{parametrization}\" with {nbtopics} number of topics')\n",
    "    default = dict(random_state=42, corpus=corpus, id2word=dictionary, num_topics=nbtopics, \n",
    "                   eval_every=eval_every, callbacks=callbacks, chunksize=20_000)\n",
    "    ldamodels = {\n",
    "        'a42': dict(),\n",
    "        'b42': dict(passes=10, iterations=100),\n",
    "        'c42': dict(passes=10, iterations=10_000),\n",
    "        'd42': dict(passes=10, iterations=200, alpha=0.1, eta=0.01),\n",
    "        'x42': dict(passes=20, iterations=1000, alpha='auto', eta='auto'),\n",
    "    }\n",
    "    for key, dic in ldamodels.items():\n",
    "        dic.update(default)\n",
    "    return ldamodels[parametrization]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Visdom via ```pip install visdom``` and run ```python -m visdom.server``` to start the server.\n",
    "You will be able to view online log stats @ http://localhost:8097/\n",
    "\n",
    "Attention: gensim is currently not fully compatible with visdom. See https://github.com/RaRe-Technologies/gensim/issues/2155 for details.\n",
    "\n",
    "To fix this issue change in gensim.models.callbacks.Callback.on_epoch_end the following line\n",
    "```\n",
    "self.viz.updateTrace(\n",
    "    Y=np.array([value]), X=np.array([epoch]), env=metric.viz_env, win=self.windows[i]\n",
    ")\n",
    "```\n",
    "to\n",
    "```\n",
    "self.viz.line(\n",
    "    Y=np.array([value]), X=np.array([epoch]), env=metric.viz_env, win=self.windows[i], update='append'\n",
    ")\n",
    "```\n",
    "You might need to re-import gensim (and probably even restart this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building LDA model \"d42\" with 10 number of topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreas/bin/anaconda3/lib/python3.6/site-packages/gensim/models/ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'Perplexity (hold_out) | 10 topics': [5096.9190789029, 2928.7997410063567, 1751.257394789178, 1231.9200346328369, 987.8664996848179, 857.2400337059956, 778.9205036519083, 728.4404881567187, 693.6528240087434, 669.0051764297576], 'Perplexity (test) | 10 topics': [5312.882484689098, 3033.548406189043, 1808.9170214194328, 1268.9769344156712, 1015.0047524540354, 879.4458826871764, 798.713355877408, 746.8234489849148, 711.5117711492551, 685.7000600154025], 'Coherence (u_mass) | 10 topics': [-2.7548603418888424, -2.652116886541265, -2.9918325587118964, -2.7308262359958446, -2.7406916212486374, -2.6045957351156916, -2.605917692473647, -2.5934489220157437, -2.5021117559987, -2.485888933834015], 'Coherence (c_v) | 10 topics': [0.4773905354360496, 0.5382716771487651, 0.5753546775332874, 0.6017238526246877, 0.5939963184718172, 0.6009822826596427, 0.6042310663132253, 0.6149887880543139, 0.6201653463698727, 0.6272596816238327], 'Diff (kullback_leibler) | 10 topics': [array([0.6357506 , 0.63346066, 0.60367328, 0.69632417, 1.        ,\n",
      "       0.63075582, 0.72509774, 0.7217279 , 0.64413711, 0.68601379]), array([0.84836405, 0.73849021, 0.54737793, 1.        , 0.86509165,\n",
      "       0.76008335, 0.70538194, 0.83461284, 0.6641004 , 0.81804727]), array([0.70564517, 0.77969924, 0.49678719, 1.        , 0.66470827,\n",
      "       0.76520582, 0.64141264, 0.84971844, 0.76134527, 0.75996035]), array([0.72734961, 0.8652002 , 0.47212721, 1.        , 0.5385795 ,\n",
      "       0.88052837, 0.61181404, 0.83591726, 0.96124139, 0.69562567]), array([0.74982203, 0.98616034, 0.44799682, 1.        , 0.43584087,\n",
      "       0.86026493, 0.69900082, 0.78415456, 0.98283006, 0.75945525]), array([0.69955153, 0.87521271, 0.38350839, 1.        , 0.34841108,\n",
      "       0.69662909, 0.54076099, 0.76599989, 0.9986315 , 0.55840086]), array([0.72906794, 0.96749537, 0.42590126, 0.99978313, 0.34707502,\n",
      "       0.72686202, 0.57098369, 0.76675613, 1.        , 0.57636508]), array([0.67130626, 0.92469118, 0.46004675, 0.91139943, 0.28894979,\n",
      "       0.75347664, 0.51653918, 0.7818873 , 1.        , 0.60015531]), array([0.59355766, 0.85384634, 0.40650879, 0.75855103, 0.23126309,\n",
      "       0.76321623, 0.43101752, 0.7387128 , 1.        , 0.43252262]), array([0.52707839, 0.77070972, 0.41244789, 0.78188403, 0.20072591,\n",
      "       0.69619827, 0.40101428, 0.85621899, 1.        , 0.36637257])], 'Convergence (jaccard) | 10 topics': [9.899140161521826, 7.470322236885183, 7.882967409528289, 5.299785547541547, 6.417152525401843, 6.815963720872374, 5.385815052093445, 6.039409823335275, 5.524752475247529, 4.029702970297035]})\n",
      "saving to ../data/preprocessed/LDAmodel/d42/OnlineParticipation_LDAmodel_d42_10\n",
      "building LDA model \"d42\" with 20 number of topics\n",
      "defaultdict(<class 'list'>, {'Perplexity (hold_out) | 20 topics': [16165.87126419881, 6703.083094018401, 3037.2363702592856, 1805.3570544840738, 1324.1703116397427, 1097.4246556021499, 972.971454181449, 896.0441269148686, 845.1251318822352, 809.168304158672], 'Perplexity (test) | 20 topics': [17015.407620057933, 6982.921149376341, 3146.96972514253, 1863.5015253862314, 1365.6460593781605, 1129.0638232218628, 999.4003003170326, 919.926111910028, 866.6359961514063, 829.2003302080785], 'Coherence (u_mass) | 20 topics': [-4.568775403782448, -4.341598563841089, -4.490172488794986, -4.323022580702633, -4.243393562692033, -4.159291603528326, -4.231805168619915, -4.224689638667676, -4.291511979144524, -4.226929004966111], 'Coherence (c_v) | 20 topics': [0.4833039615675214, 0.5047402157069973, 0.5130301029969634, 0.5109962027653057, 0.5265497530607822, 0.5337681359922867, 0.5299981368018193, 0.5292067451882829, 0.5284596156006722, 0.5258384187814837], 'Diff (kullback_leibler) | 20 topics': [array([0.62251099, 0.59826227, 0.57860343, 0.67143105, 1.        ,\n",
      "       0.61137762, 0.70380321, 0.70682919, 0.63811203, 0.6307914 ,\n",
      "       0.71083241, 0.69273046, 0.60678653, 0.64437743, 0.61984127,\n",
      "       0.71476366, 0.67113419, 0.6540781 , 0.63397503, 0.68579381]), array([0.6489734 , 0.61338716, 0.44892342, 0.79602434, 0.62294987,\n",
      "       0.58975172, 0.51361843, 0.65158925, 0.51240004, 0.58921132,\n",
      "       0.59847762, 0.66952664, 0.75838572, 0.6633027 , 0.75566888,\n",
      "       0.54146703, 0.73418126, 1.        , 0.59842597, 0.59025192]), array([0.63836808, 0.64246635, 0.51614396, 1.        , 0.38062048,\n",
      "       0.67326768, 0.51072928, 0.65938402, 0.54874566, 0.57934234,\n",
      "       0.62662145, 0.74049885, 0.8170058 , 0.71399836, 0.97164889,\n",
      "       0.73153333, 0.80714338, 0.88734606, 0.5573179 , 0.54139506]), array([0.57128139, 0.75145925, 0.55159366, 1.        , 0.30110555,\n",
      "       0.75978244, 0.48606747, 0.69004618, 0.56379754, 0.62698951,\n",
      "       0.59655462, 0.6961246 , 0.70786884, 0.66257797, 0.86499892,\n",
      "       0.88181995, 0.84710759, 0.79291009, 0.60213983, 0.57516425]), array([0.48317581, 0.74448391, 0.50815708, 0.85441815, 0.20937655,\n",
      "       0.62443718, 0.42177506, 0.56159263, 0.50302023, 0.63511123,\n",
      "       0.50142543, 0.71287901, 0.62498428, 0.60559959, 0.69945239,\n",
      "       0.84789385, 1.        , 0.58465143, 0.53336372, 0.49399326]), array([0.55850438, 0.89233618, 0.69401545, 1.        , 0.20280871,\n",
      "       0.90580686, 0.53909998, 0.61002904, 0.65205019, 0.77900576,\n",
      "       0.58871676, 0.6872172 , 0.79611385, 0.72682744, 0.75592472,\n",
      "       0.9937787 , 0.98000282, 0.67526753, 0.7806914 , 0.60729869]), array([0.51689413, 0.81295521, 0.60490045, 0.87180957, 0.15687786,\n",
      "       0.881414  , 0.51425437, 0.53559805, 0.66497472, 0.8000179 ,\n",
      "       0.63598921, 0.55183897, 0.86760309, 0.73898092, 0.66141569,\n",
      "       0.85629437, 1.        , 0.60661284, 0.81380181, 0.65287326]), array([0.57862223, 0.79183799, 0.606395  , 0.83210487, 0.14272834,\n",
      "       0.76017972, 0.5505973 , 0.50831623, 0.66635372, 1.        ,\n",
      "       0.68750315, 0.53767739, 0.95229695, 0.65297026, 0.62712307,\n",
      "       0.78622283, 0.86834044, 0.45610664, 0.84360975, 0.61905436]), array([0.43506344, 0.66075181, 0.58597861, 0.66124862, 0.10580443,\n",
      "       0.50139542, 0.42546903, 0.40258632, 0.58589653, 0.6611632 ,\n",
      "       0.56601885, 0.54297008, 1.        , 0.45865635, 0.49807285,\n",
      "       0.57000442, 0.66107058, 0.39360503, 0.6139633 , 0.51360547]), array([0.50341233, 0.76098253, 0.56739895, 0.74263276, 0.10009619,\n",
      "       0.52709277, 0.46087732, 0.45638016, 0.70178164, 0.67499286,\n",
      "       0.65764639, 0.55784459, 1.        , 0.47044347, 0.54614686,\n",
      "       0.69265311, 0.74682966, 0.38957113, 0.62243446, 0.60449691])], 'Convergence (jaccard) | 20 topics': [19.69924485511893, 14.390260078837288, 10.58348123266046, 11.983205248842449, 10.262655836805687, 8.716185042230697, 9.889929941023803, 8.438749757328678, 10.544554455445553, 8.771824241247655]})\n",
      "saving to ../data/preprocessed/LDAmodel/d42/OnlineParticipation_LDAmodel_d42_20\n",
      "building LDA model \"d42\" with 30 number of topics\n",
      "defaultdict(<class 'list'>, {'Perplexity (hold_out) | 30 topics': [35421.75160955472, 11542.18491692387, 4308.513159152518, 2293.603059927839, 1592.52707692437, 1280.9676248690382, 1115.5161299265449, 1014.6843247433736, 948.5327073267947, 902.175302376012], 'Perplexity (test) | 30 topics': [37150.4956427022, 12018.469595605884, 4459.437342208684, 2363.3224117257396, 1637.0433002095722, 1315.4591831996331, 1144.3369387041596, 1040.5395475612536, 971.9723204807784, 923.9499899405894], 'Coherence (u_mass) | 30 topics': [-5.765727161271, -5.913926369807142, -5.723948810037724, -5.391841962236276, -5.404402256134094, -5.350484126057123, -5.32474333939687, -5.147465563136488, -5.2271884408145635, -5.020964708672906], 'Coherence (c_v) | 30 topics': [0.43425873724376296, 0.4665752364923347, 0.477143539676128, 0.48826195919167287, 0.48827019863487203, 0.489077349845231, 0.48989519051659725, 0.5057367202975295, 0.5046560968176815, 0.5157436056252311], 'Diff (kullback_leibler) | 30 topics': [array([0.61421722, 0.60169177, 0.59287988, 0.65712246, 1.        ,\n",
      "       0.61879216, 0.72475602, 0.71242029, 0.63961726, 0.66136296,\n",
      "       0.6769662 , 0.70324163, 0.61275911, 0.658855  , 0.63243144,\n",
      "       0.69443698, 0.64167565, 0.69175213, 0.6507163 , 0.63682696,\n",
      "       0.70205202, 0.69613425, 0.63857153, 0.66538277, 0.65513945,\n",
      "       0.63922794, 0.69884318, 0.59816063, 0.6037097 , 0.6532415 ]), array([0.6055793 , 0.57660522, 0.39643452, 0.70473628, 0.46276617,\n",
      "       0.60344906, 0.48662333, 0.51492791, 0.40950019, 0.54207948,\n",
      "       0.46307187, 0.45872109, 0.68050372, 0.478731  , 0.7363408 ,\n",
      "       0.52294334, 0.68408791, 0.69678238, 0.46265708, 0.6777369 ,\n",
      "       1.        , 0.56871109, 0.44318983, 0.7597618 , 0.5410926 ,\n",
      "       0.69937355, 0.62447935, 0.62463023, 0.71445666, 0.59662933]), array([0.69686579, 0.68870563, 0.52833128, 0.69174559, 0.28387785,\n",
      "       0.65445398, 0.47116151, 0.55625353, 0.5468976 , 0.60756873,\n",
      "       0.52106489, 0.44860052, 0.67902889, 0.57254952, 0.70964127,\n",
      "       0.6196505 , 0.83173102, 0.71877373, 0.44007622, 0.85201849,\n",
      "       1.        , 0.54277045, 0.42789435, 0.91714157, 0.65978376,\n",
      "       0.6134898 , 0.72164907, 0.76940322, 0.7500677 , 0.70195725]), array([0.71613232, 0.86788099, 0.67365842, 0.73827977, 0.19858655,\n",
      "       0.63953056, 0.53731849, 0.71081557, 0.62088517, 0.86820156,\n",
      "       0.77605698, 0.51771941, 0.74225057, 0.64878283, 0.67696579,\n",
      "       0.65388718, 0.89771619, 0.73627066, 0.53349418, 0.96827154,\n",
      "       0.92666703, 0.60946463, 0.49856686, 1.        , 0.80490996,\n",
      "       0.65630429, 0.79029598, 0.96005336, 0.80863356, 0.85032187]), array([0.70366141, 0.91492721, 0.67544154, 0.67431789, 0.11654217,\n",
      "       0.60092759, 0.4642086 , 0.57798673, 0.55553809, 0.85472708,\n",
      "       0.63884286, 0.54646547, 0.69338241, 0.61737044, 0.61478295,\n",
      "       0.59099619, 0.74640413, 0.64157958, 0.47633175, 0.99347538,\n",
      "       0.90170549, 0.61493283, 0.47207053, 0.8493455 , 0.74465174,\n",
      "       0.59995305, 0.7517482 , 1.        , 0.67186209, 0.76915503]), array([0.61934618, 0.91636509, 0.69190846, 0.56512951, 0.09388092,\n",
      "       0.59496168, 0.40755845, 0.58268821, 0.56192113, 0.92944234,\n",
      "       0.6104105 , 0.62091806, 0.69095692, 0.58095644, 0.47656719,\n",
      "       0.73336917, 0.68909665, 0.60098661, 0.47770351, 1.        ,\n",
      "       0.73583371, 0.82319742, 0.46617071, 0.81211623, 0.77325188,\n",
      "       0.58937771, 0.70668644, 0.90681204, 0.60773127, 0.75350098]), array([0.60553822, 1.        , 0.66140975, 0.46775733, 0.07865271,\n",
      "       0.61189426, 0.32730619, 0.44538413, 0.5247693 , 0.83213164,\n",
      "       0.48435595, 0.48720261, 0.72988939, 0.55978743, 0.41658371,\n",
      "       0.76819263, 0.88864772, 0.56235695, 0.43840933, 0.8733534 ,\n",
      "       0.60871587, 0.84745378, 0.62354915, 0.75289154, 0.72332004,\n",
      "       0.64260351, 0.64911031, 0.76122685, 0.48770173, 0.72798524]), array([0.50600161, 0.68481338, 0.56884909, 0.37515693, 0.05239053,\n",
      "       0.48980714, 0.28389217, 0.34002473, 0.44802951, 0.67346151,\n",
      "       0.45138455, 0.38368214, 0.48983948, 0.42240768, 0.3325412 ,\n",
      "       1.        , 0.49767371, 0.49860099, 0.37297468, 0.63649911,\n",
      "       0.52527731, 0.77476883, 0.5005059 , 0.63706204, 0.59226057,\n",
      "       0.74188802, 0.519768  , 0.59315784, 0.36643964, 0.54661751]), array([0.7512776 , 0.87501537, 0.77379231, 0.48554318, 0.06206318,\n",
      "       0.69531287, 0.32858261, 0.4697428 , 0.63198572, 0.83559139,\n",
      "       0.57304588, 0.53352088, 0.83197516, 0.57016875, 0.43181861,\n",
      "       0.89336127, 0.72837071, 0.70801883, 0.50753955, 0.81320494,\n",
      "       0.64273528, 0.9702942 , 0.7130731 , 0.86849736, 0.80961396,\n",
      "       1.        , 0.76088395, 0.7566884 , 0.48007176, 0.86366492]), array([0.50378685, 0.63909202, 0.5550833 , 0.34877998, 0.04301463,\n",
      "       0.4741962 , 0.22132341, 0.34696174, 0.465508  , 0.55278537,\n",
      "       0.42919215, 0.43288799, 0.53387431, 0.40088718, 0.28657471,\n",
      "       0.84388594, 0.59141829, 0.46929849, 0.3857096 , 0.51133941,\n",
      "       0.33270657, 1.        , 0.57466963, 0.60819438, 0.61841526,\n",
      "       0.75590228, 0.50228848, 0.5486432 , 0.34950531, 0.53433369])], 'Convergence (jaccard) | 30 topics': [29.372873673844115, 15.38373149243998, 15.768693639230907, 16.70497019433798, 15.576987273272486, 15.073536262381666, 15.725360143095985, 7.788828970758958, 13.157833430401867, 11.158027567462634]})\n",
      "saving to ../data/preprocessed/LDAmodel/d42/OnlineParticipation_LDAmodel_d42_30\n"
     ]
    }
   ],
   "source": [
    "params = ['x42', 'a42', 'b42', 'c42', 'd42'][-1]\n",
    "implementations = [\n",
    "    ('LDAmodel', LdaModel),\n",
    "    ('LDAmulticore', LdaMulticore)\n",
    "]\n",
    "choice = 0\n",
    "model_name = implementations[choice][0]\n",
    "UsedModel = implementations[choice][1]\n",
    "save = True\n",
    "metrics = []\n",
    "env_id = f\"{dataset}-{model_name}-{params}\"\n",
    "for nbtopics in range(10, 101, 10):\n",
    "    # Choose α from [0.05, 0.1, 0.5, 1, 5, 10]\n",
    "    # Choose β from [0.05, 0.1, 0.5, 1, 5, 10]\n",
    "    callbacks = init_callbacks(viz_env=env_id, title_suffix=f\" | {nbtopics} topics\")\n",
    "    kwargs = get_parameterset(training_corpus, dictionary, callbacks=callbacks, nbtopics=nbtopics, parametrization=params)\n",
    "    if 'multicore' in model_name:\n",
    "        kwargs['workers'] = 3\n",
    "    ldamodel = UsedModel(**kwargs)\n",
    "\n",
    "    topics = [[dataset] + [dictionary[term[0]] for term in ldamodel.get_topic_terms(i)] for i in range(nbtopics)]\n",
    "    df_lda = pd.DataFrame(topics, columns=['dataset']+['term'+str(i) for i in range(10)])\n",
    "\n",
    "    # calculate (average) UMass score\n",
    "    #top_topics = ldamodel.top_topics(training_corpus, topn=10, processes=8)\n",
    "    # Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "    #avg_topic_coherence = sum([t[1] for t in top_topics]) / nbtopics\n",
    "    #print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "    current_metrics = ldamodel.metrics\n",
    "    print(current_metrics)\n",
    "    metrics.append(('env_id', current_metrics))\n",
    "\n",
    "    if save:\n",
    "        out_dir = join(ETL_PATH, f'{model_name}/{params}')\n",
    "        if not exists(out_dir):\n",
    "            makedirs(out_dir)\n",
    "        out = join(out_dir, f'{dataset}_{model_name}_{params}_{nbtopics}')\n",
    "        print('saving to', out)\n",
    "        df_lda.to_csv(out + '.csv')\n",
    "        ldamodel.save(out)\n",
    "        with open(out + '_metrics.json', 'w') as fp:\n",
    "            serializable_metrics = {}\n",
    "            for k, v in current_metrics.items():\n",
    "                if isinstance(v[0], np.ndarray):\n",
    "                    serializable_metrics[k] = [x.tolist() for x in v]\n",
    "                else:\n",
    "                    serializable_metrics[k] = [float(x) for x in v]\n",
    "            json.dump(serializable_metrics, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = ldamodel.top_topics(corpus, topn=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / len(top_topics)\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdpmodel = HdpModel(corpus=corpus, id2word=dictionary, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htf = HdpTopicFormatter(dictionary, topic_data=hdpmodel.get_topics())\n",
    "topics = [[dataset] + [term[0] for term in htf.show_topic(i, topn=10, formatted=False)] for i in range(nbtopics)]\n",
    "df_hdp = pd.DataFrame(topics, columns=['dataset']+['term'+str(i) for i in range(10)])\n",
    "df_hdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_prob_extractor(gensim_hdp):\n",
    "    shown_topics = gensim_hdp.show_topics(num_topics=-1, formatted=False)\n",
    "    topics_nos = [x[0] for x in shown_topics ]\n",
    "    weights = [ sum([item[1] for item in shown_topics[topicN][1]]) for topicN in topics_nos ]\n",
    "\n",
    "    return pd.DataFrame({'topic_id' : topics_nos, 'weight' : weights})\n",
    "\n",
    "topic_prob_extractor(hdpmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "for topic_id, topic in hdpmodel.show_topics(num_topics=10, formatted=False):\n",
    "    topic = [word for word, _ in topic]\n",
    "    topics.append(topic)\n",
    "topics[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherences = []\n",
    "for topic in topics:\n",
    "    cm = CoherenceModel(topics=[topic], corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
    "    coherences.append(cm.get_coherence())\n",
    "coherences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(coherences) / len(coherences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmlda = CoherenceModel(model=ldamodel, corpus=corpus, coherence='u_mass')\n",
    "cmlda.get_coherence()\n",
    "cmhdp = CoherenceModel(model=hdpmodel, corpus=corpus, coherence='u_mass')\n",
    "cmhdp.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoherenceModel.for_topics(topics_as_topn_terms=topics, corpus=corpus, dictionary=dictionary, coherence='u_mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.get_coherence_per_topic(segmented_topics=topics, with_std=True, with_support=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MmCorpus.serialize('../data/{}.mm'.format(dataset), corpus)\n",
    "corpus_fake = MmCorpus('../data/{}.mm'.format(dataset))\n",
    "prepared_data = ldavis.gensim.prepare(ldamodel, corpus_fake, dictionary)\n",
    "prepared_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the topic words from the model\n",
    "topics = []\n",
    "for topic_id, topic in hm.show_topics(num_topics=10, formatted=False):\n",
    "    topic = [word for word, _ in topic]\n",
    "    topics.append(topic)\n",
    "topics[:2]\n",
    "\n",
    "# Initialize CoherenceModel using `topics` parameter\n",
    "coherences = []\n",
    "for topic in topics:\n",
    "    cm = CoherenceModel(topics=[topic], corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
    "    coherences.append(cm.get_coherence())\n",
    "coherences\n",
    "\n",
    "sum(coherences) / len(coherences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start c_v coherence measure\n",
    "This is expected to take much more time since `c_v` uses a sliding window to perform probability estimation and uses the cosine similarity indirect confirmation measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cm = CoherenceModel(topics=usable_topics, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "c_v = cm.get_coherence_per_topic()\n",
    "print(\"Calculated c_v coherence for %d topics\" % len(c_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start c_uci and c_npmi coherence measures\n",
    "c_v and c_uci and c_npmi all use the boolean sliding window approach of estimating probabilities. Since the `CoherenceModel` caches the accumulated statistics, calculation of c_uci and c_npmi are practically free after calculating c_v coherence. These two methods are simpler and were shown to correlate less with human judgements than c_v but more so than u_mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cm.coherence = 'c_uci'\n",
    "c_uci = cm.get_coherence_per_topic()\n",
    "print(\"Calculated c_uci coherence for %d topics\" % len(c_uci))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cm.coherence = 'c_npmi'\n",
    "c_npmi = cm.get_coherence_per_topic()\n",
    "print(\"Calculated c_npmi coherence for %d topics\" % len(c_npmi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores = [\n",
    "    score for i, score in enumerate(human_scores)\n",
    "    if i not in invalid_topic_indices\n",
    "]\n",
    "len(final_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [values in the paper](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf) were:\n",
    "\n",
    "__`u_mass` correlation__ : 0.093\n",
    "\n",
    "__`c_v` correlation__    : 0.548\n",
    "\n",
    "__`c_uci` correlation__  : 0.473\n",
    "\n",
    "__`c_npmi` correlation__ : 0.438\n",
    "\n",
    "Our values are also very similar to these values which is good. This validates the correctness of our pipeline, as we can reasonably attribute the differences to differences in preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for our_scores in (u_mass, c_v, c_uci, c_npmi):\n",
    "    print(pearsonr(our_scores, final_scores)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(u_mass)/99, sum(c_v)/99, sum(c_uci)/99, sum(c_npmi)/99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.save('movies_coherence_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "\n",
    "\n",
    "def report_on_oov_terms(cm, topic_models):\n",
    "    \"\"\"OOV = out-of-vocabulary\"\"\"\n",
    "    topics_as_topn_terms = [\n",
    "        models.CoherenceModel.top_topics_as_word_lists(model, dictionary)\n",
    "        for model in topic_models\n",
    "    ]\n",
    "\n",
    "    oov_words = cm._accumulator.not_in_vocab(topics_as_topn_terms)\n",
    "    print('number of oov words: %d' % len(oov_words))\n",
    "    \n",
    "    for num_topics, words in zip(trained_models.keys(), topics_as_topn_terms):\n",
    "        oov_words = cm._accumulator.not_in_vocab(words)\n",
    "        print('number of oov words for num_topics=%d: %d' % (num_topics, len(oov_words)))\n",
    "\n",
    "report_on_oov_terms(cm, trained_models.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
