{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is only for development.\n",
    "\n",
    "Use phrase_extraction_in_batches.py instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from spacy.symbols import IDS\n",
    "from constants import *\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "pd.options.display.max_rows = 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = 'dewac_31'\n",
    "#Europarl\n",
    "#FAZ\n",
    "#FOCUS\n",
    "#PoliticalSpeeches\n",
    "fpath = join(NLP_PATH, corpus + '_nlp.pickle')\n",
    "df = pd.read_pickle(fpath)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:431_731]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dewac title needs to be removed from the document, as it only contains the url\n",
    "def remove_title(x):\n",
    "    ln_argmx = (x.text.values == '\\n').argmax()\n",
    "    return x[ln_argmx+1:]\n",
    "\n",
    "if corpus.startswith('dewac'):\n",
    "    goodids = pd.read_pickle(join(ETL_PATH, 'dewac_good_ids.pickle'))\n",
    "    print(len(df))\n",
    "    df = df[df.hash.isin(goodids.index)]\n",
    "    print(len(df))\n",
    "    df = df.groupby(HASH, sort=False, as_index=False).progress_apply(remove_title).reset_index(level=0, drop=True)\n",
    "    print(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_entities(column):\n",
    "    if column.name in {HASH, SENT_IDX, ENT_IDX, ENT_TYPE}:\n",
    "        return column.values[0]\n",
    "    if column.name in {'tok_idx', NOUN_PHRASE, 'np'}:\n",
    "        return tuple(column.values)\n",
    "    if column.name in {TEXT, TOKEN}:\n",
    "        return column.str.cat(sep='_')\n",
    "    return False\n",
    "\n",
    "def get_length(tpl):\n",
    "    return \n",
    "\n",
    "def get_removable_tokens(df_in):\n",
    "    remove_token = []\n",
    "    for i, sent_idx, tok_set in df_in.itertuples():\n",
    "        for tok_idx in tok_set:\n",
    "            remove_token.append((sent_idx, tok_idx))\n",
    "    df_out = (\n",
    "        pd.DataFrame\n",
    "        .from_records(remove_token, columns=[SENT_IDX, TOK_IDX])\n",
    "        .assign(hash=0, ent_idx=0)\n",
    "    )\n",
    "    return df_out\n",
    "\n",
    "def insert_phrases(df_orig, df_insert):\n",
    "    \"\"\"add phrases and replace overlapping tokens\"\"\"\n",
    "    # df_removable_tokens: this DataFrame contains all token-idx we want to replace with phrases\n",
    "    df_removable_tokens = get_removable_tokens(df_insert[[SENT_IDX, 'tok_set']])\n",
    "    df_combined = (\n",
    "        df_orig\n",
    "        # remove original unigram tokens\n",
    "        .append(df_removable_tokens)\n",
    "        .drop_duplicates(subset=[SENT_IDX, TOK_IDX], keep=False)\n",
    "        .dropna(subset=[TOKEN])\n",
    "        # insert concatenated phrase tokens\n",
    "        .append(df_insert)\n",
    "        .sort_values([SENT_IDX, TOK_IDX])\n",
    "    )\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extracting spacy NER\n",
    "df_ent = (\n",
    "    df\n",
    "    .query('ent_idx > 0 & POS != \"SPACE\"')  # phrases have an ent-index > 0 and we don't care about whitespace\n",
    "    .groupby(ENT_IDX).filter(lambda x: len(x) > 1)  # we case only about entities greater than 1 token\n",
    "    .groupby(ENT_IDX, as_index=False).agg(concat_entities)  # concatenate entities\n",
    "    .assign(\n",
    "        length=lambda x: x.tok_idx.apply(lambda y: len(y)),  # add the number of tokens per entity as a new column\n",
    "        POS='NER',  # annotations\n",
    "        ent_iob='P',\n",
    "    )\n",
    "    .astype({  # set annoation columns as categorical for memory savings\n",
    "        POS: \"category\",\n",
    "        ENT_IOB: \"category\", \n",
    "        ENT_TYPE: \"category\"\n",
    "    })\n",
    ")\n",
    "#df_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extracting spacy noun chunks\n",
    "df_np = (\n",
    "    df\n",
    "    .query('noun_phrase > 0 & POS not in [\"SPACE\", \"NUM\", \"DET\", \"SYM\"]')\n",
    "    .groupby(NOUN_PHRASE).filter(lambda x: len(x) > 1)\n",
    "    .groupby(NOUN_PHRASE, as_index=False).agg(concat_entities)\n",
    "    .assign(\n",
    "        length=lambda x: x.tok_idx.apply(lambda y: len(y)),\n",
    "        POS='NPHRASE',\n",
    "        ent_iob='P',\n",
    "    )\n",
    "    .astype({\n",
    "        POS: \"category\", \n",
    "        ENT_IOB: \"category\", \n",
    "        ENT_TYPE: \"category\"\n",
    "    })\n",
    ")\n",
    "#df_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# intersecting both extraction methods returns very nice results\n",
    "df_phrases = df_ent.append(df_np)\n",
    "mask = df_phrases.duplicated([HASH, SENT_IDX, TOK_IDX])\n",
    "df_phrases = df_phrases[mask]\n",
    "# set column token-index to start of phrase and add column column for the token-indexes instead\n",
    "df_phrases['tok_set'] = df_phrases[TOK_IDX]\n",
    "df_phrases[TOK_IDX] = df_phrases[TOK_IDX].apply(lambda x: x[0])\n",
    "#df_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# based on Philipp Grawes approach on extracting and normalizing street names\n",
    "STREET_NAME_LIST = [r'strasse$', r'straße$', r'str$', r'str.$', r'platz', r'gasse$', r'allee$', r'ufer$', r'weg$']\n",
    "STREET_NAMES = re.compile(r'(' + '|'.join(STREET_NAME_LIST) + ')', re.IGNORECASE)\n",
    "STREET_PATTERN = re.compile(r\"str(\\.|a(ss|ß)e)?\\b\", re.IGNORECASE)\n",
    "SPECIAL_CHAR = re.compile(r'[^\\w&\\/]+')\n",
    "\n",
    "def aggregate_streets(column):\n",
    "    if column.name in {HASH, SENT_IDX, ENT_IDX}:\n",
    "        return column.values[0]\n",
    "    if column.name in {'tok_idx', NOUN_PHRASE, 'np'}:\n",
    "        return tuple(column.values)\n",
    "    if column.name == TEXT:\n",
    "        return column.str.cat(sep='_')\n",
    "    if column.name == TOKEN:\n",
    "        street_candidate = False\n",
    "        for k, token in column.iteritems():\n",
    "            if re.search(STREET_NAMES, token):\n",
    "                street_candidate = True\n",
    "        if street_candidate:\n",
    "            if len(column) == 1 and re.fullmatch(STREET_NAMES, column.values[0]):\n",
    "                return False\n",
    "            else:\n",
    "                street_name = column.str.cat(sep=' ')\n",
    "                street_name = STREET_PATTERN.sub('straße', street_name)\n",
    "                street_name = SPECIAL_CHAR.sub('_', street_name)\n",
    "                street_name = street_name.strip('_').title()\n",
    "                return street_name\n",
    "    return False\n",
    "\n",
    "df_loc = (\n",
    "    df\n",
    "    .loc[(df[ENT_IDX] > 0) & (df.POS != SPACE)]\n",
    "    .groupby(ENT_IDX, as_index=False).agg(aggregate_streets)\n",
    "    .query('token != False')\n",
    "    .assign(\n",
    "        length=lambda x: x.tok_idx.apply(lambda y: len(y)),\n",
    "        tok_set=lambda x: x.tok_idx,\n",
    "        tok_idx=lambda x: x.tok_idx.apply(lambda y: y[0]),\n",
    "        POS='PROPN', \n",
    "        ent_iob='L', \n",
    "        ent_type='STREET'\n",
    "    )\n",
    "    .astype({\n",
    "        POS: \"category\", \n",
    "        ENT_IOB: \"category\", \n",
    "        ENT_TYPE: \"category\"\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# insert phrases to original tokens\n",
    "df_glued = insert_phrases(df, df_phrases)\n",
    "# insert locations / streets\n",
    "df_glued = insert_phrases(df_glued, df_loc)\n",
    "# simplify dataframe and store\n",
    "df_glued = (\n",
    "    df_glued\n",
    "    .loc[df_glued.POS != 'SPACE', [HASH, POS, SENT_IDX, TOK_IDX, TOKEN]]\n",
    "    .astype({\n",
    "        HASH: np.int64,\n",
    "        POS: \"category\",\n",
    "        SENT_IDX: np.int32,\n",
    "        TOK_IDX: np.int32,\n",
    "    })\n",
    ")\n",
    "write_path = join(ETL_PATH + '/simple', corpus + '_simple.pickle')\n",
    "df_glued.to_pickle(write_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_pickle(write_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
