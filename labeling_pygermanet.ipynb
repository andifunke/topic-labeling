{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygermanet import load_germanet\n",
    "gn = load_germanet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset(auseinandergehen.v.3),\n",
       " Synset(funktionieren.v.1),\n",
       " Synset(funktionieren.v.2),\n",
       " Synset(gehen.v.1),\n",
       " Synset(gehen.v.4),\n",
       " Synset(gehen.v.5),\n",
       " Synset(gehen.v.6),\n",
       " Synset(gehen.v.7),\n",
       " Synset(gehen.v.9),\n",
       " Synset(gehen.v.10),\n",
       " Synset(gehen.v.11),\n",
       " Synset(gehen.v.12),\n",
       " Synset(gehen.v.13),\n",
       " Synset(gehen.v.14),\n",
       " Synset(handeln.v.1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gn.synsets('gehen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['diejenigen']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gn.lemmatise(u'ginge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset(funktionieren.v.2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funktionieren = gn.synset(u'funktionieren.v.2')\n",
    "funktionieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset(vorgehen.v.1), Synset(leerlaufen.v.2)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funktionieren.hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset(GNROOT.n.1),\n",
       "  Synset(Entität.n.2),\n",
       "  Synset(Objekt.n.4),\n",
       "  Synset(Ding.n.2),\n",
       "  Synset(Teil.n.2),\n",
       "  Synset(Teilmenge.n.2),\n",
       "  Synset(Gruppe.n.1),\n",
       "  Synset(biologische Gruppe.n.1),\n",
       "  Synset(Spezies.n.1),\n",
       "  Synset(Rasse.n.1),\n",
       "  Synset(Tierrasse.n.1),\n",
       "  Synset(Hunderasse.n.1),\n",
       "  Synset(Husky.n.1)],\n",
       " [Synset(GNROOT.n.1),\n",
       "  Synset(Entität.n.2),\n",
       "  Synset(kognitives Objekt.n.1),\n",
       "  Synset(Kategorie.n.1),\n",
       "  Synset(Art.n.1),\n",
       "  Synset(Spezies.n.1),\n",
       "  Synset(Rasse.n.1),\n",
       "  Synset(Tierrasse.n.1),\n",
       "  Synset(Hunderasse.n.1),\n",
       "  Synset(Husky.n.1)],\n",
       " [Synset(GNROOT.n.1),\n",
       "  Synset(Entität.n.2),\n",
       "  Synset(Objekt.n.4),\n",
       "  Synset(natürliches Objekt.n.1),\n",
       "  Synset(Kreatur.n.1),\n",
       "  Synset(Organismus.n.1),\n",
       "  Synset(höheres Lebewesen.n.1),\n",
       "  Synset(Tier.n.1),\n",
       "  Synset(Gewebetier.n.1),\n",
       "  Synset(Chordatier.n.1),\n",
       "  Synset(Wirbeltier.n.1),\n",
       "  Synset(Säugetier.n.1),\n",
       "  Synset(Plazentatier.n.1),\n",
       "  Synset(Raubtier.n.1),\n",
       "  Synset(Landraubtier.n.1),\n",
       "  Synset(hundeartiges Landraubtier.n.1),\n",
       "  Synset(Hund.n.2),\n",
       "  Synset(Husky.n.1)],\n",
       " [Synset(GNROOT.n.1),\n",
       "  Synset(Entität.n.2),\n",
       "  Synset(Objekt.n.4),\n",
       "  Synset(natürliches Objekt.n.1),\n",
       "  Synset(Kreatur.n.1),\n",
       "  Synset(Organismus.n.1),\n",
       "  Synset(höheres Lebewesen.n.1),\n",
       "  Synset(Tier.n.1),\n",
       "  Synset(Haustier.n.1),\n",
       "  Synset(Hund.n.2),\n",
       "  Synset(Husky.n.1)]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gn.synset('Husky.n.1').hypernym_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma(funktionieren.v.2.funktionieren),\n",
       " Lemma(funktionieren.v.2.funzen),\n",
       " Lemma(funktionieren.v.2.gehen),\n",
       " Lemma(funktionieren.v.2.laufen),\n",
       " Lemma(funktionieren.v.2.arbeiten)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funktionieren.lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma(brennen.v.1.brennen),\n",
       " Lemma(verbrennen.v.1.brennen),\n",
       " Lemma(brennen.v.3.brennen),\n",
       " Lemma(brennen.v.4.brennen),\n",
       " Lemma(brennen.v.5.brennen),\n",
       " Lemma(destillieren.v.1.brennen),\n",
       " Lemma(brennen.v.7.brennen),\n",
       " Lemma(brennen.v.8.brennen)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gn.lemmas('brennen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreas/bin/anaconda3/lib/python3.6/site-packages/numpy/core/records.py:847: FutureWarning: fromrecords expected a list of tuples, may have received a list of lists instead. In the future that will raise an error\n",
      "  return fromrecords(obj, dtype=dtype, shape=shape, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity computed on 65 of 65 word pairs\n",
      "\n",
      "metric   r\n",
      "---------------\n",
      "lch      0.778\n",
      "res      0.768\n",
      "jcn      -0.809\n",
      "lin      0.787\n"
     ]
    }
   ],
   "source": [
    "from pygermanet import load_germanet, Synset\n",
    "from scipy.stats.stats import pearsonr\n",
    "#import codecs\n",
    "import numpy as np\n",
    "\n",
    "GUR65_FILENAME = '../data/corpora/GermanRelatednessDatasets/gurevych_datasets/wortpaare65.gold.pos.txt'\n",
    "\n",
    "def load_gurevych():\n",
    "    gur65 = []\n",
    "    with open(GUR65_FILENAME, 'r') as input_file:\n",
    "        for idx, line in enumerate(input_file):\n",
    "            fields = line.strip().replace('#', '').split(':')\n",
    "            if idx == 0:\n",
    "                header = fields\n",
    "            else:\n",
    "                # fix typo in gur65\n",
    "                fields[1] = {'Reis': 'Reise'}.get(fields[1], fields[1])\n",
    "                fields[2] = float(fields[2])\n",
    "                gur65.append(fields)\n",
    "    gur65 = np.core.records.array(\n",
    "        gur65,\n",
    "        dtype=np.dtype({'formats': ['U30', 'U30', '<f8', 'U8', 'U8'],\n",
    "                        'names': header}))\n",
    "    return gur65\n",
    "\n",
    "gur65 = load_gurevych()\n",
    "gn    = load_germanet()\n",
    "\n",
    "# select those words which are found in GermaNet; exclude the\n",
    "# adjective \"jung\"\n",
    "pred = lambda w1, w2: bool(gn.synsets(w1) and gn.synsets(w2) and\n",
    "                           w1 != 'jung' and w2 != 'jung')\n",
    "\n",
    "print('Semantic similarity computed on {0} of {1} word pairs'.format(\n",
    "    sum([1 for word1, word2 in zip(gur65['Word1'], gur65['Word2'])\n",
    "         if pred(word1, word2)]),\n",
    "    len(gur65)))\n",
    "\n",
    "sim_funcs = [('lch', Synset.sim_lch,  np.max),\n",
    "             ('res', Synset.sim_res,  np.max),\n",
    "             ('jcn', Synset.dist_jcn, np.min),\n",
    "             ('lin', Synset.sim_lin,  np.max)]\n",
    "\n",
    "print()\n",
    "print('metric   r')\n",
    "print('---------------')\n",
    "for sim_name, sim_func, comb_func in sim_funcs:\n",
    "    scores = []\n",
    "    for word1, word2, human, po1, pos2 in gur65:\n",
    "        if not pred(word1, word2):\n",
    "            continue\n",
    "        score = comb_func(np.array([sim_func(ss1, ss2)\n",
    "                                    for ss1 in gn.synsets(word1)\n",
    "                                    for ss2 in gn.synsets(word2)]))\n",
    "        scores.append([score, human])\n",
    "    scores = np.array(scores)\n",
    "    r, _p = pearsonr(scores[:,0],scores[:,1])\n",
    "    print('{0}      {1:.3f}'.format(sim_name, r))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
