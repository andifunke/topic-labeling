{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import pandas as pd\n",
    "import hunspell\n",
    "from tabulate import tabulate\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- default constants definitions ---\n",
    "\n",
    "DATA_BASE = \"../../master_cloud/corpora\"\n",
    "ETL_BASE = \"preprocessed\"\n",
    "ETL_PATH = join(DATA_BASE, ETL_BASE)\n",
    "NLP_BASE = \"preprocessed/nlp\"\n",
    "NLP_PATH = join(DATA_BASE, NLP_BASE)\n",
    "\n",
    "# standard meta data fields\n",
    "DATASET = 'dataset'\n",
    "SUBSET = 'subset'\n",
    "ID = 'doc_id'\n",
    "ID2 = 'doc_subid'\n",
    "TITLE = 'title'\n",
    "TAGS = 'tags'\n",
    "TIME = 'date_time'\n",
    "#AUTHOR\n",
    "#SUBTITLE\n",
    "#CATEGORY\n",
    "META = [DATASET, SUBSET, ID, ID2, TITLE, TAGS, TIME]\n",
    "TEXT = 'text'\n",
    "HASH = 'hash'\n",
    "\n",
    "### --- additional constants\n",
    "\n",
    "# tags\n",
    "PUNCT = 'PUNCT'\n",
    "DET = 'DET'\n",
    "PHRASE = 'PHRASE'\n",
    "\n",
    "# keys\n",
    "IWNLP = 'IWNLP'\n",
    "POS = 'POS'\n",
    "INDEX = 'index'\n",
    "START = 'start'\n",
    "NOUN = 'NOUN'\n",
    "PROPN = 'PROPN'\n",
    "LEMMA = 'lemma'\n",
    "TAG = 'tag'\n",
    "STOP = 'stop'\n",
    "ENT_TYPE = 'ent_type'\n",
    "ENT_IOB = 'ent_iob'\n",
    "KNOWN = 'known'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- load spacy and iwnlp\n",
    "\n",
    "nlp = spacy.load('de')  # <-- load with dependency parser (slower)\n",
    "# nlp = spacy.load('de', disable=['parser'])\n",
    "\n",
    "from iwnlp.iwnlp_wrapper import IWNLPWrapper\n",
    "lemmatizer = IWNLPWrapper(lemmatizer_path='../data/IWNLP.Lemmatizer_20170501.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- definitions and reading for certain corpus\n",
    "\n",
    "LOCAL_PATH = ETL_BASE\n",
    "FULL_PATH = join(DATA_BASE, LOCAL_PATH)\n",
    "\n",
    "#CORPUS = \"OnlineParticipation\"\n",
    "#CORPUS = \"PoliticalSpeeches\"\n",
    "CORPUS = \"FAZ\"\n",
    "files = sorted([f for f in listdir(FULL_PATH) if isfile(join(FULL_PATH, f)) if f[:3] == CORPUS[:3]])\n",
    "\n",
    "for name in files:\n",
    "    fname = join(FULL_PATH, name)\n",
    "    # change here for multi-file corpora\n",
    "    df = read(fname)\n",
    "    print(fname)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, phrase_lookups = zip(*[tple for tple in process_docs(df[TEXT], size=None)])\n",
    "docs = pd.concat(docs).reset_index(drop=True)\n",
    "phrase_lookups = pd.concat(phrase_lookups).reset_index(drop=True)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = store(CORPUS+'_nlp', docs)\n",
    "fname = store(CORPUS+'_phrase_lookups', phrase_lookups)\n",
    "nlp.to_disk(join(NLP_PATH, 'spacy_model'))\n",
    "#nlp.vocab.to_disk(join(NLP_PATH, 'vocab'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
