{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snippets\n",
    "\n",
    "short scripts worth keeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import defaultdict\n",
    "from constants import *\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "pd.options.display.max_rows = 2001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## All corpora: Fix sent_idx in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_sent_idx(f, store=False):\n",
    "    print(\"read\", f)\n",
    "    df = pd.read_pickle(f)\n",
    "    columns = df.columns.tolist()\n",
    "    len_old = len(df)\n",
    "    size_old = df.size\n",
    "\n",
    "    # create a count for each document\n",
    "    df_group = pd.DataFrame(df.groupby(HASH).head(1)[HASH]).reset_index(drop=True)\n",
    "    df_group['count'] = df_group.index\n",
    "\n",
    "    # append count to original DataFrame\n",
    "    df = df.merge(df_group[[HASH, 'count']], on=HASH)\n",
    "\n",
    "    # add document count to sent_idx\n",
    "    df['sent_idx'] += df['count']\n",
    "    df = df[columns]\n",
    "    len_new = len(df)\n",
    "    size_new = df.size\n",
    "    assert len_old == len_new\n",
    "    assert size_old == size_new\n",
    "\n",
    "    if store:\n",
    "        print(\"write\", f)\n",
    "        df.to_pickle(f)\n",
    "\n",
    "    #return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply on nlp\n",
    "path = NLP_PATH\n",
    "number = None\n",
    "files = sorted([f for f in listdir(path) if isfile(join(path, f))], key=lambda s: s.lower())\n",
    "for name in files[:number]:\n",
    "    if name.startswith('dewac'):\n",
    "        gc.collect()\n",
    "        gc.collect()\n",
    "        f = join(path, name)\n",
    "        fix_sent_idx(f, store=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply on simple ... will not work. better redo simple\n",
    "path = SMPL_PATH\n",
    "number = 2\n",
    "files = sorted([f for f in listdir(path) if isfile(join(path, f))], key=lambda s: s.lower())\n",
    "for name in files[:number]:\n",
    "    f = join(path, name)\n",
    "    df = fix_sent_idx(f, store=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(HASH, sort=False).apply(lambda x: x.head(5).append(x.tail(5)))[[SENT_IDX, TOK_IDX, TOKEN]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Restoring original document hashes\n",
    "\n",
    "for *_simple files (after accidental dtype conversion during phrase extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = 'FAZ'\n",
    "f = join(NLP_PATH, corpus + '_nlp.pickle')\n",
    "dfn = pd.read_pickle(f)\n",
    "f = join(SMPL_PATH, corpus + '_simple.pickle')\n",
    "dfs = pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 2005\n",
    "dfn.loc[0:500, [HASH, SENT_IDX, TOK_IDX, TOKEN]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = dfs.rename(columns={'hash': 'hash_wrong'})\n",
    "dfs[0:300]\n",
    "# hm this is bad: first word of document is in wrong order - better redo simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfs_u = dfs[SENT_IDX].unique()\n",
    "dfng = dfn.groupby(['hash', SENT_IDX]).head(1)\n",
    "dfsg = dfs.groupby(['hash_wrong', SENT_IDX]).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfsg[['hash_wrong', SENT_IDX, POS]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1) joining on index: -> not working\n",
    "# 2) joining \n",
    "dfx = dfs.join(dfn[HASH])\n",
    "dfx['diff'] = (dfx['hash_wrong'] - dfx[HASH]).abs()\n",
    "dfx[[HASH, 'hash_wrong', 'diff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfx[dfx.POS == 'NPHRASE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## preprocess FAZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join FAZ and FAZ2 and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname1 = 'FAZ.pickle'\n",
    "fpath1 = join(ETL_PATH, fname1)\n",
    "df1 = pd.read_pickle(fpath1)\n",
    "fname2 = 'FAZ2.pickle'\n",
    "fpath2 = join(ETL_PATH, fname2)\n",
    "df2 = pd.read_pickle(fpath2)\n",
    "\n",
    "df1['new'] = False\n",
    "combined = pd.concat([df1, df2])\n",
    "c1 = combined\n",
    "c2 = combined[~combined.duplicated(subset=[TITLE])]\n",
    "c1.shape, c2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = c2\n",
    "del c1, c2, df1, df2\n",
    "df = df[~df.subset.isin(\n",
    "    [\n",
    "        'angst-in-chemnitz-und-arroganz-im-dfb-team-15762511.html',\n",
    "        'faz-net-sprinter-dumm-aber-sexy-15758998.html',\n",
    "        'faz-net-sprinter-gehoert-sachsen-noch-zu-deutschland-15760532.html',\n",
    "        'reise',\n",
    "        #'rhein-main',\n",
    "        'sport',\n",
    "        #'technik-motor',\n",
    "    ]\n",
    ")]\n",
    "df.groupby('subset').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find encrypted articles (FAZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inspect the first entries to create a training set\n",
    "df['str_len'] = df[TEXT].str.len()\n",
    "half = df.apply(lambda x: x[TEXT][x['str_len']//2:], axis=1)\n",
    "half.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 25 articles are ok\n",
    "trainset = df.iloc[:25][TEXT].tolist()\n",
    "char_freq_train = defaultdict(int)\n",
    "train_len = 0\n",
    "FACTOR = 0.001\n",
    "for string in trainset:\n",
    "    train_len += len(string)\n",
    "    for char in string.lower():\n",
    "        char_freq_train[char] += 1\n",
    "for k in char_freq_train.keys():\n",
    "    char_freq_train[k] //= (train_len * FACTOR)\n",
    "print(char_freq_train)\n",
    "\n",
    "def encryption_score(string):\n",
    "    distance = 0\n",
    "    doc_freq = defaultdict(int)\n",
    "    doc_len = len(string)//2\n",
    "    if doc_len == 0:\n",
    "        return 1000\n",
    "    string = string[doc_len:]\n",
    "    for char in string.lower():\n",
    "        doc_freq[char] += 1\n",
    "    for k in doc_freq.keys():\n",
    "        doc_freq[k] //= (doc_len * FACTOR)\n",
    "        distance += abs(doc_freq[k] - char_freq_train.get(k, 0))\n",
    "    return int(distance)\n",
    "    \n",
    "# compare scores for non-encrypted and encrypted articles\n",
    "good = df.iloc[-20:][TEXT].tolist()\n",
    "print([encryption_score(string) for string in good])\n",
    "bad = df.loc[5598736571409986456, TEXT]\n",
    "print(encryption_score(bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply metric on corpus\n",
    "df['enc_score'] = df[TEXT].apply(encryption_score)\n",
    "df[[TEXT, 'enc_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate results\n",
    "plt.scatter(df.index, df.enc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 300 seems to be a valid limit. We don't loose too many good articles here\n",
    "outlier1 = df.loc[df.enc_score.between(300, 350), TEXT].tolist()\n",
    "outlier2 = df.loc[df.enc_score.between(350, 550), TEXT].tolist()\n",
    "len(outlier1), len(outlier2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.enc_score <= 300]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['enc_score', 'str_len', 'new'], axis=1)\n",
    "df.to_pickle(join(ETL_PATH, 'FAZ_combined.pickle'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49758, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle(join(ETL_PATH, 'FAZ_combined.pickle')).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### balanced sample of dataset (FAZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = df.sample(frac=1, random_state=12345)  # shuffle DataFrame for a random sample\n",
    "dfx = dfx.groupby('subset').head(2000)\n",
    "dfx.groupby('subset').describe().enc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfx.subset.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx.subset.to_pickle(join(ETL_PATH, 'FAZ_document_sample3.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## preprocess FOCUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname1 = 'FOCUS_cleansed.pickle'\n",
    "fpath1 = join(ETL_PATH, fname1)\n",
    "df = pd.read_pickle(fpath1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find bad encoded articles (FOCUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 25 articles are ok\n",
    "trainset = df.iloc[:25][TEXT].tolist()\n",
    "char_freq_train = defaultdict(int)\n",
    "train_len = 0\n",
    "FACTOR = 0.001\n",
    "for string in trainset:\n",
    "    train_len += len(string)\n",
    "    for char in string.lower():\n",
    "        char_freq_train[char] += 1\n",
    "for k in char_freq_train.keys():\n",
    "    char_freq_train[k] //= (train_len * FACTOR)\n",
    "print(char_freq_train)\n",
    "\n",
    "def encryption_score(string):\n",
    "    distance = 0\n",
    "    doc_freq = defaultdict(int)\n",
    "    doc_len = len(string)//2\n",
    "    if doc_len == 0:\n",
    "        return 1000\n",
    "    string = string[doc_len:]\n",
    "    for char in string.lower():\n",
    "        doc_freq[char] += 1\n",
    "    for k in doc_freq.keys():\n",
    "        doc_freq[k] //= (doc_len * FACTOR)\n",
    "        distance += abs(doc_freq[k] - char_freq_train.get(k, 0))\n",
    "    return int(distance)\n",
    "    \n",
    "# compare scores for non-encrypted and encrypted articles\n",
    "good = df.iloc[-20:][TEXT].tolist()\n",
    "print([encryption_score(string) for string in good])\n",
    "#bad = df.loc[5598736571409986456, TEXT]\n",
    "#print(encryption_score(bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply metric on corpus\n",
    "df['enc_score'] = df[TEXT].apply(encryption_score)\n",
    "df[[TEXT, 'enc_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate results\n",
    "plt.scatter(df.index, df.enc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 300 seems to be a valid limit. We don't loose too many good articles here\n",
    "outlier1 = df.loc[df.enc_score.between(300, 500), TEXT].tolist()\n",
    "outlier2 = df.loc[df.enc_score.between(400, 3000), TEXT].tolist()\n",
    "len(outlier1), len(outlier2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('subset').describe().enc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.enc_score <= 300]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['enc_score'], axis=1)\n",
    "df.to_pickle(join(ETL_PATH, 'FOCUS_cleansed.pickle'))\n",
    "pd.read_pickle(join(ETL_PATH, 'FOCUS_cleansed.pickle')).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### balanced sample of dataset (FOCUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = df.sample(frac=1, random_state=12345)  # shuffle DataFrame for a random sample\n",
    "dfx = dfx.groupby('subset').head(2000)\n",
    "dfx.groupby('subset').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx.subset.to_pickle(join(ETL_PATH, 'FOCUS_document_sample3.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply cleansing to nlp dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = 'FOCUS_cleansed'\n",
    "f = join(ETL_PATH, corpus + '.pickle')\n",
    "dfetl = pd.read_pickle(f)\n",
    "corpus = 'FOCUS_nlp'\n",
    "f = join(NLP_PATH, corpus + '.pickle')\n",
    "dfnlp = pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove problematic formatted articles\n",
    "ev = np.unique(dfetl.index.values)\n",
    "nv = np.unique(dfnlp.hash.values)\n",
    "print(ev.size, nv.size)\n",
    "print((np.union1d(ev, nv)).size, (np.intersect1d(ev, nv)).size)\n",
    "\n",
    "dfnlpx = dfnlp[dfnlp.hash.isin(ev)]\n",
    "print(dfnlpx.shape, dfnlp.shape)\n",
    "print(dfnlpx.groupby(HASH).head(1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "corpus = 'FOCUS_cleansed_nlp'\n",
    "f = join(NLP_PATH, corpus + '.pickle')\n",
    "dfnlpx.to_pickle(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## preprocess Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia - phrase extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - applying phrases on simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodids = pd.read_pickle(join(ETL_PATH, 'dewiki_good_ids.pickle'))\n",
    "\n",
    "p = pd.read_pickle(join(ETL_PATH, 'dewiki_phrases_joined.pickle'))\n",
    "ps = set(p)\n",
    "bad = {\n",
    "    'ab', 'seit',\n",
    "}\n",
    "def ngrams(ser):\n",
    "    if ser[0] not in bad:\n",
    "        s = ser.str.cat(sep='_')\n",
    "        size = len(ser)\n",
    "        while size > 1:\n",
    "            if (s in ps):\n",
    "                return s, size\n",
    "            s = s.rsplit('_', 1)[0]\n",
    "            size -= 1\n",
    "    return np.nan, 0\n",
    "\n",
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'dewiki_\\d')\n",
    "files = sorted([f for f in listdir(SMPL_PATH)\n",
    "                if (isfile(join(SMPL_PATH, f)) and pattern.match(f))])\n",
    "\n",
    "for name in files[:]:\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    corpus = name.split('.')[0]\n",
    "    print(corpus, end=', ')\n",
    "    \n",
    "    f = join(SMPL_PATH, corpus + '.pickle')\n",
    "    df = pd.read_pickle(f)\n",
    "    df = df[df.hash.isin(goodids.index)]\n",
    "    df = df.reset_index(drop=True)\n",
    "    #df = df[:10_000]\n",
    "    df['__2'] = df.token.shift(-1)\n",
    "    df['__3'] = df.token.shift(-2)\n",
    "    df['__4'] = df.token.shift(-3)\n",
    "    df['__5'] = df.token.shift(-4)\n",
    "    d = df[[TOKEN, '__2', '__3', '__4', '__5']].progress_apply(ngrams, axis=1)\n",
    "    d = pd.DataFrame.from_records(d.tolist(), columns=['phrase', 'length'])\n",
    "    mask = ~d.phrase.isnull()\n",
    "    df = pd.concat([df, d], axis=1).drop(['__2', '__3', '__4', '__5'], axis=1)\n",
    "    df.loc[mask, TOKEN] = df.loc[mask, 'phrase']\n",
    "    df.loc[mask, POS] = 'NPHRASE'\n",
    "    lv = df.length.values\n",
    "    keep = np.ones_like(lv, dtype=bool)\n",
    "    length = len(keep)\n",
    "    for i, v in enumerate(lv):\n",
    "        if v > 0:\n",
    "            for j in range(i+1, min(i+v, length)):\n",
    "                if lv[j] == 0:\n",
    "                    keep[j] = False\n",
    "    df['keep'] = keep\n",
    "    df = df[df.keep].drop(['phrase', 'length', 'keep'], axis=1)\n",
    "    df.to_pickle(f+'_x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - learning phrases from titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titles = []\n",
    "\n",
    "def unique(ser):\n",
    "    return ser.dropna().str.strip().unique()\n",
    "\n",
    "f = 'dewiki_links.pickle'\n",
    "dfl = pd.read_pickle(join(ETL_PATH, f))\n",
    "f = 'dewiki_categories.pickle'\n",
    "dfc = pd.read_pickle(join(ETL_PATH, f))\n",
    "\n",
    "print(1, end=', ')\n",
    "titles.append(unique(dfl.norm))\n",
    "# do NOT use dfl.category -> it contains a couple of weird phrases\n",
    "print(2, end=', ')\n",
    "titles.append(unique(dfc.category))\n",
    "del dfc, dfl\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "\n",
    "pattern = re.compile(r'dewiki_\\d')\n",
    "files = sorted([f for f in listdir(ETL_PATH)\n",
    "                if (isfile(join(ETL_PATH, f)) and pattern.match(f))])\n",
    "\n",
    "for name in files[:]:\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    corpus = name.split('.')[0]\n",
    "    print(corpus, end=', ')\n",
    "    f = join(ETL_PATH, corpus + '.pickle')\n",
    "    titles.append(unique(pd.read_pickle(f)[TITLE]))\n",
    "\n",
    "print(titles)\n",
    "df = pd.DataFrame(np.unique(np.hstack(titles)), columns=['phrase'])\n",
    "del titles\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "\n",
    "print(df.head())\n",
    "df['length'] = df.phrase.apply(lambda x: x.count(' ') + 1)\n",
    "df = df[(df.length > 1) & (df.length < 6)]\n",
    "r = re.compile(r'^[a-zA-Z0-9_äöüÄÖÜß]')\n",
    "df = df[df.phrase.str.match(r)]\n",
    "\n",
    "df = df['phrase']\n",
    "print('expand')\n",
    "df = df.str.split(' ', expand=True)\n",
    "print('write')\n",
    "df.to_pickle(join(ETL_PATH, 'dewiki_phrases.pickle'))\n",
    "df = df.progress_apply(lambda x: x.str.cat(sep='_'), axis=1)\n",
    "df.to_pickle(join(ETL_PATH, 'dewiki_phrases_joined.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replace document hashes with updates (Wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(r'dewiki_\\d')\n",
    "files = sorted([f for f in listdir(ETL_PATH)\n",
    "                if (isfile(join(ETL_PATH, f)) and pattern.match(f))])\n",
    "\n",
    "mappings = []\n",
    "for name in files[:]:\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    corpus = name.split('.')[0]\n",
    "    print(corpus)\n",
    "    \n",
    "    f = join(NLP_PATH, corpus + '_nlp.pickle')\n",
    "    dfnlp = (\n",
    "        pd.read_pickle(f)\n",
    "        .groupby(HASH).head(1)[[HASH, TEXT]]\n",
    "        .reset_index()\n",
    "    )\n",
    "    f = join(ETL_PATH, corpus + '.pickle')\n",
    "    dfetl = (\n",
    "        pd.read_pickle(f)\n",
    "        .astype({\n",
    "            DATASET: \"category\",\n",
    "            SUBSET: \"category\", \n",
    "        })\n",
    "        .reset_index()\n",
    "        .join(dfnlp, rsuffix='_nlp')\n",
    "    )\n",
    "    dfjoin = dfetl[[HASH, TITLE, 'hash_nlp', 'text_nlp']].copy()\n",
    "    dfjoin['equal_start'] = dfjoin.apply(lambda x: x.title.startswith(x.text_nlp), axis=1)\n",
    "    #dfjoin['first_token'] = None\n",
    "    #dfjoin['first_token'] = dfjoin.title.str.split(expand=True)\n",
    "    #dfjoin['equal'] = (dfjoin.text_nlp == dfjoin.first_token)\n",
    "    assert dfjoin.equal_start.sum() == len(dfjoin)\n",
    "    dfetl = (\n",
    "        dfetl\n",
    "        .set_index('hash_nlp', drop=True)\n",
    "        .drop(['text_nlp', 'hash', 'index'], axis=1)\n",
    "    )\n",
    "    dfetl.to_pickle(f)\n",
    "    mappings.append(dfjoin[['hash', 'hash_nlp']])\n",
    "    \n",
    "dfmap = pd.concat(mappings)\n",
    "dfmap = dfmap.set_index(HASH)['hash_nlp']\n",
    "dfmap.to_pickle(join(ETL_PATH, 'dewiki_hashmap.pickle'))\n",
    "pd.read_pickle(join(ETL_PATH, 'dewiki_hashmap.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_docid(f):\n",
    "    print(f)\n",
    "    df = (\n",
    "        pd.read_pickle(join(ETL_PATH, f))\n",
    "        .join(dfmap, on='doc_id')\n",
    "        .drop('doc_id', axis=1)\n",
    "    )\n",
    "    df.to_pickle(join(ETL_PATH, f))\n",
    "\n",
    "replace_docid('dewiki_categories.pickle')\n",
    "replace_docid('dewiki_links.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find bad encoded articles (Wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'dewiki_\\d')\n",
    "files = sorted([f for f in listdir(ETL_PATH)\n",
    "                if (isfile(join(ETL_PATH, f)) and pattern.match(f))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# take samples from the corpus as training set\n",
    "samples = []\n",
    "for name in files[:]:\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    corpus = name.split('.')[0]\n",
    "    print(corpus, end=', ')\n",
    "    \n",
    "    f = join(ETL_PATH, corpus + '.pickle')\n",
    "    df = pd.read_pickle(f)\n",
    "    df = df.loc[df.subset == 'ARTICLE']\n",
    "    df[TEXT] = df.text.str.replace(pat=r'\\[\\]|\\]\\]', repl='')\n",
    "    sample_size = len(df) // 5_000\n",
    "    print(len(df), sample_size)\n",
    "    samples.append(df[[TITLE, TEXT]].sample(sample_size, random_state=1))\n",
    "\n",
    "samples = pd.concat(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove a few problematic documents from training set\n",
    "removable = [\n",
    "    841808810294450851,\n",
    "    -4692350076498843548,\n",
    "    -8049734301321653789,\n",
    "    -4131305296668036145,\n",
    "    -8810233646059586724,\n",
    "    2361495353241257464,\n",
    "    2496670190106332015,\n",
    "    7370294376993591077,\n",
    "    -7758054498192490695,\n",
    "    -4994773231110680494,\n",
    "    2192724857258762575,\n",
    "    1420892661644474111,\n",
    "]\n",
    "samples = samples[~samples.index.isin(removable)]\n",
    "trainset = df[TEXT].tolist()\n",
    "char_freq_train = defaultdict(int)\n",
    "train_len = 0\n",
    "FACTOR = 0.001\n",
    "for string in trainset:\n",
    "    train_len += len(string)\n",
    "    for char in string.lower():\n",
    "        char_freq_train[char] += 1\n",
    "for k in char_freq_train.keys():\n",
    "    char_freq_train[k] //= (train_len * FACTOR)\n",
    "print(char_freq_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing with FOCUS score\n",
    "doc_freq = {'b': 17.0, 'e': 134.0, 'i': 64.0, 'm': 20.0, ' ': 137.0, 'h': 35.0, 'z': 10.0, 'ö': 2.0, 'l': 33.0, '-': 4.0, 'k': 13.0, 'a': 47.0, 'u': 31.0, 'f': 13.0, 's': 53.0, 't': 52.0, 'd': 39.0, 'r': 58.0, 'c': 24.0, 'g': 22.0, 'n': 81.0, 'o': 26.0, '.': 9.0, 'w': 10.0, 'ü': 5.0, 'v': 7.0, ':': 1.0, ',': 6.0, 'p': 11.0, 'ä': 4.0, '8': 0.0, '1': 1.0, '2': 1.0, '0': 1.0, 'j': 1.0, '–': 0.0, 'ß': 1.0, '9': 0.0, '\\n': 3.0, 'y': 1.0, '6': 0.0, '5': 0.0, '„': 0.0, '“': 0.0, '?': 0.0, '’': 0.0, '7': 0.0, 'x': 0.0, '(': 0.0, ')': 0.0, '*': 0.0, '3': 0.0, '4': 0.0, 'q': 0.0, '&': 0.0, ';': 0.0, '\"': 0.0, '!': 0.0, '@': 0.0, '/': 0.0, \"'\": 0.0, '+': 0.0, 'é': 0.0}\n",
    "\n",
    "distance = 0\n",
    "for k in doc_freq.keys():\n",
    "    distance += abs(doc_freq[k] - char_freq_train.get(k, 0))\n",
    "int(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encryption_score(string):\n",
    "    distance = 0\n",
    "    doc_freq = defaultdict(int)\n",
    "    doc_len = len(string)//2\n",
    "    if doc_len == 0:\n",
    "        return 1000\n",
    "    string = string[doc_len:]\n",
    "    for char in string.lower():\n",
    "        doc_freq[char] += 1\n",
    "    for k in doc_freq.keys():\n",
    "        doc_freq[k] //= (doc_len * FACTOR)\n",
    "        distance += abs(doc_freq[k] - char_freq_train.get(k, 0))\n",
    "    return int(distance)\n",
    "\n",
    "good_ids = []\n",
    "for name in files[:]:\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    corpus = name.split('.')[0]\n",
    "    print(corpus, end=', ')\n",
    "    \n",
    "    f = join(ETL_PATH, corpus + '.pickle')\n",
    "    df = pd.read_pickle(f)\n",
    "    df = df[df.subset == 'ARTICLE']\n",
    "    df = df[df.description != 'Begriffsklärung']\n",
    "    df[TEXT] = df.text.str.replace(pat=r'\\[\\]|\\]\\]', repl='')\n",
    "    df = df[~(df.text == '')]\n",
    "    df['enc_score'] = df[TEXT].apply(encryption_score)\n",
    "    # keep articles with distance < 400\n",
    "    good_ids.append(df.loc[df.enc_score < 400, TITLE])\n",
    "\n",
    "good_ids = pd.concat(good_ids)\n",
    "good_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_ids.to_pickle(join(ETL_PATH, 'dewiki_good_ids.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[[TEXT, 'enc_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate results\n",
    "plt.scatter(df.index, df.enc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df.enc_score, bins=20)\n",
    "# would be multimodal if not restircted to ARTICLEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 400 seems to be a valid limit. We don't loose too many good articles here (~6 %)\n",
    "#df[df.enc_score.between(400, 500)][TEXT].tolist()\n",
    "len(df[df.enc_score > 400]) / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collecting links from Wikipedia in a separate DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# collecting links from Wikipedia in a separate DataFrame\n",
    "LINK_LIST = []\n",
    "TAGS_LIST = []\n",
    "\n",
    "def collect_links(df):\n",
    "    for doc_id, links in df[LINKS].iteritems():\n",
    "        for link in links:\n",
    "            LINK_LIST.append((doc_id, *link))\n",
    "\n",
    "def collect_tags(df):\n",
    "    for doc_id, categories in df[TAGS].iteritems():\n",
    "        for category in categories:\n",
    "            TAGS_LIST.append((doc_id, category))\n",
    "\n",
    "pattern = re.compile(r'^dewiki_\\d')\n",
    "files = sorted([f for f in listdir(FULL_PATH)\n",
    "                if isfile(join(FULL_PATH, f))\n",
    "                and pattern.match(f)\n",
    "               ])\n",
    "\n",
    "for name in files:\n",
    "    fname = join(ETL_PATH, name)\n",
    "    print(fname)\n",
    "    df = pd.read_pickle(fname)\n",
    "    collect_links(df)\n",
    "    collect_tags(df)\n",
    "\n",
    "del df\n",
    "df_links = pd.DataFrame.from_records(LINK_LIST, columns=['doc_id', 'link', 'norm', 'category'])\n",
    "del LINK_LIST\n",
    "df_tags = pd.DataFrame.from_records(TAGS_LIST, columns=['doc_id', 'category'])\n",
    "del TAGS_LIST\n",
    "df_links.to_pickle(join(ETL_PATH, 'dewiki_links.pickle'))\n",
    "df_tags.to_pickle(join(ETL_PATH, 'dewiki_categories.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting the largest files from the wikipedia corpus to parallelize the nlp pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the largest files from the wikipedia corpus to parallelize the nlp pipeline\n",
    "fname = 'dewiki_01.pickle'\n",
    "fpath = join(ETL_PATH, fname)\n",
    "df = pd.read_pickle(fpath)\n",
    "split = 25*1000\n",
    "#split *= 2\n",
    "dfs = [df[:1*split], \n",
    "       #df[1*split:], \n",
    "       df[1*split:2*split], \n",
    "       df[2*split:3*split], \n",
    "       df[3*split:]\n",
    "      ]\n",
    "length = sum([len(d) for d in dfs])\n",
    "assert len(df) == length\n",
    "df.shape, [d.shape for d in dfs]\n",
    "fsplit = fpath.rsplit('.', 1)\n",
    "for i, d in enumerate(dfs):\n",
    "    f = \"{}_{:d}.{}\".format(fsplit[0], i, fsplit[1])\n",
    "    print(f)\n",
    "    d.to_pickle(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## preprocess Dewac\n",
    "\n",
    "(unlike to the earlier cells, the following code assumes that the data is placed in 'preprocessed/tmp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'dewac_\\d')\n",
    "files = sorted([f for f in listdir(TMP_PATH)\n",
    "                if (isfile(join(TMP_PATH, f)) and pattern.match(f))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# take samples from the corpus as training set\n",
    "samples = []\n",
    "for name in files[:]:\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    corpus = name.split('.')[0]\n",
    "    print(corpus, end=', ')\n",
    "    \n",
    "    f = join(TMP_PATH, corpus + '.pickle')\n",
    "    df = pd.read_pickle(f)\n",
    "    #df = df.loc[df.subset == 'ARTICLE']\n",
    "    #df[TEXT] = df.text.str.replace(pat=r'\\[\\]|\\]\\]', repl='')\n",
    "    sample_size = len(df) // 5_000\n",
    "    print(len(df), sample_size)\n",
    "    samples.append(df[[TITLE, TEXT]].sample(sample_size, random_state=1))\n",
    "\n",
    "samples = pd.concat(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[TEXT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'1': 2.0, '7': 0.0, '.': 9.0, '0': 2.0, '5': 0.0, '2': 1.0, '4': 0.0, ',': 8.0, ' ': 138.0, ':': 1.0, 'l': 30.0, 'a': 45.0, 'n': 82.0, 'd': 39.0, 'b': 17.0, 'e': 131.0, 'r': 60.0, 'i': 65.0, 't': 51.0, 'u': 33.0, 'h': 35.0, 'm': 21.0, 'f': 14.0, 's': 52.0, 'o': 21.0, 'v': 7.0, '/': 0.0, 'x': 0.0, 'ß': 1.0, 'w': 11.0, 'z': 10.0, 'k': 11.0, 'p': 7.0, 'ü': 5.0, 'g': 25.0, 'ä': 4.0, 'c': 23.0, '&': 0.0, 'q': 0.0, ';': 0.0, 'j': 1.0, '(': 1.0, ')': 1.0, '-': 2.0, '8': 0.0, '9': 1.0, 'y': 0.0, '6': 0.0, '3': 0.0, 'ö': 2.0, '\"': 1.0, '=': 0.0, '\\x96': 0.0, '\\x84': 0.0, '\\x93': 0.0, '?': 0.0, '+': 0.0, '*': 0.0, '#': 0.0, '©': 0.0, '!': 0.0, '@': 0.0, '[': 0.0, ']': 0.0, '<': 0.0, '_': 0.0, '´': 0.0, '|': 0.0, '»': 0.0, '«': 0.0, \"'\": 0.0, '`': 0.0, 'é': 0.0, '>': 0.0, '%': 0.0, '^': 0.0, '°': 0.0, '\\x85': 0.0, '§': 0.0, '·': 0.0, '\\xad': 0.0, 'à': 0.0, '\\x9b': 0.0, '\\x8b': 0.0, 'â': 0.0, '\\x82': 0.0, '¬': 0.0, 'è': 0.0, 'î': 0.0, '\\x80': 0.0, 'ó': 0.0, '\\x94': 0.0, '\\t': 0.0, '\\x99': 0.0, '\\x98': 0.0, '\\x9e': 0.0, '\\x9c': 0.0, 'á': 0.0, '®': 0.0, 'ÿ': 0.0, 'ñ': 0.0, '~': 0.0, 'ú': 0.0, '\\x91': 0.0, '\\\\': 0.0, '½': 0.0, 'ç': 0.0, 'ë': 0.0, 'ø': 0.0, '\\xa0': 0.0, 'ï': 0.0, '¿': 0.0, 'í': 0.0, '×': 0.0, '\\x95': 0.0, 'ì': 0.0, 'ý': 0.0, '}': 0.0, 'ê': 0.0, '\\x97': 0.0, '\\x92': 0.0, '$': 0.0, 'ô': 0.0, '¦': 0.0, 'µ': 0.0, '¾': 0.0, 'ò': 0.0, '¹': 0.0, '\\x8c': 0.0, '¯': 0.0, '¸': 0.0, '\\x87': 0.0, '\\x86': 0.0, '²': 0.0, 'å': 0.0, 'ã': 0.0, '³': 0.0, 'õ': 0.0, '¼': 0.0, '¨': 0.0, 'û': 0.0, '{': 0.0, '\\x9a': 0.0, '\\x89': 0.0, '\\x8a': 0.0, '\\x9f': 0.0, '\\x81': 0.0, '¢': 0.0, '\\x9d': 0.0, 'º': 0.0, 'þ': 0.0, '¤': 0.0, '\\x7f': 0.0, '±': 0.0, 'ù': 0.0, 'æ': 0.0, '¶': 0.0, '¡': 0.0, '£': 0.0, '\\x1f': 0.0, '÷': 0.0, 'ð': 0.0, '\\x8e': 0.0, 'ª': 0.0, '\\x88': 0.0, '\\x90': 0.0, '\\x8d': 0.0, '\\n': 0.0, '\\x8f': 0.0, '\\x83': 0.0, '¥': 0.0})\n"
     ]
    }
   ],
   "source": [
    "trainset = df[TEXT].tolist()\n",
    "char_freq_train = defaultdict(int)\n",
    "train_len = 0\n",
    "FACTOR = 0.001\n",
    "for string in trainset:\n",
    "    train_len += len(string)\n",
    "    for char in string.lower():\n",
    "        char_freq_train[char] += 1\n",
    "for k in char_freq_train.keys():\n",
    "    char_freq_train[k] //= (train_len * FACTOR)\n",
    "print(char_freq_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparing with FOCUS score\n",
    "doc_freq = {'b': 17.0, 'e': 134.0, 'i': 64.0, 'm': 20.0, ' ': 137.0, 'h': 35.0, 'z': 10.0, 'ö': 2.0, 'l': 33.0, '-': 4.0, 'k': 13.0, 'a': 47.0, 'u': 31.0, 'f': 13.0, 's': 53.0, 't': 52.0, 'd': 39.0, 'r': 58.0, 'c': 24.0, 'g': 22.0, 'n': 81.0, 'o': 26.0, '.': 9.0, 'w': 10.0, 'ü': 5.0, 'v': 7.0, ':': 1.0, ',': 6.0, 'p': 11.0, 'ä': 4.0, '8': 0.0, '1': 1.0, '2': 1.0, '0': 1.0, 'j': 1.0, '–': 0.0, 'ß': 1.0, '9': 0.0, '\\n': 3.0, 'y': 1.0, '6': 0.0, '5': 0.0, '„': 0.0, '“': 0.0, '?': 0.0, '’': 0.0, '7': 0.0, 'x': 0.0, '(': 0.0, ')': 0.0, '*': 0.0, '3': 0.0, '4': 0.0, 'q': 0.0, '&': 0.0, ';': 0.0, '\"': 0.0, '!': 0.0, '@': 0.0, '/': 0.0, \"'\": 0.0, '+': 0.0, 'é': 0.0}\n",
    "\n",
    "distance = 0\n",
    "for k in doc_freq.keys():\n",
    "    distance += abs(doc_freq[k] - char_freq_train.get(k, 0))\n",
    "int(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encryption_score(string):\n",
    "    distance = 0\n",
    "    doc_freq = defaultdict(int)\n",
    "    doc_len = len(string)//2\n",
    "    if doc_len == 0:\n",
    "        return 1000\n",
    "    string = string[doc_len:]\n",
    "    for char in string.lower():\n",
    "        doc_freq[char] += 1\n",
    "    for k in doc_freq.keys():\n",
    "        doc_freq[k] //= (doc_len * FACTOR)\n",
    "        distance += abs(doc_freq[k] - char_freq_train.get(k, 0))\n",
    "    return int(distance)\n",
    "\n",
    "good_ids = []\n",
    "bad = []\n",
    "for name in files[:]:\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    corpus = name.split('.')[0]\n",
    "    print(corpus, end=', ')\n",
    "    \n",
    "    f = join(TMP_PATH, corpus + '.pickle')\n",
    "    df = pd.read_pickle(f)\n",
    "    #df = df.loc[df.subset == 'ARTICLE']\n",
    "    #df[TEXT] = df.text.str.replace(pat=r'\\[\\]|\\]\\]', repl='')\n",
    "    df['enc_score'] = df[TEXT].apply(encryption_score)\n",
    "    # keep articles with distance < 400\n",
    "    good_ids.append(df.loc[df.enc_score < 400, TITLE])\n",
    "    bad.append(df[df.enc_score >= 400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002495617542615866"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad = pd.concat(bad)\n",
    "good_ids = pd.concat(good_ids)\n",
    "good_ids.to_pickle(join(TMP_PATH, 'dewac_good_ids.pickle'))\n",
    "len(bad) / (len(good_ids)+len(bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bad[TEXT].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f549172f9e8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate results\n",
    "plt.scatter(df.index, df.enc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.1291e+04, 2.6785e+04, 8.8010e+03, 1.3570e+03, 2.2700e+02,\n",
       "        6.5000e+01, 2.3000e+01, 7.0000e+00, 1.0000e+00, 2.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00]),\n",
       " array([  39. ,  117.3,  195.6,  273.9,  352.2,  430.5,  508.8,  587.1,\n",
       "         665.4,  743.7,  822. ,  900.3,  978.6, 1056.9, 1135.2, 1213.5,\n",
       "        1291.8, 1370.1, 1448.4, 1526.7, 1605. ]),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df.enc_score, bins=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
