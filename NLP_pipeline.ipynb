{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import pandas as pd\n",
    "import hunspell\n",
    "from tabulate import tabulate\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "\n",
    "\n",
    "# pretty print of dataframes on the console (slightly better in PyCharm notebooks than default print)\n",
    "#def tprint(df: pd.DataFrame, head=0):\n",
    "#    if head > 0:\n",
    "#        df = df.head(head)\n",
    "#    elif head < 0:\n",
    "#        df = df.tail(-head)\n",
    "#    print(tabulate(df, headers=\"keys\", tablefmt=\"pipe\") + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- default constants definitions ---\n",
    "\n",
    "DATA_BASE = \"../../master_cloud/corpora\"\n",
    "ETL_BASE = \"preprocessed\"\n",
    "ETL_PATH = join(DATA_BASE, ETL_BASE)\n",
    "NLP_BASE = \"preprocessed/nlp\"\n",
    "NLP_PATH = join(DATA_BASE, NLP_BASE)\n",
    "\n",
    "# standard meta data fields\n",
    "DATASET = 'dataset'\n",
    "SUBSET = 'subset'\n",
    "ID = 'doc_id'\n",
    "ID2 = 'doc_subid'\n",
    "TITLE = 'title'\n",
    "TAGS = 'tags'\n",
    "TIME = 'date_time'\n",
    "#AUTHOR\n",
    "#SUBTITLE\n",
    "#CATEGORY\n",
    "META = [DATASET, SUBSET, ID, ID2, TITLE, TAGS, TIME]\n",
    "TEXT = 'text'\n",
    "HASH = 'hash'\n",
    "\n",
    "### --- additional constants\n",
    "\n",
    "# tags\n",
    "PUNCT = 'PUNCT'\n",
    "DET = 'DET'\n",
    "PHRASE = 'PHRASE'\n",
    "\n",
    "# keys\n",
    "IWNLP = 'IWNLP'\n",
    "POS = 'POS'\n",
    "INDEX = 'index'\n",
    "START = 'start'\n",
    "NOUN = 'NOUN'\n",
    "PROPN = 'PROPN'\n",
    "LEMMA = 'lemma'\n",
    "TAG = 'tag'\n",
    "STOP = 'stop'\n",
    "ENT_TYPE = 'ent_type'\n",
    "ENT_IOB = 'ent_iob'\n",
    "KNOWN = 'known'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- load spacy and iwnlp\n",
    "\n",
    "nlp = spacy.load('de')  # <-- load with dependency parser (slower)\n",
    "# nlp = spacy.load('de', disable=['parser'])\n",
    "\n",
    "from iwnlp.iwnlp_wrapper import IWNLPWrapper\n",
    "lemmatizer = IWNLPWrapper(lemmatizer_path='../data/IWNLP.Lemmatizer_20170501.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- function definitions ---\n",
    "\n",
    "def process_phrases(doc_):\n",
    "    \"\"\" \n",
    "        given a doc process and return the contained noun phrases.\n",
    "        This function is based on spacy's noun chunk detection. \n",
    "        It also creates items for a global phrase lookup table, which are currently not used.\n",
    "    \"\"\"\n",
    "\n",
    "    # clean the noun chuncs from spacy first\n",
    "    noun_chunks = []\n",
    "    for chunk in doc_.noun_chunks:\n",
    "        start = False\n",
    "        noun_chunk = []\n",
    "        for token in chunk:\n",
    "            # exclude punctuation\n",
    "            if token.pos_ == PUNCT:\n",
    "                continue\n",
    "            # exclude leading determiners\n",
    "            if not start and (token.pos_ == DET or token.is_stop):\n",
    "                continue\n",
    "            start = True\n",
    "            noun_chunk.append(token)\n",
    "        if len(noun_chunk) > 1:\n",
    "            noun_chunks.append(noun_chunk)\n",
    "    \n",
    "    # the remaining, adjusted noun chunks will be lemmatized and indexed\n",
    "    phrase_list_lookup = []\n",
    "    phrase_list_doc = []\n",
    "    for chunk in noun_chunks:\n",
    "        phrase = []\n",
    "        for token in chunk:\n",
    "            lemma, _ = lemmatize(token.text, token.pos_)\n",
    "            if lemma:\n",
    "                phrase.append(lemma)\n",
    "            else:\n",
    "                phrase.append(token.text)\n",
    "        phrase = ' '.join(phrase)\n",
    "        text = ' '.join([t.text for t in chunk])\n",
    "        \n",
    "        # add to phrase collection of corpus\n",
    "        phrase_lookup = pd.Series()\n",
    "        phrase_lookup['lemmatized'] = phrase\n",
    "        phrase_lookup['original'] = text\n",
    "        #phrase_lookup['Spacy Tokens'] = tuple(chunk)\n",
    "        phrase_list_lookup.append(phrase_lookup)\n",
    "        \n",
    "        # add to document dataframe\n",
    "        phrase_series = pd.Series()\n",
    "        phrase_series[TEXT] = text\n",
    "        phrase_series[IWNLP] = phrase\n",
    "        phrase_series[POS] = PHRASE\n",
    "        phrase_series[INDEX] = chunk[0].i\n",
    "        phrase_series[START] = chunk[0].idx\n",
    "        phrase_list_doc.append(phrase_series)\n",
    "\n",
    "    # return the dataframes and for the doc dataframe and for the global phrase lookup table\n",
    "    return pd.DataFrame(phrase_list_doc), pd.DataFrame(phrase_list_lookup)\n",
    "\n",
    "\n",
    "def lemmatize(token: str, pos: str) -> (str, bool):\n",
    "    \"\"\" \n",
    "    This function uses the IWNLP lemmatizer with a few enhancements for compund nouns and nouns \n",
    "    with uncommon capitalization. Can also be used to lemmatize tokens with different POS-tags.\n",
    "    Do not use this function to lemmatize phrases.\n",
    "    :param token: white space stripped single token (str)\n",
    "    :param pos:   string constant, one of Universal tagset.\n",
    "    :return: tuple of type (str, bool)\n",
    "           value[0]: The lemma of the token if a lemma can be derived, else None.\n",
    "           value[1]: True if the token can be retrieved from the Wiktionary database as is, else False.\n",
    "    \"\"\"\n",
    "    \n",
    "    if pos == PHRASE:\n",
    "        try:\n",
    "            raise ValueError\n",
    "        except ValueError:\n",
    "            print(\"Don't lemmatize Phrases with this function!\")\n",
    "    \n",
    "    lemm = lemmatizer.lemmatize(token, pos)\n",
    "    # default lemmatization ok?\n",
    "    if lemm:\n",
    "        return lemm[0], True\n",
    "\n",
    "    # some rules to derive a lemma from the original token (nouns only)\n",
    "    # TODO: define rules for hyphenated nouns\n",
    "    if pos == NOUN or pos == PROPN:\n",
    "        # first try default noun capitalization\n",
    "        lemm = lemmatizer.lemmatize(token.title(), pos)\n",
    "        if lemm:\n",
    "            return lemm[0], False\n",
    "\n",
    "    # still no results: try noun suffixes\n",
    "        for i in range(1, len(token)-1):\n",
    "            token_edit = token[i:].title()\n",
    "            lemm = lemmatizer.lemmatize_plain(token_edit, ignore_case=True)\n",
    "            if lemm:\n",
    "                lemm = lemm[0]\n",
    "                lemm = token[:i].title() + lemm.lower()\n",
    "                return lemm, False\n",
    "    \n",
    "    # sorry, no results found:\n",
    "    return None, False\n",
    "\n",
    "\n",
    "def essence_from_doc(doc_, key):\n",
    "    \"\"\"\n",
    "    Creates a pandas DataFrame from a given spacy.doc that contains only nouns and noun phrases.\n",
    "    :param doc_: spacy.doc \n",
    "    :return:     pandas.DataFrame\n",
    "    \"\"\"\n",
    "    tags = [\n",
    "        (\n",
    "         token.text, token.lemma_, token.pos_, token.tag_, token.is_stop,\n",
    "         token.i, token.idx,\n",
    "         token.ent_type_, token.ent_iob_, # token.ent_id_,\n",
    "         ) for token in doc_ ]\n",
    "    df_ = pd.DataFrame(tags)\n",
    "    df_ = df_.rename(columns={k:v for k,v in enumerate([\n",
    "          TEXT, LEMMA, POS, TAG, STOP, INDEX, START, ENT_TYPE, ENT_IOB,\n",
    "          #\"Dep\", \"Shape\", \"alpha\", \"Ent_id\"  # currently not used :(\n",
    "    ])})\n",
    "    \n",
    "    # add IWNLP lemmatization\n",
    "    df_[IWNLP], df_[KNOWN] = zip(*df_.apply(lambda row: lemmatize(row[TEXT], row[POS]), axis=1))\n",
    "    \n",
    "    # add phrases\n",
    "    df_phrases, phrase_lookup = process_phrases(doc_)\n",
    "    df_ = df_.append(df_phrases).sort_values(START)\n",
    "    df_ = df_[df_.POS.isin([NOUN, PROPN, PHRASE])].reset_index(drop=True)\n",
    "    \n",
    "    # replace Text with lemmatization, if lemmatization exists\n",
    "    mask = ~df_[IWNLP].isnull()\n",
    "    df_.loc[mask, TEXT] = df_.loc[mask, IWNLP]\n",
    "    \n",
    "    # add hash-key\n",
    "    df_[HASH] = key\n",
    "    \n",
    "    return df_[[HASH, INDEX, TEXT, POS]], phrase_lookup\n",
    "\n",
    "\n",
    "def process_docs(series, size=None):\n",
    "    \"\"\" main function for sending the dataframes from the ETL pipeline to the NLP pipeline \"\"\"\n",
    "    for k, v in series[:size].iteritems():\n",
    "        # build spacy doc\n",
    "        doc = nlp(v)\n",
    "        essential_token, phrase_lookup = essence_from_doc(doc, key=k)\n",
    "        yield essential_token, phrase_lookup\n",
    "        \n",
    "        \n",
    "def store(corpus, df):\n",
    "    \"\"\"returns the file path where the dataframe was stores\"\"\"\n",
    "    makedirs(NLP_PATH, exist_ok=True)\n",
    "    fname = join(NLP_PATH, corpus + '.pickle')\n",
    "    print('saving to', fname)\n",
    "    df.to_pickle(fname)\n",
    "    return fname\n",
    "\n",
    "\n",
    "def read(f):\n",
    "    \"\"\" reads a dataframe from pickle format \"\"\"\n",
    "    return pd.read_pickle(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../master_cloud/corpora/preprocessed/FAZ.pickle\n"
     ]
    }
   ],
   "source": [
    "# --- definitions and reading for certain corpus\n",
    "\n",
    "LOCAL_PATH = ETL_BASE\n",
    "FULL_PATH = join(DATA_BASE, LOCAL_PATH)\n",
    "\n",
    "#CORPUS = \"OnlineParticipation\"\n",
    "#CORPUS = \"PoliticalSpeeches\"\n",
    "CORPUS = \"FAZ\"\n",
    "files = sorted([f for f in listdir(FULL_PATH) if isfile(join(FULL_PATH, f)) if f[:3] == CORPUS[:3]])\n",
    "\n",
    "for name in files:\n",
    "    fname = join(FULL_PATH, name)\n",
    "    # change here for multi-file corpora\n",
    "    df = read(fname)\n",
    "    print(fname)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, phrase_lookups = zip(*[tple for tple in process_docs(df[TEXT], size=None)])\n",
    "docs = pd.concat(docs).reset_index(drop=True)\n",
    "phrase_lookups = pd.concat(phrase_lookups).reset_index(drop=True)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to ../../master_cloud/corpora/preprocessed/nlp/FAZ_nlp.pickle\n",
      "saving to ../../master_cloud/corpora/preprocessed/nlp/FAZ_phrase_lookups.pickle\n"
     ]
    }
   ],
   "source": [
    "fname = store(CORPUS+'_nlp', docs)\n",
    "fname = store(CORPUS+'_phrase_lookups', phrase_lookups)\n",
    "nlp.to_disk(join(NLP_PATH, 'spacy_model'))\n",
    "#nlp.vocab.to_disk(join(NLP_PATH, 'vocab'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "Not shure if I want to use these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    ents.append((ent.text, ent.start_char, ent.end_char, ent.label_))\n",
    "    \n",
    "df_ent = pd.DataFrame(ents)\n",
    "df_ent = df_ent.rename(columns=\n",
    "                       {0: \"Text\", 1: \"Start\", 2: \"End\", 3: \"Label\", \n",
    "                        4: \"Description\"})\n",
    "\n",
    "tprint(df_ent, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_lookup import Entity\n",
    "\n",
    "#nlp = spacy.load('en')\n",
    "entity = Entity(nlp, keywords_list=['python', 'java platform'])\n",
    "nlp.add_pipe(entity, last=True)\n",
    "\n",
    "doc = nlp(u\"I am a product manager for a java and python.\")\n",
    "assert doc._.has_entities == True\n",
    "assert doc[2:5]._.has_entities == True\n",
    "assert doc[0]._.is_entity == False\n",
    "assert doc[3]._.is_entity == True\n",
    "print(doc._.entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spellchecking\n",
    "# may be used to correct case errors -> very limited usage\n",
    "\n",
    "spellchecker = hunspell.HunSpell('/usr/share/hunspell/de_DE.dic', \n",
    "                                 '/usr/share/hunspell/de_DE.aff')\n",
    "enc = spellchecker.get_dic_encoding()  # 'ISO8859-1' might be an issue\n",
    "\n",
    "df_noun = df_doc[df_doc.POS == 'NOUN'].copy()\n",
    "df_noun['Spell'] = \\\n",
    "    df_noun['Text'].map(lambda noun: spellchecker.spell(noun))\n",
    "df_noun['Suggest'] = \\\n",
    "    df_noun['Text'].map(lambda noun: spellchecker.suggest(noun))\n",
    "\n",
    "tprint(df_noun[['Text', 'Spell', 'Suggest']])\n",
    "\n",
    "## Alternative:\n",
    "from spacy_hunspell import spaCyHunSpell\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "hunspell = spaCyHunSpell(nlp, 'mac')\n",
    "nlp.add_pipe(hunspell)\n",
    "doc = nlp('I can haz cheezeburger.')\n",
    "haz = doc[2]\n",
    "haz._.hunspell_spell  # False\n",
    "haz._.hunspell_suggest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
