{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- default imports ---\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join\n",
    "from datetime import datetime\n",
    "import re\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "from html import unescape\n",
    "\n",
    "from constants import DATA_BASE, ETL_PATH, NLP_PATH, LOCL_PATH, FULL_PATH, \\\n",
    "    DATASET, SUBSET, ID, ID2, TITLE, TIME, META, TEXT, DESCR, LINKS, TAGS, DATA, HASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- store meta data and content ---\n",
    "\n",
    "def store(corpus, df):\n",
    "    \"\"\"returns the file name where the dataframe was stores\"\"\"\n",
    "    makedirs(ETL_PATH, exist_ok=True)\n",
    "    fname = join(ETL_PATH, corpus + '.pickle')\n",
    "    print('saving to', fname)\n",
    "    df.to_pickle(fname)\n",
    "    return fname\n",
    "\n",
    "def read(f):\n",
    "    return pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = store(CORPUS, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ, PROCESS and *STORE*\n",
    "\n",
    "files = sorted([f for f in listdir(FULL_PATH) if isfile(join(FULL_PATH, f)) \n",
    "                and f[:3] == 'dew'\n",
    "               ])\n",
    "\n",
    "for name in files:\n",
    "    fname = join(FULL_PATH, name)\n",
    "    df = read(fname)\n",
    "    df['doc_id'] = df['doc_id'].str.strip()\n",
    "    df['title']  = df['title'].str.strip()\n",
    "    print('saving to', fname)\n",
    "    #df.to_pickle(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines for certain corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- extract, transform, load (save) the following corpus:\n",
    "\n",
    "CORPUS = \"OnlineParticipation\"\n",
    "LOCL_PATH = \"OnlineParticipationDatasets/downloads\"\n",
    "FULL_PATH = join(DATA_BASE, LOCL_PATH)\n",
    "\n",
    "def transform_subset(source, subset_name: str):\n",
    "    \"\"\"\n",
    "    :param source: list of dictionaries in original key/value format\n",
    "    :param subset_name: string identifier of the subset the data belongs to\n",
    "    :return: list of dictionaries in standard key/value format\n",
    "    \"\"\"\n",
    "    category_lookup = {}\n",
    "    print('transform', subset_name)\n",
    "\n",
    "    for doc in source:\n",
    "        target = dict()\n",
    "        target[DATASET] = CORPUS\n",
    "        target[SUBSET]  = subset_name\n",
    "        target[ID]      = doc['suggestion_id']\n",
    "        target[TITLE]   = doc['title']\n",
    "        target[TIME]    = doc['date_time']\n",
    "        target[DESCR]   = None\n",
    "\n",
    "        # wuppertal has a different data scheme\n",
    "        if subset_name == 'wuppertal2017':\n",
    "            if 'tags' in doc:\n",
    "                target[TAGS] = tuple(doc['tags'])\n",
    "                category_lookup[target[ID]] = target[TAGS]\n",
    "            else:\n",
    "                target[TAGS] = category_lookup[target[ID]]\n",
    "            target[ID2]      = None\n",
    "            target[TEXT]     = doc['content'] + ' .\\n' \\\n",
    "                             + doc['Voraussichtliche Rolle für die Stadt Wuppertal'] + ' .\\n' \\\n",
    "                             + doc['Mehrwert der Idee für Wuppertal'] + ' .\\n'\n",
    "                           # + doc['Eigene Rolle bei der Projektidee'] + ' .\\n'\n",
    "                           # + doc['Geschätzte Umsetzungsdauer und Startschuss'] + ' .\\n'\n",
    "                           # + doc['Kostenschätzung der Ideeneinreicher'] + ' .\\n'\n",
    "        else:\n",
    "            if 'category' in doc:\n",
    "                target[TAGS] = doc['category']\n",
    "                category_lookup[target[ID]] = target[TAGS]\n",
    "            else:\n",
    "                target[TAGS] = category_lookup[target[ID]]\n",
    "            target[ID2]      = doc['comment_id'] if ('comment_id' in doc) else 0\n",
    "            target[LINKS]    = target[ID] if target[ID2] else None\n",
    "            # ignore if no content\n",
    "            if not doc['content']:\n",
    "                continue\n",
    "            target[TEXT]     = doc['content']\n",
    "        \n",
    "        target[HASH] = hash(tuple([target[key] for key in META]))\n",
    "        yield target\n",
    "\n",
    "\n",
    "def load_data(number_of_subsets: int=None, start: int=0):\n",
    "    \"\"\"\n",
    "    :param number_of_subsets: number of subsets to process in one call (None for no limit)\n",
    "    :param start: index of first subset to process\n",
    "    :yield: data set name, data subset name, data json\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"process\", CORPUS)\n",
    "    \n",
    "    # --- read files ---\n",
    "    files = [f for f in listdir(FULL_PATH) if isfile(join(FULL_PATH, f))]\n",
    "\n",
    "    if number_of_subsets:\n",
    "        number_of_subsets += start\n",
    "        if number_of_subsets > len(files):\n",
    "            number_of_subsets = None\n",
    "\n",
    "    for name in files[start:number_of_subsets]:\n",
    "        if name[-9:-5] != 'flat':\n",
    "            continue\n",
    "        \n",
    "        fpath = join(FULL_PATH, name)\n",
    "        try: \n",
    "            with open(fpath, 'r') as fp:\n",
    "                print('open:', fpath)\n",
    "                data = json.load(fp)\n",
    "                if not data:\n",
    "                    continue\n",
    "        except IOError:\n",
    "            print(\"Could not open\", fpath)\n",
    "            continue\n",
    "        subset = name[6:-10]\n",
    "\n",
    "        yield transform_subset(data, subset)\n",
    "\n",
    "\n",
    "dfs = [pd.DataFrame(item) for item in load_data()]\n",
    "if dfs:\n",
    "    df = pd.concat(dfs)\n",
    "    del dfs\n",
    "    df = df.set_index(HASH)[META+DATA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- extract, transform, load (save) the following corpus:\n",
    "\n",
    "CORPUS = [\"FAZ\", \"FOCUS\"][0]\n",
    "LOCL_PATH = \"scrapy/\" + CORPUS.lower()\n",
    "FULL_PATH = join(DATA_BASE, LOCL_PATH)\n",
    "\n",
    "depth = {\n",
    "    \"FAZ\": 4,\n",
    "    \"FOCUS\": 3,\n",
    "}\n",
    "\n",
    "re_date = re.compile(r'^(.*?\\+\\d\\d):(\\d\\d)$')\n",
    "\n",
    "def transform_subset(source, subset_name: str):\n",
    "    \"\"\"\n",
    "    :param source: list of dictionaries in original key/value format\n",
    "    :param subset_name: string identifier of the subset the data belongs to\n",
    "    :return: list of dictionaries in standard key/value format\n",
    "    \"\"\"\n",
    "    print('transform', subset_name)\n",
    "\n",
    "    for doc in source:\n",
    "        target = dict()\n",
    "        target[DATASET]  = CORPUS\n",
    "        target[SUBSET]   = doc['url'].split('/')[depth[CORPUS]]\n",
    "        target[ID]       = doc['url']\n",
    "        target[ID2]      = None\n",
    "        target[TITLE]    = doc['title']\n",
    "        target[LINKS]    = None\n",
    "        if doc['published']:\n",
    "            date = re_date.sub(r'\\g<1>\\g<2>', doc['published'])\n",
    "            target[TIME] = datetime.strptime(date, '%Y-%m-%dT%H:%M:%S%z')  \n",
    "                         # FAZ:   2018-08-22T11:11:45+0200\n",
    "                         # FOCUS: 2013-02-15T12:10:49+01:00\n",
    "        else: \n",
    "            target[TIME] = None\n",
    "        target[TEXT]     = doc['text']\n",
    "        target[DESCR]    = doc['description']\n",
    "        target[TAGS]     = tuple(doc['keywords'])\n",
    "\n",
    "        target[HASH] = hash(tuple([target[key] for key in META]))\n",
    "        yield target\n",
    "\n",
    "\n",
    "def load_data(number_of_subsets: int=None, start: int=0):\n",
    "    \"\"\"\n",
    "    :param number_of_subsets: number of subsets to process in one call (None for no limit)\n",
    "    :param start: index of first subset to process\n",
    "    :yield: data set name, data subset name, data json\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"process\", CORPUS)\n",
    "    \n",
    "    # --- read files ---\n",
    "    files = [f for f in listdir(FULL_PATH) if isfile(join(FULL_PATH, f))]\n",
    "    print(files)\n",
    "\n",
    "    if number_of_subsets:\n",
    "        number_of_subsets += start\n",
    "        if number_of_subsets > len(files):\n",
    "            number_of_subsets = None\n",
    "\n",
    "    for name in files[start:number_of_subsets]:\n",
    "        fpath =join(FULL_PATH, name)\n",
    "        try: \n",
    "            with open(fpath, 'r') as fp:\n",
    "                print('open:', fpath)\n",
    "                data = [json.loads(d) for d in fp.readlines()]\n",
    "                if not data:\n",
    "                    continue\n",
    "        except IOError:\n",
    "            print(\"Could not open\", fpath)\n",
    "            continue\n",
    "        subset = name[len(CORPUS)+1:-3]\n",
    "\n",
    "        yield transform_subset(data, subset)\n",
    "\n",
    "\n",
    "dfs = [pd.DataFrame(item) for item in load_data()]\n",
    "if dfs:\n",
    "    df = pd.concat(dfs)\n",
    "    del dfs\n",
    "    df = df.set_index(HASH)[META+DATA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- extract, transform, load (save) the following corpus:\n",
    "\n",
    "CORPUS = \"Europarl\"\n",
    "LOCL_PATH = \"Europarl/Europarl/xml/de\"\n",
    "FULL_PATH = join(DATA_BASE, LOCL_PATH)\n",
    "\n",
    "re_ws = re.compile(r'& #160 ;')\n",
    "\n",
    "def transform_subset(source, subset_name: str):\n",
    "    \"\"\"\n",
    "    :param source: list of dictionaries in original key/value format\n",
    "    :param subset_name: string identifier of the subset the data belongs to\n",
    "    :return: list of dictionaries in standard key/value format\n",
    "    \"\"\"\n",
    "    print('transform', subset_name)\n",
    "    \n",
    "    soup = BeautifulSoup(source, 'xml')\n",
    "\n",
    "    for chapter in soup.find_all('CHAPTER'):\n",
    "        target = dict()\n",
    "        target[DATASET] = CORPUS\n",
    "        target[SUBSET]  = subset_name\n",
    "        target[ID]      = subset_name\n",
    "        target[ID2]     = chapter.attrs['ID']\n",
    "        first = chapter.find([\"SPEAKER\", \"P\"])\n",
    "        if first.name == 'SPEAKER':\n",
    "            title = first.get('NAME')\n",
    "        elif first.name == 'P':\n",
    "            title = ' '.join([w.string for w in chapter.find('s').find_all('w')])\n",
    "        target[TIME]    = datetime.strptime(subset_name[3:11], '%y-%m-%d')  # ep-07-01-18-009-07\n",
    "        \n",
    "        text = []\n",
    "        first = True\n",
    "        for paragraph in chapter.find_all([\"SPEAKER\", \"P\"]):\n",
    "            if paragraph.name == 'SPEAKER':\n",
    "                text.append(paragraph.get('NAME'))\n",
    "            elif paragraph.name == 'P':\n",
    "                text.append(' '.join([w.string for w in paragraph.find_all('w')]))\n",
    "        text = '\\n'.join(text)\n",
    "        target[TITLE]   = re_ws.sub(\" \", unescape(title)).strip()\n",
    "        target[TEXT]    = re_ws.sub(\" \", unescape(text[len(title):])).strip()\n",
    "        target[TAGS]    = None\n",
    "        target[LINKS]   = None\n",
    "        target[DESCR]   = None\n",
    "\n",
    "        target[HASH] = hash(tuple([target[key] for key in META]))\n",
    "        yield target\n",
    "\n",
    "\n",
    "def load_data(number_of_subsets: int=None, start: int=0):\n",
    "    \"\"\"\n",
    "    :param number_of_subsets: number of subsets to process in one call (None for no limit)\n",
    "    :param start: index of first subset to process\n",
    "    :yield: data set name, data subset name, data json\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"process\", CORPUS)\n",
    "    \n",
    "    # --- read files ---\n",
    "    files = [f for f in listdir(FULL_PATH) if isfile(join(FULL_PATH, f))]\n",
    "\n",
    "    if number_of_subsets:\n",
    "        number_of_subsets += start\n",
    "        if number_of_subsets > len(files):\n",
    "            number_of_subsets = None\n",
    "\n",
    "    for name in files[start:number_of_subsets]:\n",
    "        fpath = join(FULL_PATH, name)\n",
    "        try:\n",
    "            with gzip.open(fpath, 'rb') as fp:\n",
    "                data = fp.read()\n",
    "        except IOError:\n",
    "            print(\"Could not open\", fpath)\n",
    "            continue\n",
    "        subset = name[:-7]\n",
    "\n",
    "        yield transform_subset(data, subset)\n",
    "\n",
    "\n",
    "dfs = [pd.DataFrame(item) for item in load_data(number_of_subsets=None)]\n",
    "if dfs:\n",
    "    df = pd.concat(dfs)\n",
    "    del dfs\n",
    "    df = df.set_index(HASH)[META+DATA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- extract, transform, load (save) the following corpus:\n",
    "\n",
    "CORPUS = \"PoliticalSpeeches\"\n",
    "LOCAL_PATH = \"German-political-speeches-2018-release\"\n",
    "FULL_PATH = join(DATA_BASE, LOCAL_PATH)\n",
    "\n",
    "months = dict(Januar='01', Februar='02', März='03', April='04', Mai='05', Juni='06',\n",
    "              Juli='07', August='08', September='09', Oktober='10', November='11', Dezember='12')\n",
    "pattern = re.compile(r'(' + '|'.join(months.keys()) + r')')\n",
    "\n",
    "def transform_subset(source, subset_name: str):\n",
    "    \"\"\"\n",
    "    :param source: list of dictionaries in original key/value format\n",
    "    :param subset_name: string identifier of the subset the data belongs to\n",
    "    :return: list of dictionaries in standard key/value format\n",
    "    \"\"\"\n",
    "    print('transform', subset_name)\n",
    "    \n",
    "    soup = BeautifulSoup(source, 'xml')\n",
    "\n",
    "    for speech in soup.find_all('text'):\n",
    "        target = dict()\n",
    "        target[DATASET] = CORPUS\n",
    "        target[SUBSET]  = subset_name\n",
    "        target[ID]      = speech.get('url')\n",
    "        target[ID2]     = None\n",
    "        target[TITLE]   = speech.get('titel').strip()\n",
    "        target[TAGS]    = speech.get('person')\n",
    "        target[LINKS]   = None\n",
    "        if speech.attrs['datum']:\n",
    "            match = pattern.search(speech.attrs['datum'])\n",
    "            if match:\n",
    "                datum = speech.attrs['datum'].replace(\" \", \"\")\n",
    "                time = pattern.sub(lambda key: months[key.group()] + '.', datum)\n",
    "                target[TIME] = datetime.strptime(time, '%d.%m.%Y')\n",
    "            else:\n",
    "                target[TIME] = datetime.strptime(speech.attrs['datum'], '%d.%m.%Y')\n",
    "        else:\n",
    "            target[TIME]     = None\n",
    "\n",
    "        target[TEXT]   = speech.find('rohtext').string.strip()\n",
    "        target[DESCR]  = speech.get('untertitel')\n",
    "\n",
    "        target[HASH] = hash(tuple([target[key] for key in META]))\n",
    "        yield target\n",
    "\n",
    "\n",
    "def load_data(number_of_subsets: int=None, start: int=0):\n",
    "    \"\"\"\n",
    "    :param number_of_subsets: number of subsets to process in one call (None for no limit)\n",
    "    :param start: index of first subset to process\n",
    "    :yield: data set name, data subset name, data json\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"process\", CORPUS)\n",
    "    \n",
    "    # --- read files ---\n",
    "    files = [f for f in listdir(FULL_PATH) if isfile(join(FULL_PATH, f))]\n",
    "\n",
    "    if number_of_subsets:\n",
    "        number_of_subsets += start\n",
    "        if number_of_subsets > len(files):\n",
    "            number_of_subsets = None\n",
    "\n",
    "    for name in files[start:number_of_subsets]:\n",
    "        if name[-3:] != 'xml':\n",
    "            continue\n",
    "        \n",
    "        fpath = join(FULL_PATH, name)\n",
    "        try:\n",
    "            with open(fpath, 'r') as fp:\n",
    "                data = fp.read()\n",
    "        except IOError:\n",
    "            print(\"Could not open\", fpath)\n",
    "            continue\n",
    "        subset = name[:-4]\n",
    "\n",
    "        yield transform_subset(data, subset)\n",
    "\n",
    "\n",
    "dfs = [pd.DataFrame(item) for item in load_data(number_of_subsets=None)]\n",
    "if dfs:\n",
    "    df = pd.concat(dfs)\n",
    "    df = df.set_index(HASH)[META+DATA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- extract, transform, load (save) the following corpus:\n",
    "\n",
    "CORPUS = \"dewac\"\n",
    "LOCAL_PATH = \"WaCKy/dewac\"\n",
    "FULL_PATH = join(DATA_BASE, LOCAL_PATH)\n",
    "\n",
    "mn = 12\n",
    "\n",
    "def transform_line(source, subset):\n",
    "    \"\"\"\n",
    "    :param source: list of dictionaries in original key/value format\n",
    "    :param subset_name: string identifier of the subset the data belongs to\n",
    "    :return: list of dictionaries in standard key/value format\n",
    "    \"\"\"\n",
    "    if source['url'][:mn] != \"CURRENT URL \" or source['text'][:mn] == \"CURRENT URL \":\n",
    "        # print('INFO: bad formatted doc')\n",
    "        return None\n",
    "    target = dict()\n",
    "    target[DATASET] = CORPUS\n",
    "    target[SUBSET]  = subset\n",
    "    target[ID]      = source['url'][mn:].strip()\n",
    "    target[ID2]     = None\n",
    "    target[TITLE]   = target[ID].split('/')[-1]\n",
    "    target[TAGS]    = None\n",
    "    target[TIME]    = None\n",
    "    target[TEXT]    = source['text'].strip()\n",
    "    target[DESCR]   = None\n",
    "    target[LINKS]   = None\n",
    "    target[HASH]    = hash(tuple([target[key] for key in META]))\n",
    "    return target\n",
    "\n",
    "\n",
    "def transform_subset(fp, subset, number_of_documents):\n",
    "    url = ''\n",
    "    text = ''\n",
    "    for i in range(number_of_documents * 2):\n",
    "        line = fp.readline()\n",
    "        if line[:mn] != \"CURRENT URL \":\n",
    "            text += line\n",
    "        else:\n",
    "            data = {'url': url, 'text': text}\n",
    "            url = line\n",
    "            text = ''\n",
    "            if i:\n",
    "                row = transform_line(data, subset)\n",
    "                if row:\n",
    "                    yield row\n",
    "\n",
    "def load_data(number_of_subsets: None, number_of_documents=100000):\n",
    "    \"\"\"\n",
    "    :param number_of_subsets: number of subsets to process in one call (None for no limit)\n",
    "    :param start: index of first subset to process\n",
    "    :yield: data set name, data subset name, data json\n",
    "    \"\"\"\n",
    "    print(\"process\", CORPUS)\n",
    "    \n",
    "    # --- read files ---\n",
    "    name = 'dewac_preproc'\n",
    "    fpath = join(FULL_PATH, name)\n",
    "    print(fpath)\n",
    "    with open(fpath, 'r', encoding='latin-1') as fp:\n",
    "        i = 1\n",
    "        count_total = 0\n",
    "        while True:\n",
    "            if number_of_subsets and i > number_of_subsets:\n",
    "                print('limit of subsets reached')\n",
    "                break\n",
    "            print(\"process subset {:02d}\".format(i))\n",
    "            dfs = [item for item in transform_subset(fp, i, number_of_documents)]\n",
    "            count_tmp = len(dfs)\n",
    "            count_total += count_tmp\n",
    "            print(\"{:d} documents processed ({:d} in total)\".format(count_tmp, count_total))\n",
    "            if dfs:\n",
    "                df = pd.DataFrame.from_dict(dfs).set_index(HASH, drop=True)[META+DATA]\n",
    "                del dfs\n",
    "                store((\"%s_%02d\" % (CORPUS, i)), df)\n",
    "            else:\n",
    "                print('no more documents')\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "\n",
    "load_data(number_of_subsets=None, number_of_documents=55000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
