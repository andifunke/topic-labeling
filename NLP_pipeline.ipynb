{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, exists\n",
    "from iwnlp.iwnlp_wrapper import IWNLPWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- default constants definitions ---\n",
    "\n",
    "DATA_BASE = \"../../master_cloud/corpora\"\n",
    "ETL_BASE = \"preprocessed\"\n",
    "ETL_PATH = join(DATA_BASE, ETL_BASE)\n",
    "NLP_BASE = \"preprocessed/nlp\"\n",
    "NLP_PATH = join(DATA_BASE, NLP_BASE)\n",
    "SPACY_PATH = join(NLP_PATH, 'spacy_model')\n",
    "VOCAB_PATH = join(SPACY_PATH, 'vocab')\n",
    "\n",
    "# standard meta data fields\n",
    "DATASET = 'dataset'\n",
    "SUBSET = 'subset'\n",
    "ID = 'doc_id'\n",
    "ID2 = 'doc_subid'\n",
    "TITLE = 'title'\n",
    "TAGS = 'tags'\n",
    "TIME = 'date_time'\n",
    "# AUTHOR\n",
    "# SUBTITLE\n",
    "# CATEGORY\n",
    "META = [DATASET, SUBSET, ID, ID2, TITLE, TAGS, TIME]\n",
    "TEXT = 'text'\n",
    "HASH = 'hash'\n",
    "\n",
    "### --- additional constants\n",
    "\n",
    "# tags\n",
    "PUNCT = 'PUNCT'\n",
    "DET = 'DET'\n",
    "PHRASE = 'PHRASE'\n",
    "\n",
    "# keys\n",
    "IWNLP = 'IWNLP'\n",
    "POS = 'POS'\n",
    "INDEX = 'index'\n",
    "START = 'start'\n",
    "NOUN = 'NOUN'\n",
    "PROPN = 'PROPN'\n",
    "LEMMA = 'lemma'\n",
    "TAG = 'tag'\n",
    "STOP = 'stop'\n",
    "ENT_TYPE = 'ent_type'\n",
    "ENT_IOB = 'ent_iob'\n",
    "KNOWN = 'known'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- load spacy and iwnlp ---\n",
    "\n",
    "if len(sys.argv) > 1 and sys.argv[1] == '--hpc':\n",
    "    print('on hpc')\n",
    "    de = '/home/funkea/.local/lib/python3.4/site-packages/de_core_news_sm/de_core_news_sm-2.0.0'\n",
    "else:\n",
    "    de = 'de'\n",
    "\n",
    "print(\"loading spacy\")\n",
    "nlp = spacy.load(de)  # <-- load with dependency parser (slower)\n",
    "# nlp = spacy.load(de, disable=['parser'])\n",
    "\n",
    "if exists(VOCAB_PATH):\n",
    "    print(\"reading vocab from\", VOCAB_PATH)\n",
    "    nlp.vocab.from_disk(VOCAB_PATH)\n",
    "\n",
    "print(\"loading IWNLPWrapper\")\n",
    "lemmatizer = IWNLPWrapper(lemmatizer_path='../data/IWNLP.Lemmatizer_20170501.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- function definitions ---\n",
    "\n",
    "def process_phrases(doc):\n",
    "    \"\"\" \n",
    "        given a doc process and return the contained noun phrases.\n",
    "        This function is based on spacy's noun chunk detection. \n",
    "        It also creates items for a global phrase lookup table, which are currently not used.\n",
    "    \"\"\"\n",
    "\n",
    "    # clean the noun chuncs from spacy first\n",
    "    noun_chunks = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        start = False\n",
    "        noun_chunk = []\n",
    "        for token in chunk:\n",
    "            # exclude punctuation\n",
    "            if token.pos_ == PUNCT:\n",
    "                continue\n",
    "            # exclude leading determiners\n",
    "            if not start and (token.pos_ == DET or token.is_stop):\n",
    "                continue\n",
    "            start = True\n",
    "            noun_chunk.append(token)\n",
    "        if len(noun_chunk) > 1:\n",
    "            noun_chunks.append(noun_chunk)\n",
    "    \n",
    "    # the remaining, adjusted noun chunks will be lemmatized and indexed\n",
    "    phrase_list_lookup = []\n",
    "    phrase_list_doc = []\n",
    "    for chunk in noun_chunks:\n",
    "        phrase = []\n",
    "        for token in chunk:\n",
    "            lemma, _ = lemmatize(token.text, token.pos_)\n",
    "            if lemma:\n",
    "                phrase.append(lemma)\n",
    "            else:\n",
    "                phrase.append(token.text)\n",
    "        phrase = ' '.join(phrase)\n",
    "        text = ' '.join([t.text for t in chunk])\n",
    "        \n",
    "        # add to phrase collection of corpus\n",
    "        phrase_lookup = pd.Series()\n",
    "        phrase_lookup['lemmatized'] = phrase\n",
    "        phrase_lookup['original'] = text\n",
    "        # phrase_lookup['Spacy Tokens'] = tuple(chunk)\n",
    "        phrase_list_lookup.append(phrase_lookup)\n",
    "        \n",
    "        # add to document dataframe\n",
    "        phrase_series = pd.Series()\n",
    "        phrase_series[TEXT] = text\n",
    "        phrase_series[IWNLP] = phrase\n",
    "        phrase_series[POS] = PHRASE\n",
    "        phrase_series[INDEX] = chunk[0].i\n",
    "        phrase_series[START] = chunk[0].idx\n",
    "        phrase_list_doc.append(phrase_series)\n",
    "\n",
    "    # return the dataframes and for the doc dataframe and for the global phrase lookup table\n",
    "    return pd.DataFrame(phrase_list_doc), pd.DataFrame(phrase_list_lookup)\n",
    "\n",
    "\n",
    "def lemmatize(token: str, pos: str) -> (str, bool):\n",
    "    \"\"\" \n",
    "    This function uses the IWNLP lemmatizer with a few enhancements for compund nouns and nouns \n",
    "    with uncommon capitalization. Can also be used to lemmatize tokens with different POS-tags.\n",
    "    Do not use this function to lemmatize phrases.\n",
    "    :param token: white space stripped single token (str)\n",
    "    :param pos:   string constant, one of Universal tagset.\n",
    "    :return: tuple of type (str, bool)\n",
    "           value[0]: The lemma of the token if a lemma can be derived, else None.\n",
    "           value[1]: True if the token can be retrieved from the Wiktionary database as is, else False.\n",
    "    \"\"\"\n",
    "    \n",
    "    if pos == PHRASE:\n",
    "        try:\n",
    "            raise ValueError\n",
    "        except ValueError:\n",
    "            print(\"Don't lemmatize Phrases with this function!\")\n",
    "    \n",
    "    lemm = lemmatizer.lemmatize(token, pos)\n",
    "    # default lemmatization ok?\n",
    "    if lemm:\n",
    "        return lemm[0], True\n",
    "\n",
    "    # some rules to derive a lemma from the original token (nouns only)\n",
    "    # TODO: define rules for hyphenated nouns\n",
    "    if pos == NOUN or pos == PROPN:\n",
    "        # first try default noun capitalization\n",
    "        lemm = lemmatizer.lemmatize(token.title(), pos)\n",
    "        if lemm:\n",
    "            return lemm[0], False\n",
    "\n",
    "    # still no results: try noun suffixes\n",
    "        for i in range(1, len(token)-1):\n",
    "            token_edit = token[i:].title()\n",
    "            lemm = lemmatizer.lemmatize_plain(token_edit, ignore_case=True)\n",
    "            if lemm:\n",
    "                lemm = lemm[0]\n",
    "                lemm = token[:i].title() + lemm.lower()\n",
    "                return lemm, False\n",
    "    \n",
    "    # sorry, no results found:\n",
    "    return None, False\n",
    "\n",
    "\n",
    "def essence_from_doc(doc, key):\n",
    "    \"\"\"\n",
    "    Creates a pandas DataFrame from a given spacy.doc that contains only nouns and noun phrases.\n",
    "    :param doc: spacy.doc\n",
    "    :return:     pandas.DataFrame\n",
    "    \"\"\"\n",
    "    tags = [\n",
    "        (\n",
    "         token.text, token.lemma_, token.pos_, token.tag_, token.is_stop,\n",
    "         token.i, token.idx,\n",
    "         token.ent_type_, token.ent_iob_, # token.ent_id_,\n",
    "         ) for token in doc]\n",
    "    df = pd.DataFrame(tags)\n",
    "    df = df.rename(columns={k:v for k,v in enumerate([\n",
    "          TEXT, LEMMA, POS, TAG, STOP, INDEX, START, ENT_TYPE, ENT_IOB,\n",
    "          # \"Dep\", \"Shape\", \"alpha\", \"Ent_id\"  # currently not used :(\n",
    "    ])})\n",
    "    \n",
    "    # add IWNLP lemmatization\n",
    "    df[IWNLP], df[KNOWN] = zip(*df.apply(lambda row: lemmatize(row[TEXT], row[POS]), axis=1))\n",
    "    \n",
    "    # add phrases\n",
    "    df_phrases, phrase_lookup = process_phrases(doc)\n",
    "    df = df.append(df_phrases).sort_values(START)\n",
    "    df = df[df.POS.isin([NOUN, PROPN, PHRASE])].reset_index(drop=True)\n",
    "    \n",
    "    # replace Text with lemmatization, if lemmatization exists\n",
    "    mask = ~df[IWNLP].isnull()\n",
    "    df.loc[mask, TEXT] = df.loc[mask, IWNLP]\n",
    "    \n",
    "    # add hash-key\n",
    "    df[HASH] = key\n",
    "    \n",
    "    return df[[HASH, INDEX, TEXT, POS]], phrase_lookup\n",
    "\n",
    "\n",
    "def process_docs(series, size=None):\n",
    "    \"\"\" main function for sending the dataframes from the ETL pipeline to the NLP pipeline \"\"\"\n",
    "    length = len(series)\n",
    "    steps = 100\n",
    "    step_len = 100//steps\n",
    "    percent = length//steps\n",
    "    done = 0\n",
    "    for i, kv in enumerate(series[:size].iteritems()):\n",
    "        if i % percent == 0:\n",
    "            print(\"{:d}%: {:d} documents processed\".format(done, i))\n",
    "            done += step_len\n",
    "\n",
    "        k, v = kv\n",
    "        # build spacy doc\n",
    "        doc = nlp(v)\n",
    "        essential_token, phrase_lookup = essence_from_doc(doc, key=k)\n",
    "        yield essential_token, phrase_lookup\n",
    "\n",
    "\n",
    "def store(corpus, df):\n",
    "    \"\"\"returns the file path where the dataframe was stores\"\"\"\n",
    "    makedirs(NLP_PATH, exist_ok=True)\n",
    "    fname = join(NLP_PATH, corpus + '.pickle')\n",
    "    print('saving', corpus, 'to', fname)\n",
    "    df.to_pickle(fname)\n",
    "    return fname\n",
    "\n",
    "\n",
    "def read(f):\n",
    "    \"\"\" reads a dataframe from pickle format \"\"\"\n",
    "    print(\"reading corpus from\", f)\n",
    "    return pd.read_pickle(f)\n",
    "\n",
    "\n",
    "def read_process_load(file_path, corpus):\n",
    "    df = read(file_path)\n",
    "    print(\"processing\", corpus)\n",
    "    docs, phrase_lookups = zip(*[tple for tple in process_docs(df[TEXT], size=None)])\n",
    "    docs = pd.concat(docs).reset_index(drop=True)\n",
    "    phrase_lookups = pd.concat(phrase_lookups).reset_index(drop=True)\n",
    "\n",
    "    store(corpus + '_nlp', docs)\n",
    "    store(corpus + '_phrase_lookups', phrase_lookups)\n",
    "    print(\"writing spacy model to disk:\", NLP_PATH)\n",
    "    # stored with each corpus, in case anythings goes wrong\n",
    "    nlp.to_disk(SPACY_PATH)\n",
    "    # nlp.vocab.to_disk(VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- run notebook ---\n",
    "\n",
    "LOCAL_PATH = ETL_BASE\n",
    "FULL_PATH = join(DATA_BASE, LOCAL_PATH)\n",
    "\n",
    "files = sorted([f for f in listdir(FULL_PATH) if isfile(join(FULL_PATH, f))])\n",
    "\n",
    "for name in files:\n",
    "    corpus = re.split(r'\\.|_', name)[0]\n",
    "    fname = join(FULL_PATH, name)\n",
    "    read_process_load(fname, corpus)\n",
    "    \n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
