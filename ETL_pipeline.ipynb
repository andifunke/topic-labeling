{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- default imports ---\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- constants definitions ---\n",
    "\n",
    "DATA_BASE = \"../../master_cloud/corpora\"\n",
    "ETL_BASE = \"preprocessed\"\n",
    "ETL_PATH = join(DATA_BASE, ETL_BASE)\n",
    "\n",
    "# standard meta data fields\n",
    "DATASET = 'dataset'\n",
    "SUBSET = 'subset'\n",
    "ID = 'doc_id'\n",
    "ID2 = 'doc_subid'\n",
    "TITLE = 'title'\n",
    "TAGS = 'tags'\n",
    "TIME = 'date_time'\n",
    "#AUTHOR\n",
    "#SUBTITLE\n",
    "#CATEGORY\n",
    "META = [DATASET, SUBSET, ID, ID2, TITLE, TAGS, TIME]\n",
    "TEXT = 'text'\n",
    "HASH = 'hash'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- store meta data and content ---\n",
    "\n",
    "def store(corpus, df):\n",
    "    \"\"\"returns the file name where the dataframe was stores\"\"\"\n",
    "    makedirs(ETL_PATH, exist_ok=True)\n",
    "    fname = join(ETL_PATH, corpus + '.pickle')\n",
    "    print('saving to', fname)\n",
    "    df.to_pickle(fname)\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = store(CORPUS, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to ../../master_cloud/corpora/preprocessed/dewac_00.pickle\n",
      "saving to ../../master_cloud/corpora/preprocessed/dewac_01.pickle\n",
      "saving to ../../master_cloud/corpora/preprocessed/dewac_02.pickle\n",
      "saving to ../../master_cloud/corpora/preprocessed/dewac_03.pickle\n",
      "saving to ../../master_cloud/corpora/preprocessed/dewac_04.pickle\n",
      "saving to ../../master_cloud/corpora/preprocessed/dewac_05.pickle\n",
      "saving to ../../master_cloud/corpora/preprocessed/dewac_06.pickle\n",
      "saving to ../../master_cloud/corpora/preprocessed/dewac_07.pickle\n",
      "saving to ../../master_cloud/corpora/preprocessed/dewac_08.pickle\n",
      "saving to ../../master_cloud/corpora/preprocessed/dewac_09.pickle\n",
      "saving to ../../master_cloud/corpora/preprocessed/dewac_10.pickle\n",
      "saving to ../../master_cloud/corpora/preprocessed/dewac_11.pickle\n",
      "saving to ../../master_cloud/corpora/preprocessed/dewac_12.pickle\n",
      "saving to ../../master_cloud/corpora/preprocessed/dewac_13.pickle\n",
      "saving to ../../master_cloud/corpora/preprocessed/dewac_14.pickle\n",
      "saving to ../../master_cloud/corpora/preprocessed/dewac_15.pickle\n",
      "saving to ../../master_cloud/corpora/preprocessed/dewac_16.pickle\n",
      "saving to ../../master_cloud/corpora/preprocessed/dewac_17.pickle\n"
     ]
    }
   ],
   "source": [
    "LOCAL_PATH = ETL_BASE\n",
    "FULL_PATH = join(DATA_BASE, LOCAL_PATH)\n",
    "\n",
    "files = sorted([f for f in listdir(FULL_PATH) if isfile(join(FULL_PATH, f)) if f[:3] == 'dew'])\n",
    "\n",
    "def read(f):\n",
    "     return pd.read_pickle(f)\n",
    "\n",
    "for name in files:\n",
    "    fname = join(FULL_PATH, name)\n",
    "    df = read(fname)\n",
    "    df['doc_id'] = df['doc_id'].str.strip()\n",
    "    df['title'] = df['title'].str.strip()\n",
    "    print('saving to', fname)\n",
    "    #df.to_pickle(fname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines for certain corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- extract, transform, load (save) the following corpus:\n",
    "\n",
    "CORPUS = \"OnlineParticipation\"\n",
    "LOCAL_PATH = \"OnlineParticipationDatasets/downloads\"\n",
    "FULL_PATH = join(DATA_BASE, LOCAL_PATH)\n",
    "\n",
    "\n",
    "def transform_subset(source, subset_name: str):\n",
    "    \"\"\"\n",
    "    :param source: list of dictionaries in original key/value format\n",
    "    :param subset_name: string identifier of the subset the data belongs to\n",
    "    :return: list of dictionaries in standard key/value format\n",
    "    \"\"\"\n",
    "    category_lookup = {}\n",
    "    print('transform', subset_name)\n",
    "\n",
    "    for doc in source:\n",
    "        target = dict()\n",
    "        target[DATASET] = CORPUS\n",
    "        target[SUBSET] = subset_name\n",
    "        target[ID] = doc['suggestion_id']\n",
    "        target[TITLE] = doc['title']\n",
    "        target[TIME] = doc['date_time']\n",
    "\n",
    "        # wuppertal has a different data scheme\n",
    "        if subset_name == 'wuppertal2017':\n",
    "            if 'tags' in doc:\n",
    "                target[TAGS] = tuple(doc['tags'])\n",
    "                category_lookup[target[ID]] = target[TAGS]\n",
    "            else:\n",
    "                target[TAGS] = category_lookup[target[ID]]\n",
    "            target[ID2] = 0\n",
    "            target[TEXT] = doc['title'] + ' .\\n' \\\n",
    "                      + doc['content'] + ' .\\n' \\\n",
    "                      + doc['Voraussichtliche Rolle f체r die Stadt Wuppertal'] + ' .\\n' \\\n",
    "                      + doc['Mehrwert der Idee f체r Wuppertal'] + ' .\\n'\n",
    "                      # + doc['Eigene Rolle bei der Projektidee'] + ' .\\n'\n",
    "                      # + doc['Gesch채tzte Umsetzungsdauer und Startschuss'] + ' .\\n'\n",
    "                      # + doc['Kostensch채tzung der Ideeneinreicher'] + ' .\\n'\n",
    "                    \n",
    "        else:\n",
    "            if 'category' in doc:\n",
    "                target[TAGS] = doc['category']\n",
    "                category_lookup[target[ID]] = target[TAGS]\n",
    "            else:\n",
    "                target[TAGS] = category_lookup[target[ID]]\n",
    "            target[ID2] = doc['comment_id'] if ('comment_id' in doc) else 0\n",
    "            # ignore if no content\n",
    "            if not doc['content']:\n",
    "                continue\n",
    "            target[TEXT] = doc['title'] + ' .\\n' + doc['content'] if doc['title'] else doc['content']\n",
    "        \n",
    "        target[HASH] = hash(tuple([target[key] for key in META]))\n",
    "        yield target\n",
    "\n",
    "\n",
    "def load_data(number_of_subsets: int=None, start: int=0):\n",
    "    \"\"\"\n",
    "    :param number_of_subsets: number of subsets to process in one call (None for no limit)\n",
    "    :param start: index of first subset to process\n",
    "    :yield: data set name, data subset name, data json\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"process\", CORPUS)\n",
    "    \n",
    "    # --- read files ---\n",
    "    files = [f for f in listdir(FULL_PATH) if isfile(join(FULL_PATH, f))]\n",
    "\n",
    "    if number_of_subsets:\n",
    "        number_of_subsets += start\n",
    "        if number_of_subsets > len(files):\n",
    "            number_of_subsets = None\n",
    "\n",
    "    for name in files[start:number_of_subsets]:\n",
    "        if name[-9:-5] != 'flat':\n",
    "            continue\n",
    "        \n",
    "        fpath = os.path.join(FULL_PATH, name)\n",
    "        try: \n",
    "            with open(fpath, 'r') as fp:\n",
    "                print('open:', fpath)\n",
    "                data = json.load(fp)\n",
    "                if not data:\n",
    "                    continue\n",
    "        except IOError:\n",
    "            print(\"Could not open\", fpath)\n",
    "            continue\n",
    "        subset = name[6:-10]\n",
    "\n",
    "        yield transform_subset(data, subset)\n",
    "\n",
    "\n",
    "dfs = [pd.DataFrame(item) for item in load_data()]\n",
    "if dfs:\n",
    "    df = pd.concat(dfs)\n",
    "    df = df.set_index(HASH)[META+[TEXT]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- extract, transform, load (save) the following corpus:\n",
    "\n",
    "CORPUS = \"FAZ\"\n",
    "LOCAL_PATH = \"scrapy/faz\"\n",
    "FULL_PATH = join(DATA_BASE, LOCAL_PATH)\n",
    "\n",
    "\n",
    "def transform_subset(source, subset_name: str):\n",
    "    \"\"\"\n",
    "    :param source: list of dictionaries in original key/value format\n",
    "    :param subset_name: string identifier of the subset the data belongs to\n",
    "    :return: list of dictionaries in standard key/value format\n",
    "    \"\"\"\n",
    "    print('transform', subset_name)\n",
    "\n",
    "    for doc in source:\n",
    "        target = dict()\n",
    "        target[DATASET] = CORPUS\n",
    "        target[SUBSET] = doc['url'].split('/')[4]\n",
    "        target[ID] = doc['url']\n",
    "        target[ID2] = 0\n",
    "        target[TITLE] = doc['title']\n",
    "        target[TAGS] = tuple(doc['keywords'])\n",
    "        if doc['published']:\n",
    "            target[TIME] = datetime.strptime(doc['published'], '%Y-%m-%dT%H:%M:%S%z')  # 2018-08-22T11:11:45+0200\n",
    "            # target[TIME] = datetime.fromisoformat(doc['published'])  # may work in Python 3.7\n",
    "        else: \n",
    "            target[TIME] = None\n",
    "        target[TEXT] = doc['title'] + ' .\\n' \\\n",
    "                     + doc['description'] + ' .\\n' \\\n",
    "                     + doc['text']\n",
    "\n",
    "        target[HASH] = hash(tuple([target[key] for key in META]))\n",
    "        yield target\n",
    "\n",
    "\n",
    "def load_data(number_of_subsets: int=None, start: int=0):\n",
    "    \"\"\"\n",
    "    :param number_of_subsets: number of subsets to process in one call (None for no limit)\n",
    "    :param start: index of first subset to process\n",
    "    :yield: data set name, data subset name, data json\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"process\", CORPUS)\n",
    "    \n",
    "    # --- read files ---\n",
    "    files = [f for f in listdir(FULL_PATH) if isfile(join(FULL_PATH, f))]\n",
    "    print(files)\n",
    "\n",
    "    if number_of_subsets:\n",
    "        number_of_subsets += start\n",
    "        if number_of_subsets > len(files):\n",
    "            number_of_subsets = None\n",
    "\n",
    "    for name in files[start:number_of_subsets]:\n",
    "        fpath =join(FULL_PATH, name)\n",
    "        try: \n",
    "            with open(fpath, 'r') as fp:\n",
    "                print('open:', fpath)\n",
    "                data = [json.loads(d) for d in fp.readlines()]\n",
    "                if not data:\n",
    "                    continue\n",
    "        except IOError:\n",
    "            print(\"Could not open\", fpath)\n",
    "            continue\n",
    "        subset = name[4:-3]\n",
    "\n",
    "        yield transform_subset(data, subset)\n",
    "\n",
    "\n",
    "dfs = [pd.DataFrame(item) for item in load_data()]\n",
    "if dfs:\n",
    "    df = pd.concat(dfs)\n",
    "    df = df.set_index(HASH)[META+[TEXT]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- extract, transform, load (save) the following corpus:\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "CORPUS = \"Europarl\"\n",
    "LOCAL_PATH = \"Europarl/Europarl/xml/de\"\n",
    "FULL_PATH = join(DATA_BASE, LOCAL_PATH)\n",
    "\n",
    "\n",
    "def transform_subset(source, subset_name: str):\n",
    "    \"\"\"\n",
    "    :param source: list of dictionaries in original key/value format\n",
    "    :param subset_name: string identifier of the subset the data belongs to\n",
    "    :return: list of dictionaries in standard key/value format\n",
    "    \"\"\"\n",
    "    print('transform', subset_name)\n",
    "    \n",
    "    soup = BeautifulSoup(source, 'xml')\n",
    "\n",
    "    for chapter in soup.find_all('CHAPTER'):\n",
    "        target = dict()\n",
    "        target[DATASET] = CORPUS\n",
    "        target[SUBSET] = subset_name\n",
    "        target[ID] = subset_name\n",
    "        target[ID2] = chapter.attrs['ID']\n",
    "        target[TITLE] = ' '.join([w.string for w in chapter.find('s').find_all('w')])\n",
    "        target[TAGS] = None\n",
    "        target[TIME] = datetime.strptime(subset_name[3:11], '%y-%m-%d')  # ep-07-01-18-009-07\n",
    "        \n",
    "        text = []\n",
    "        for paragraph in chapter.find_all([\"SPEAKER\", \"P\"]):\n",
    "            if paragraph.name == 'SPEAKER':\n",
    "                text.append(paragraph.attrs['NAME'])\n",
    "            elif paragraph.name == 'P':\n",
    "                text.append(' '.join([w.string for w in paragraph.find_all('w')]))\n",
    "        target[TEXT] = '\\n'.join(text)\n",
    "\n",
    "        target[HASH] = hash(tuple([target[key] for key in META]))\n",
    "        yield target\n",
    "\n",
    "\n",
    "def load_data(number_of_subsets: int=None, start: int=0):\n",
    "    \"\"\"\n",
    "    :param number_of_subsets: number of subsets to process in one call (None for no limit)\n",
    "    :param start: index of first subset to process\n",
    "    :yield: data set name, data subset name, data json\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"process\", CORPUS)\n",
    "    \n",
    "    # --- read files ---\n",
    "    files = [f for f in listdir(FULL_PATH) if isfile(join(FULL_PATH, f))]\n",
    "\n",
    "    if number_of_subsets:\n",
    "        number_of_subsets += start\n",
    "        if number_of_subsets > len(files):\n",
    "            number_of_subsets = None\n",
    "\n",
    "    for name in files[start:number_of_subsets]:\n",
    "        fpath = join(FULL_PATH, name)\n",
    "        try:\n",
    "            with gzip.open(fpath, 'rb') as fp:\n",
    "                data = fp.read()\n",
    "        except IOError:\n",
    "            print(\"Could not open\", fpath)\n",
    "            continue\n",
    "        subset = name[:-7]\n",
    "\n",
    "        yield transform_subset(data, subset)\n",
    "\n",
    "\n",
    "dfs = [pd.DataFrame(item) for item in load_data(number_of_subsets=None)]\n",
    "if dfs:\n",
    "    df = pd.concat(dfs)\n",
    "    df = df.set_index(HASH)[META+[TEXT]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- extract, transform, load (save) the following corpus:\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "CORPUS = \"PoliticalSpeeches\"\n",
    "LOCAL_PATH = \"German-political-speeches-2018-release\"\n",
    "FULL_PATH = join(DATA_BASE, LOCAL_PATH)\n",
    "\n",
    "\n",
    "def transform_subset(source, subset_name: str):\n",
    "    \"\"\"\n",
    "    :param source: list of dictionaries in original key/value format\n",
    "    :param subset_name: string identifier of the subset the data belongs to\n",
    "    :return: list of dictionaries in standard key/value format\n",
    "    \"\"\"\n",
    "    print('transform', subset_name)\n",
    "    \n",
    "    soup = BeautifulSoup(source, 'xml')\n",
    "\n",
    "    months = dict(Januar='01', Februar='02', M채rz='03', April='04', Mai='05', Juni='06',\n",
    "                  Juli='07', August='08', September='09', Oktober='10', November='11', Dezember='12')\n",
    "    pattern = re.compile(r'(' + '|'.join(months.keys()) + r')')\n",
    "\n",
    "    for speech in soup.find_all('text'):\n",
    "        target = dict()\n",
    "        target[DATASET] = CORPUS\n",
    "        target[SUBSET] = subset_name\n",
    "        target[ID] = speech.attrs['url']\n",
    "        target[ID2] = 0\n",
    "        target[TITLE] = speech.attrs['titel']\n",
    "        target[TAGS] = speech.attrs['person']\n",
    "        if speech.attrs['datum']:\n",
    "            match = pattern.search(speech.attrs['datum'])\n",
    "            if match:\n",
    "                datum = speech.attrs['datum'].replace(\" \", \"\")\n",
    "                time = pattern.sub(lambda key: months[key.group()] + '.', datum)\n",
    "                target[TIME] = datetime.strptime(time, '%d.%m.%Y')\n",
    "            else:\n",
    "                target[TIME] = datetime.strptime(speech.attrs['datum'], '%d.%m.%Y')\n",
    "        else:\n",
    "            target[TIME] = None\n",
    "\n",
    "        target[TEXT] = speech.attrs['titel'] + ' .\\n' \\\n",
    "                     + speech.attrs['untertitel'] + ' .\\n' if 'untertitel' in speech.attrs else ''\\\n",
    "                     + speech.find('rohtext').string\n",
    "\n",
    "        target[HASH] = hash(tuple([target[key] for key in META]))\n",
    "        yield target\n",
    "\n",
    "\n",
    "def load_data(number_of_subsets: int=None, start: int=0):\n",
    "    \"\"\"\n",
    "    :param number_of_subsets: number of subsets to process in one call (None for no limit)\n",
    "    :param start: index of first subset to process\n",
    "    :yield: data set name, data subset name, data json\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"process\", CORPUS)\n",
    "    \n",
    "    # --- read files ---\n",
    "    files = [f for f in listdir(FULL_PATH) if isfile(join(FULL_PATH, f))]\n",
    "\n",
    "    if number_of_subsets:\n",
    "        number_of_subsets += start\n",
    "        if number_of_subsets > len(files):\n",
    "            number_of_subsets = None\n",
    "\n",
    "    for name in files[start:number_of_subsets]:\n",
    "        if name[-3:] != 'xml':\n",
    "            continue\n",
    "        \n",
    "        fpath = join(FULL_PATH, name)\n",
    "        try:\n",
    "            with open(fpath, 'r') as fp:\n",
    "                data = fp.read()\n",
    "        except IOError:\n",
    "            print(\"Could not open\", fpath)\n",
    "            continue\n",
    "        subset = name[:-4]\n",
    "\n",
    "        yield transform_subset(data, subset)\n",
    "\n",
    "\n",
    "dfs = [pd.DataFrame(item) for item in load_data(number_of_subsets=None)]\n",
    "if dfs:\n",
    "    df = pd.concat(dfs)\n",
    "    df = df.set_index(HASH)[META+[TEXT]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process dewac\n",
      "../../master_cloud/corpora/WaCKy/dewac/dewac_preproc\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "### --- extract, transform, load (save) the following corpus:\n",
    "\n",
    "CORPUS = \"dewac\"\n",
    "LOCAL_PATH = \"WaCKy/dewac\"\n",
    "FULL_PATH = join(DATA_BASE, LOCAL_PATH)\n",
    "mn = 12\n",
    "\n",
    "\n",
    "def transform_subset(source, subset_name: str):\n",
    "    \"\"\"\n",
    "    :param source: list of dictionaries in original key/value format\n",
    "    :param subset_name: string identifier of the subset the data belongs to\n",
    "    :return: list of dictionaries in standard key/value format\n",
    "    \"\"\"\n",
    "    # print('transform', source['url'][mn:])\n",
    "    \n",
    "    assert source['url'][:mn] == \"CURRENT URL \"\n",
    "    target = dict()\n",
    "    target[DATASET] = CORPUS\n",
    "    target[SUBSET] = subset_name\n",
    "    target[ID] = source['url'][mn:]\n",
    "    target[ID2] = None\n",
    "    target[TITLE] = target[ID].split('/')[-1]\n",
    "    target[TAGS] = None\n",
    "    target[TIME] = None\n",
    "    target[TEXT] = source['text']\n",
    "    target[HASH] = hash(tuple([target[key] for key in META]))\n",
    "    yield target\n",
    "\n",
    "\n",
    "def load_data(number_of_documents: int=None, start: int=0):\n",
    "    \"\"\"\n",
    "    :param number_of_subsets: number of subsets to process in one call (None for no limit)\n",
    "    :param start: index of first subset to process\n",
    "    :yield: data set name, data subset name, data json\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"process\", CORPUS)\n",
    "    \n",
    "    # --- read files ---\n",
    "    name = 'dewac_preproc'\n",
    "    fpath = join(FULL_PATH, name)\n",
    "    print(fpath)\n",
    "    try:\n",
    "        with open(fpath, 'r', encoding='latin-1') as fp:\n",
    "            i = 0\n",
    "            # not the most efficient with respect to CPU-cycles and I/O operations \n",
    "            # but quite feasible in regards of memory consumption\n",
    "            while True:\n",
    "                data = dict()\n",
    "                if number_of_documents and i >= start+number_of_documents:\n",
    "                    break\n",
    "                data['url'] = fp.readline().strip()\n",
    "                if not data['url']:\n",
    "                    break \n",
    "                data['text'] = fp.readline().strip()\n",
    "                i += 1\n",
    "                if i < start:\n",
    "                    # could be more efficient if we don't close the file pointer\n",
    "                    continue\n",
    "                if data['url'][:mn] != \"CURRENT URL \":\n",
    "                    fp.readline()\n",
    "                    continue\n",
    "                # print(i, ':')\n",
    "                yield transform_subset(data, '')\n",
    "    except IOError:\n",
    "        print(\"Could not open\", fpath)\n",
    "\n",
    "\n",
    "for i in range(0, 20):\n",
    "    dfs = [pd.DataFrame(item) for item in load_data(number_of_documents=100000, start=i*100000)]\n",
    "    if dfs:\n",
    "        df = pd.concat(dfs)\n",
    "        df = df.set_index(HASH)[META+[TEXT]]\n",
    "        store((\"%s_%02d\" % CORPUS, i), df)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print('done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
