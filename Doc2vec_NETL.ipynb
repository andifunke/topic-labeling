{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author:         Shraey Bhatia\n",
    "Date:           October 2016\n",
    "File:           doc2vectrain.py\n",
    "\n",
    "Gives a trained document vectors model (also known as Doc2VecModel). \n",
    "The input format are documents extrated using wiki extractor and tokenized using stanford tokenizer stored in a directory\n",
    "Output would be a trained doc2vec model.\n",
    "Parameters taken for main_train.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import gensim\n",
    "import argparse\n",
    "import codecs\n",
    "import unicodedata\n",
    "import multiprocessing\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] epochs input_dir output_dir\n",
      "ipykernel_launcher.py: error: the following arguments are required: input_dir, output_dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreas/bin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#These are the arguments that are passed in main_train.py \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"epochs\")        # Number of training epochs\n",
    "parser.add_argument(\"input_dir\")     \n",
    "parser.add_argument(\"output_dir\")\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3edb3dca7a51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Checks if the output directory specified already exists. If it does remove it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdel_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"rm -r \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'outdir'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdel_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# Checks if the output directory specified already exists. If it does remove it.\n",
    "\n",
    "if os.path.isdir(args.output_dir):\n",
    "    del_query = \"rm -r \"+'outdir'\n",
    "    os.system(del_query)\n",
    "\n",
    "# Create an output directory for the model\n",
    "#query = \"mkdir \"+args.output_dir\n",
    "query = \"mkdir \"+'outdir'\n",
    "os.system(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "from constants import *\n",
    "output = os.path.join(TMP_PATH, \"doc2vecmodel.d2v\")\n",
    "\n",
    "# Doc2vec Input documents Class. It uses yield to optimize memory usage and \"tag\" is Document title with an undescore.\n",
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        for source in self.sources:\n",
    "            with codecs.open(source, \"r\", \"utf-8\") as fin:\n",
    "                for cnt,line in enumerate(fin):\n",
    "                    if \"<doc\" in line:           # Every new document starts with this format\n",
    "                        found = \"\"\n",
    "\n",
    "                        m = re.search('title=\"(.*)\">',line)    # This gives the document title of Wikipedia\n",
    "                        if m:\n",
    "                            found = m.group(1)\n",
    "                            #found = found.lower()\n",
    "                            found = unicodedata.normalize(\"NFKD\", found) \n",
    "                            found = found.replace(\" \",\"_\") \n",
    "                            found = found.encode('utf-8')\n",
    "\n",
    "                        else:\n",
    "                            found = \"\"\n",
    "                        values = []\n",
    "                    else:\n",
    "                        if \"</doc\" not in line:                      #</doc tells us end of document, till not reached it is same document\n",
    "                            for word in line.split(\" \"):\n",
    "                                values.append(word.strip())\n",
    "                        if \"</doc\" in line:\n",
    "                            if found!= \"\":\n",
    "                            \n",
    "                                yield LabeledSentence(words = values, tags = [found])\n",
    "                                \n",
    "cores = multiprocessing.cpu_count() \n",
    "print(cores)\n",
    "filenames=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/docs_tokenised/AA/wiki_07',\n",
       " '../data/docs_tokenised/AA/wiki_08',\n",
       " '../data/docs_tokenised/AA/wiki_01',\n",
       " '../data/docs_tokenised/AA/wiki_06',\n",
       " '../data/docs_tokenised/AA/wiki_03',\n",
       " '../data/docs_tokenised/AA/wiki_04',\n",
       " '../data/docs_tokenised/AA/wiki_10',\n",
       " '../data/docs_tokenised/AA/wiki_05',\n",
       " '../data/docs_tokenised/AA/wiki_00',\n",
       " '../data/docs_tokenised/AA/wiki_09',\n",
       " '../data/docs_tokenised/AA/wiki_02']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all tokenised files in directory and subdirectories ( tokenised by stanford parser)\n",
    "input_dir = os.path.join(DATA_BASE, 'docs_tokenised')\n",
    "for path,subdirs,files in os.walk(input_dir):\n",
    "    for name in files:\n",
    "        temp = os.path.join(path, name)    \n",
    "        filenames.append(temp) \n",
    "        \n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = LabeledLineSentence(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreas/bin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2215484"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, s in enumerate(sentences):\n",
    "    s\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreas/bin/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "2018-10-10 01:59:54,859 : INFO : collecting all words and their counts\n",
      "/home/andreas/bin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "2018-10-10 01:59:54,860 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-10-10 01:59:57,106 : INFO : PROGRESS: at example #10000, processed 2724686 words (1213376/s), 205860 word types, 10000 tags\n",
      "2018-10-10 01:59:59,519 : INFO : PROGRESS: at example #20000, processed 5663655 words (1218480/s), 344754 word types, 20000 tags\n",
      "2018-10-10 02:00:01,955 : INFO : PROGRESS: at example #30000, processed 8603776 words (1207322/s), 461667 word types, 30000 tags\n",
      "2018-10-10 02:00:04,407 : INFO : PROGRESS: at example #40000, processed 11584163 words (1215861/s), 564945 word types, 39999 tags\n",
      "2018-10-10 02:00:06,901 : INFO : PROGRESS: at example #50000, processed 14593658 words (1207082/s), 660100 word types, 49999 tags\n",
      "2018-10-10 02:00:09,541 : INFO : PROGRESS: at example #60000, processed 17769851 words (1203176/s), 754134 word types, 59998 tags\n",
      "2018-10-10 02:00:12,149 : INFO : PROGRESS: at example #70000, processed 20978921 words (1230906/s), 846593 word types, 69998 tags\n",
      "2018-10-10 02:00:14,575 : INFO : PROGRESS: at example #80000, processed 23906736 words (1207161/s), 923575 word types, 79998 tags\n",
      "2018-10-10 02:00:17,081 : INFO : PROGRESS: at example #90000, processed 27019344 words (1242091/s), 999676 word types, 89998 tags\n",
      "2018-10-10 02:00:19,564 : INFO : PROGRESS: at example #100000, processed 30104432 words (1242672/s), 1072485 word types, 99997 tags\n",
      "2018-10-10 02:00:22,037 : INFO : PROGRESS: at example #110000, processed 33166167 words (1238310/s), 1143762 word types, 109997 tags\n",
      "2018-10-10 02:00:24,403 : INFO : PROGRESS: at example #120000, processed 36039586 words (1215166/s), 1208121 word types, 119997 tags\n",
      "2018-10-10 02:00:26,889 : INFO : PROGRESS: at example #130000, processed 39077862 words (1222028/s), 1272076 word types, 129997 tags\n",
      "2018-10-10 02:00:29,326 : INFO : PROGRESS: at example #140000, processed 42006542 words (1202194/s), 1333411 word types, 139997 tags\n",
      "2018-10-10 02:00:31,757 : INFO : PROGRESS: at example #150000, processed 44959112 words (1214770/s), 1392938 word types, 149996 tags\n",
      "2018-10-10 02:00:34,171 : INFO : PROGRESS: at example #160000, processed 47847700 words (1197164/s), 1450323 word types, 159994 tags\n",
      "2018-10-10 02:00:36,703 : INFO : PROGRESS: at example #170000, processed 50866142 words (1192069/s), 1508608 word types, 169991 tags\n",
      "2018-10-10 02:00:39,182 : INFO : PROGRESS: at example #180000, processed 53793239 words (1181187/s), 1564621 word types, 179990 tags\n",
      "2018-10-10 02:00:41,704 : INFO : PROGRESS: at example #190000, processed 56853437 words (1213324/s), 1623425 word types, 189989 tags\n",
      "2018-10-10 02:00:44,115 : INFO : PROGRESS: at example #200000, processed 59749556 words (1201727/s), 1677898 word types, 199989 tags\n",
      "2018-10-10 02:00:46,610 : INFO : PROGRESS: at example #210000, processed 62817110 words (1229774/s), 1733495 word types, 209988 tags\n",
      "2018-10-10 02:00:48,988 : INFO : PROGRESS: at example #220000, processed 65783531 words (1247606/s), 1785190 word types, 219988 tags\n",
      "2018-10-10 02:00:51,338 : INFO : PROGRESS: at example #230000, processed 68700047 words (1241716/s), 1833863 word types, 229988 tags\n",
      "2018-10-10 02:00:53,930 : INFO : PROGRESS: at example #240000, processed 71826176 words (1206471/s), 1883410 word types, 239986 tags\n",
      "2018-10-10 02:00:56,529 : INFO : PROGRESS: at example #250000, processed 74823452 words (1153199/s), 1935402 word types, 249986 tags\n",
      "2018-10-10 02:00:59,211 : INFO : PROGRESS: at example #260000, processed 78123587 words (1230762/s), 1989888 word types, 259986 tags\n",
      "2018-10-10 02:01:01,932 : INFO : PROGRESS: at example #270000, processed 81423013 words (1213183/s), 2044800 word types, 269986 tags\n",
      "2018-10-10 02:01:04,745 : INFO : PROGRESS: at example #280000, processed 84803386 words (1201839/s), 2101824 word types, 279986 tags\n",
      "2018-10-10 02:01:07,549 : INFO : PROGRESS: at example #290000, processed 88115809 words (1181623/s), 2155413 word types, 289984 tags\n",
      "2018-10-10 02:01:10,300 : INFO : PROGRESS: at example #300000, processed 91370739 words (1183502/s), 2207480 word types, 299984 tags\n",
      "2018-10-10 02:01:12,969 : INFO : PROGRESS: at example #310000, processed 94575931 words (1201244/s), 2256965 word types, 309984 tags\n",
      "2018-10-10 02:01:15,363 : INFO : PROGRESS: at example #320000, processed 97460820 words (1205309/s), 2300052 word types, 319982 tags\n",
      "2018-10-10 02:01:17,853 : INFO : PROGRESS: at example #330000, processed 100459538 words (1204459/s), 2346244 word types, 329981 tags\n",
      "2018-10-10 02:01:20,390 : INFO : PROGRESS: at example #340000, processed 103408736 words (1162408/s), 2391066 word types, 339981 tags\n",
      "2018-10-10 02:01:22,955 : INFO : PROGRESS: at example #350000, processed 106475304 words (1195837/s), 2435007 word types, 349980 tags\n",
      "2018-10-10 02:01:25,515 : INFO : PROGRESS: at example #360000, processed 109557568 words (1204495/s), 2480662 word types, 359979 tags\n",
      "2018-10-10 02:01:28,192 : INFO : PROGRESS: at example #370000, processed 112945629 words (1266014/s), 2530821 word types, 369978 tags\n",
      "2018-10-10 02:01:30,929 : INFO : PROGRESS: at example #380000, processed 116396183 words (1260932/s), 2579749 word types, 379976 tags\n",
      "2018-10-10 02:01:33,689 : INFO : PROGRESS: at example #390000, processed 119837737 words (1247184/s), 2628798 word types, 389975 tags\n",
      "2018-10-10 02:01:36,368 : INFO : PROGRESS: at example #400000, processed 123098042 words (1217123/s), 2674781 word types, 399975 tags\n",
      "2018-10-10 02:01:38,982 : INFO : PROGRESS: at example #410000, processed 126326819 words (1235666/s), 2721991 word types, 409973 tags\n",
      "2018-10-10 02:01:41,507 : INFO : PROGRESS: at example #420000, processed 129465140 words (1243254/s), 2765793 word types, 419972 tags\n",
      "2018-10-10 02:01:44,257 : INFO : PROGRESS: at example #430000, processed 132662593 words (1162751/s), 2811281 word types, 429972 tags\n",
      "2018-10-10 02:01:46,823 : INFO : PROGRESS: at example #440000, processed 135881134 words (1254577/s), 2858128 word types, 439972 tags\n",
      "2018-10-10 02:01:49,614 : INFO : PROGRESS: at example #450000, processed 139076195 words (1145234/s), 2904216 word types, 449972 tags\n",
      "2018-10-10 02:01:52,408 : INFO : PROGRESS: at example #460000, processed 142447772 words (1206804/s), 2950656 word types, 459970 tags\n",
      "2018-10-10 02:01:55,071 : INFO : PROGRESS: at example #470000, processed 145709996 words (1225271/s), 2993657 word types, 469970 tags\n",
      "2018-10-10 02:01:57,766 : INFO : PROGRESS: at example #480000, processed 148950231 words (1202782/s), 3038785 word types, 479970 tags\n",
      "2018-10-10 02:02:00,527 : INFO : PROGRESS: at example #490000, processed 152283431 words (1207531/s), 3084027 word types, 489969 tags\n",
      "2018-10-10 02:02:03,259 : INFO : PROGRESS: at example #500000, processed 155368825 words (1129512/s), 3123543 word types, 499968 tags\n",
      "2018-10-10 02:02:05,841 : INFO : PROGRESS: at example #510000, processed 158545740 words (1230584/s), 3165146 word types, 509968 tags\n",
      "2018-10-10 02:02:08,296 : INFO : PROGRESS: at example #520000, processed 161546659 words (1222718/s), 3203066 word types, 519968 tags\n",
      "2018-10-10 02:02:11,679 : INFO : PROGRESS: at example #530000, processed 165953722 words (1303031/s), 3267622 word types, 529968 tags\n",
      "2018-10-10 02:02:18,035 : INFO : PROGRESS: at example #540000, processed 174157855 words (1290727/s), 3397900 word types, 539966 tags\n",
      "2018-10-10 02:02:23,687 : INFO : PROGRESS: at example #550000, processed 181477892 words (1295466/s), 3511224 word types, 549964 tags\n",
      "2018-10-10 02:02:29,100 : INFO : PROGRESS: at example #560000, processed 188270322 words (1254839/s), 3615934 word types, 559955 tags\n",
      "2018-10-10 02:02:34,570 : INFO : PROGRESS: at example #570000, processed 195072968 words (1243866/s), 3716990 word types, 569947 tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-10 02:02:40,188 : INFO : PROGRESS: at example #580000, processed 201484091 words (1141226/s), 3808673 word types, 579938 tags\n",
      "2018-10-10 02:02:45,294 : INFO : PROGRESS: at example #590000, processed 207745002 words (1226486/s), 3895537 word types, 589932 tags\n",
      "2018-10-10 02:02:49,946 : INFO : PROGRESS: at example #600000, processed 213638276 words (1266955/s), 3978489 word types, 599923 tags\n",
      "2018-10-10 02:02:54,588 : INFO : PROGRESS: at example #610000, processed 219591473 words (1282726/s), 4058848 word types, 609921 tags\n",
      "2018-10-10 02:02:59,022 : INFO : PROGRESS: at example #620000, processed 225184383 words (1261466/s), 4135294 word types, 619912 tags\n",
      "2018-10-10 02:03:03,409 : INFO : PROGRESS: at example #630000, processed 230673972 words (1251505/s), 4207912 word types, 629906 tags\n",
      "2018-10-10 02:03:08,188 : INFO : PROGRESS: at example #640000, processed 236841385 words (1290658/s), 4287261 word types, 639904 tags\n",
      "2018-10-10 02:03:12,790 : INFO : PROGRESS: at example #650000, processed 242566531 words (1244066/s), 4359071 word types, 649897 tags\n",
      "2018-10-10 02:03:16,924 : INFO : PROGRESS: at example #660000, processed 247631550 words (1225352/s), 4421264 word types, 659892 tags\n",
      "2018-10-10 02:03:19,547 : INFO : PROGRESS: at example #670000, processed 250767128 words (1196177/s), 4455946 word types, 669888 tags\n",
      "2018-10-10 02:03:22,177 : INFO : PROGRESS: at example #680000, processed 253934915 words (1204483/s), 4492439 word types, 679885 tags\n",
      "2018-10-10 02:03:24,872 : INFO : PROGRESS: at example #690000, processed 257197023 words (1210724/s), 4530645 word types, 689884 tags\n",
      "2018-10-10 02:03:27,788 : INFO : PROGRESS: at example #700000, processed 260740698 words (1215342/s), 4569273 word types, 699883 tags\n",
      "2018-10-10 02:03:30,450 : INFO : PROGRESS: at example #710000, processed 264067424 words (1250002/s), 4605692 word types, 709880 tags\n",
      "2018-10-10 02:03:32,940 : INFO : PROGRESS: at example #720000, processed 267140997 words (1234861/s), 4638750 word types, 719876 tags\n",
      "2018-10-10 02:03:35,468 : INFO : PROGRESS: at example #730000, processed 270260215 words (1234027/s), 4671550 word types, 729874 tags\n",
      "2018-10-10 02:03:38,027 : INFO : PROGRESS: at example #740000, processed 273418553 words (1234417/s), 4704893 word types, 739873 tags\n",
      "2018-10-10 02:03:40,609 : INFO : PROGRESS: at example #750000, processed 276570523 words (1221522/s), 4738172 word types, 749869 tags\n",
      "2018-10-10 02:03:43,390 : INFO : PROGRESS: at example #760000, processed 279845839 words (1177930/s), 4772708 word types, 759867 tags\n",
      "2018-10-10 02:03:46,262 : INFO : PROGRESS: at example #770000, processed 283007219 words (1100999/s), 4804422 word types, 769864 tags\n",
      "2018-10-10 02:03:48,895 : INFO : PROGRESS: at example #780000, processed 286120975 words (1182865/s), 4836316 word types, 779863 tags\n",
      "2018-10-10 02:03:51,557 : INFO : PROGRESS: at example #790000, processed 289441741 words (1247400/s), 4871402 word types, 789862 tags\n",
      "2018-10-10 02:03:54,323 : INFO : PROGRESS: at example #800000, processed 292788487 words (1210297/s), 4910431 word types, 799861 tags\n",
      "2018-10-10 02:03:57,035 : INFO : PROGRESS: at example #810000, processed 296075024 words (1211974/s), 4943876 word types, 809860 tags\n",
      "2018-10-10 02:03:59,696 : INFO : PROGRESS: at example #820000, processed 299316098 words (1218493/s), 4976870 word types, 819857 tags\n",
      "2018-10-10 02:04:02,380 : INFO : PROGRESS: at example #830000, processed 302539066 words (1201096/s), 5009718 word types, 829851 tags\n",
      "2018-10-10 02:04:04,948 : INFO : PROGRESS: at example #840000, processed 305705605 words (1233298/s), 5041046 word types, 839847 tags\n",
      "2018-10-10 02:04:07,592 : INFO : PROGRESS: at example #850000, processed 308915101 words (1214117/s), 5072221 word types, 849845 tags\n",
      "2018-10-10 02:04:10,370 : INFO : PROGRESS: at example #860000, processed 312263469 words (1205398/s), 5106606 word types, 859842 tags\n",
      "2018-10-10 02:04:13,006 : INFO : PROGRESS: at example #870000, processed 315533371 words (1240653/s), 5139434 word types, 869840 tags\n",
      "2018-10-10 02:04:15,596 : INFO : PROGRESS: at example #880000, processed 318593070 words (1181923/s), 5169549 word types, 879838 tags\n",
      "2018-10-10 02:04:17,981 : INFO : PROGRESS: at example #890000, processed 321453530 words (1199388/s), 5197785 word types, 889837 tags\n",
      "2018-10-10 02:04:20,269 : INFO : PROGRESS: at example #900000, processed 324108406 words (1160570/s), 5224462 word types, 899835 tags\n",
      "2018-10-10 02:04:22,795 : INFO : PROGRESS: at example #910000, processed 327027470 words (1156035/s), 5252031 word types, 909833 tags\n",
      "2018-10-10 02:04:25,899 : INFO : PROGRESS: at example #920000, processed 330797719 words (1214907/s), 5292353 word types, 919828 tags\n",
      "2018-10-10 02:04:29,535 : INFO : PROGRESS: at example #930000, processed 335378312 words (1259913/s), 5343848 word types, 929824 tags\n",
      "2018-10-10 02:04:32,998 : INFO : PROGRESS: at example #940000, processed 339748901 words (1262196/s), 5392355 word types, 939816 tags\n",
      "2018-10-10 02:04:36,741 : INFO : PROGRESS: at example #950000, processed 344291683 words (1214051/s), 5441213 word types, 949804 tags\n",
      "2018-10-10 02:04:40,136 : INFO : PROGRESS: at example #960000, processed 348579199 words (1263022/s), 5488094 word types, 959799 tags\n",
      "2018-10-10 02:04:43,064 : INFO : PROGRESS: at example #970000, processed 352226463 words (1245732/s), 5527017 word types, 969787 tags\n",
      "2018-10-10 02:04:46,233 : INFO : PROGRESS: at example #980000, processed 356099806 words (1222634/s), 5568345 word types, 979774 tags\n",
      "2018-10-10 02:04:50,043 : INFO : PROGRESS: at example #990000, processed 360261885 words (1092605/s), 5612602 word types, 989766 tags\n",
      "2018-10-10 02:04:53,387 : INFO : PROGRESS: at example #1000000, processed 364513290 words (1271283/s), 5656793 word types, 999754 tags\n",
      "2018-10-10 02:04:56,739 : INFO : PROGRESS: at example #1010000, processed 368614335 words (1223717/s), 5700451 word types, 1009747 tags\n",
      "2018-10-10 02:04:59,962 : INFO : PROGRESS: at example #1020000, processed 372448991 words (1190009/s), 5741082 word types, 1019740 tags\n",
      "2018-10-10 02:05:03,214 : INFO : PROGRESS: at example #1030000, processed 376542716 words (1259199/s), 5783172 word types, 1029735 tags\n",
      "2018-10-10 02:05:06,575 : INFO : PROGRESS: at example #1040000, processed 380724367 words (1244314/s), 5826542 word types, 1039728 tags\n",
      "2018-10-10 02:05:09,827 : INFO : PROGRESS: at example #1050000, processed 384819743 words (1259471/s), 5868173 word types, 1049717 tags\n",
      "2018-10-10 02:05:13,106 : INFO : PROGRESS: at example #1060000, processed 388811116 words (1217522/s), 5908565 word types, 1059713 tags\n",
      "2018-10-10 02:05:16,111 : INFO : PROGRESS: at example #1070000, processed 392528903 words (1237660/s), 5946139 word types, 1069711 tags\n",
      "2018-10-10 02:05:17,551 : INFO : PROGRESS: at example #1080000, processed 394034357 words (1045741/s), 5961013 word types, 1079702 tags\n",
      "2018-10-10 02:05:18,896 : INFO : PROGRESS: at example #1090000, processed 395414579 words (1026414/s), 5974493 word types, 1089700 tags\n",
      "2018-10-10 02:05:21,724 : INFO : PROGRESS: at example #1100000, processed 398895667 words (1231406/s), 6009284 word types, 1099691 tags\n",
      "2018-10-10 02:05:24,992 : INFO : PROGRESS: at example #1110000, processed 402883656 words (1220238/s), 6048221 word types, 1109686 tags\n",
      "2018-10-10 02:05:28,254 : INFO : PROGRESS: at example #1120000, processed 406846053 words (1214996/s), 6086271 word types, 1119676 tags\n",
      "2018-10-10 02:05:31,483 : INFO : PROGRESS: at example #1130000, processed 410751694 words (1209935/s), 6123282 word types, 1129666 tags\n",
      "2018-10-10 02:05:34,385 : INFO : PROGRESS: at example #1140000, processed 414230642 words (1199030/s), 6156574 word types, 1139652 tags\n",
      "2018-10-10 02:05:37,505 : INFO : PROGRESS: at example #1150000, processed 417936758 words (1187943/s), 6192089 word types, 1149648 tags\n",
      "2018-10-10 02:05:40,602 : INFO : PROGRESS: at example #1160000, processed 421728514 words (1224600/s), 6227927 word types, 1159642 tags\n",
      "2018-10-10 02:05:43,414 : INFO : PROGRESS: at example #1170000, processed 425239148 words (1248697/s), 6260720 word types, 1169633 tags\n",
      "2018-10-10 02:05:46,481 : INFO : PROGRESS: at example #1180000, processed 429051970 words (1243651/s), 6297014 word types, 1179626 tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-10 02:05:49,654 : INFO : PROGRESS: at example #1190000, processed 432913023 words (1217047/s), 6333309 word types, 1189619 tags\n",
      "2018-10-10 02:05:52,969 : INFO : PROGRESS: at example #1200000, processed 436896865 words (1201701/s), 6371167 word types, 1199610 tags\n",
      "2018-10-10 02:05:56,121 : INFO : PROGRESS: at example #1210000, processed 440626026 words (1183670/s), 6406729 word types, 1209596 tags\n",
      "2018-10-10 02:05:59,320 : INFO : PROGRESS: at example #1220000, processed 444516229 words (1215964/s), 6442918 word types, 1219585 tags\n",
      "2018-10-10 02:06:02,408 : INFO : PROGRESS: at example #1230000, processed 448153071 words (1178019/s), 6476143 word types, 1229579 tags\n",
      "2018-10-10 02:06:05,362 : INFO : PROGRESS: at example #1240000, processed 451690387 words (1197603/s), 6509373 word types, 1239567 tags\n",
      "2018-10-10 02:06:08,302 : INFO : PROGRESS: at example #1250000, processed 455303178 words (1229449/s), 6542394 word types, 1249562 tags\n",
      "2018-10-10 02:06:11,720 : INFO : PROGRESS: at example #1260000, processed 459123055 words (1117455/s), 6577477 word types, 1259557 tags\n",
      "2018-10-10 02:06:14,888 : INFO : PROGRESS: at example #1270000, processed 462937579 words (1204453/s), 6612077 word types, 1269553 tags\n",
      "2018-10-10 02:06:17,770 : INFO : PROGRESS: at example #1280000, processed 466470457 words (1226124/s), 6642389 word types, 1279551 tags\n",
      "2018-10-10 02:06:20,854 : INFO : PROGRESS: at example #1290000, processed 470216234 words (1214938/s), 6675919 word types, 1289548 tags\n",
      "2018-10-10 02:06:23,935 : INFO : PROGRESS: at example #1300000, processed 473816185 words (1168468/s), 6706994 word types, 1299543 tags\n",
      "2018-10-10 02:06:26,882 : INFO : PROGRESS: at example #1310000, processed 477328953 words (1192335/s), 6739303 word types, 1309530 tags\n",
      "2018-10-10 02:06:29,782 : INFO : PROGRESS: at example #1320000, processed 480797346 words (1196605/s), 6770054 word types, 1319523 tags\n",
      "2018-10-10 02:06:32,812 : INFO : PROGRESS: at example #1330000, processed 484577592 words (1247759/s), 6802458 word types, 1329512 tags\n",
      "2018-10-10 02:06:35,862 : INFO : PROGRESS: at example #1340000, processed 488412901 words (1257448/s), 6834983 word types, 1339504 tags\n",
      "2018-10-10 02:06:38,888 : INFO : PROGRESS: at example #1350000, processed 492113241 words (1223203/s), 6867029 word types, 1349499 tags\n",
      "2018-10-10 02:06:41,436 : INFO : PROGRESS: at example #1360000, processed 495147897 words (1191126/s), 6894383 word types, 1359497 tags\n",
      "2018-10-10 02:06:43,925 : INFO : PROGRESS: at example #1370000, processed 498062738 words (1171644/s), 6921775 word types, 1369494 tags\n",
      "2018-10-10 02:06:46,387 : INFO : PROGRESS: at example #1380000, processed 500930290 words (1164924/s), 6948103 word types, 1379492 tags\n",
      "2018-10-10 02:06:48,897 : INFO : PROGRESS: at example #1390000, processed 503877181 words (1174461/s), 6974042 word types, 1389489 tags\n",
      "2018-10-10 02:06:51,428 : INFO : PROGRESS: at example #1400000, processed 506797117 words (1153910/s), 7002081 word types, 1399485 tags\n",
      "2018-10-10 02:06:53,898 : INFO : PROGRESS: at example #1410000, processed 509653588 words (1156488/s), 7027995 word types, 1409475 tags\n",
      "2018-10-10 02:06:56,480 : INFO : PROGRESS: at example #1420000, processed 512643545 words (1158404/s), 7054884 word types, 1419470 tags\n",
      "2018-10-10 02:06:58,926 : INFO : PROGRESS: at example #1430000, processed 515550946 words (1189204/s), 7081285 word types, 1429462 tags\n",
      "2018-10-10 02:07:01,475 : INFO : PROGRESS: at example #1440000, processed 518720003 words (1243665/s), 7110491 word types, 1439456 tags\n",
      "2018-10-10 02:07:03,902 : INFO : PROGRESS: at example #1450000, processed 521663815 words (1212997/s), 7138622 word types, 1449452 tags\n",
      "2018-10-10 02:07:06,321 : INFO : PROGRESS: at example #1460000, processed 524480806 words (1164957/s), 7165516 word types, 1459447 tags\n",
      "2018-10-10 02:07:08,586 : INFO : PROGRESS: at example #1470000, processed 527115503 words (1163249/s), 7190430 word types, 1469442 tags\n",
      "2018-10-10 02:07:10,843 : INFO : PROGRESS: at example #1480000, processed 529780825 words (1181469/s), 7214570 word types, 1479435 tags\n",
      "2018-10-10 02:07:13,444 : INFO : PROGRESS: at example #1490000, processed 532952136 words (1219689/s), 7242739 word types, 1489427 tags\n",
      "2018-10-10 02:07:16,351 : INFO : PROGRESS: at example #1500000, processed 536593246 words (1252559/s), 7273204 word types, 1499420 tags\n",
      "2018-10-10 02:07:19,222 : INFO : PROGRESS: at example #1510000, processed 540150210 words (1239301/s), 7302430 word types, 1509410 tags\n",
      "2018-10-10 02:07:21,961 : INFO : PROGRESS: at example #1520000, processed 543435933 words (1200020/s), 7328672 word types, 1519402 tags\n",
      "2018-10-10 02:07:24,718 : INFO : PROGRESS: at example #1530000, processed 546860282 words (1241971/s), 7357019 word types, 1529397 tags\n",
      "2018-10-10 02:07:27,700 : INFO : PROGRESS: at example #1540000, processed 550512629 words (1225094/s), 7387199 word types, 1539386 tags\n",
      "2018-10-10 02:07:30,508 : INFO : PROGRESS: at example #1550000, processed 553906897 words (1209190/s), 7415597 word types, 1549379 tags\n",
      "2018-10-10 02:07:33,490 : INFO : PROGRESS: at example #1560000, processed 557445583 words (1186811/s), 7445872 word types, 1559373 tags\n",
      "2018-10-10 02:07:36,296 : INFO : PROGRESS: at example #1570000, processed 560862717 words (1218129/s), 7475258 word types, 1569366 tags\n",
      "2018-10-10 02:07:38,943 : INFO : PROGRESS: at example #1580000, processed 564126992 words (1233490/s), 7502159 word types, 1579362 tags\n",
      "2018-10-10 02:07:41,671 : INFO : PROGRESS: at example #1590000, processed 567513308 words (1241508/s), 7529607 word types, 1589357 tags\n",
      "2018-10-10 02:07:44,396 : INFO : PROGRESS: at example #1600000, processed 570899643 words (1242969/s), 7556858 word types, 1599348 tags\n",
      "2018-10-10 02:07:47,522 : INFO : PROGRESS: at example #1610000, processed 574278939 words (1081155/s), 7583840 word types, 1609344 tags\n",
      "2018-10-10 02:07:50,272 : INFO : PROGRESS: at example #1620000, processed 577687039 words (1239705/s), 7611879 word types, 1619335 tags\n",
      "2018-10-10 02:07:53,076 : INFO : PROGRESS: at example #1630000, processed 581162019 words (1239401/s), 7640106 word types, 1629330 tags\n",
      "2018-10-10 02:07:55,977 : INFO : PROGRESS: at example #1640000, processed 584590361 words (1181954/s), 7667071 word types, 1639323 tags\n",
      "2018-10-10 02:07:58,798 : INFO : PROGRESS: at example #1650000, processed 587944345 words (1189050/s), 7692533 word types, 1649319 tags\n",
      "2018-10-10 02:08:01,770 : INFO : PROGRESS: at example #1660000, processed 591541756 words (1210807/s), 7720345 word types, 1659310 tags\n",
      "2018-10-10 02:08:04,707 : INFO : PROGRESS: at example #1670000, processed 595082654 words (1205817/s), 7748880 word types, 1669301 tags\n",
      "2018-10-10 02:08:07,647 : INFO : PROGRESS: at example #1680000, processed 598624602 words (1204821/s), 7776737 word types, 1679291 tags\n",
      "2018-10-10 02:08:10,248 : INFO : PROGRESS: at example #1690000, processed 601752651 words (1203466/s), 7801892 word types, 1689283 tags\n",
      "2018-10-10 02:08:13,151 : INFO : PROGRESS: at example #1700000, processed 605248814 words (1204603/s), 7828738 word types, 1699274 tags\n",
      "2018-10-10 02:08:16,092 : INFO : PROGRESS: at example #1710000, processed 608706701 words (1176060/s), 7855083 word types, 1709267 tags\n",
      "2018-10-10 02:08:19,178 : INFO : PROGRESS: at example #1720000, processed 612404028 words (1198455/s), 7883534 word types, 1719259 tags\n",
      "2018-10-10 02:08:31,981 : INFO : PROGRESS: at example #1730000, processed 629619789 words (1344649/s), 8017654 word types, 1729218 tags\n",
      "2018-10-10 02:08:42,633 : INFO : PROGRESS: at example #1740000, processed 644401624 words (1387830/s), 8139191 word types, 1739178 tags\n",
      "2018-10-10 02:08:52,283 : INFO : PROGRESS: at example #1750000, processed 657659547 words (1373980/s), 8246739 word types, 1749124 tags\n",
      "2018-10-10 02:09:00,636 : INFO : PROGRESS: at example #1760000, processed 669048196 words (1363530/s), 8340201 word types, 1759095 tags\n",
      "2018-10-10 02:09:08,470 : INFO : PROGRESS: at example #1770000, processed 679499640 words (1334089/s), 8422105 word types, 1769064 tags\n",
      "2018-10-10 02:09:15,924 : INFO : PROGRESS: at example #1780000, processed 689588791 words (1353719/s), 8506633 word types, 1779020 tags\n",
      "2018-10-10 02:09:21,146 : INFO : PROGRESS: at example #1790000, processed 696325773 words (1290263/s), 8562305 word types, 1788997 tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-10 02:09:23,794 : INFO : PROGRESS: at example #1800000, processed 699548234 words (1217189/s), 8586039 word types, 1798991 tags\n",
      "2018-10-10 02:09:26,458 : INFO : PROGRESS: at example #1810000, processed 702806130 words (1223420/s), 8611092 word types, 1808986 tags\n",
      "2018-10-10 02:09:29,124 : INFO : PROGRESS: at example #1820000, processed 706100126 words (1235625/s), 8634997 word types, 1818981 tags\n",
      "2018-10-10 02:09:31,743 : INFO : PROGRESS: at example #1830000, processed 709309306 words (1225790/s), 8658286 word types, 1828976 tags\n",
      "2018-10-10 02:09:34,343 : INFO : PROGRESS: at example #1840000, processed 712454652 words (1209987/s), 8681124 word types, 1838967 tags\n",
      "2018-10-10 02:09:36,881 : INFO : PROGRESS: at example #1850000, processed 715527626 words (1211165/s), 8703905 word types, 1848963 tags\n",
      "2018-10-10 02:09:39,507 : INFO : PROGRESS: at example #1860000, processed 718644369 words (1187065/s), 8727316 word types, 1858960 tags\n",
      "2018-10-10 02:09:42,098 : INFO : PROGRESS: at example #1870000, processed 721800974 words (1218675/s), 8750605 word types, 1868955 tags\n",
      "2018-10-10 02:09:44,600 : INFO : PROGRESS: at example #1880000, processed 724862159 words (1223611/s), 8772794 word types, 1878954 tags\n",
      "2018-10-10 02:09:47,142 : INFO : PROGRESS: at example #1890000, processed 727960134 words (1218974/s), 8794800 word types, 1888950 tags\n",
      "2018-10-10 02:09:49,715 : INFO : PROGRESS: at example #1900000, processed 731045070 words (1199350/s), 8817813 word types, 1898946 tags\n",
      "2018-10-10 02:09:52,298 : INFO : PROGRESS: at example #1910000, processed 734107317 words (1185689/s), 8840212 word types, 1908936 tags\n",
      "2018-10-10 02:09:54,946 : INFO : PROGRESS: at example #1920000, processed 737320684 words (1213570/s), 8863632 word types, 1918932 tags\n",
      "2018-10-10 02:09:57,506 : INFO : PROGRESS: at example #1930000, processed 740468510 words (1230160/s), 8886628 word types, 1928923 tags\n",
      "2018-10-10 02:10:00,056 : INFO : PROGRESS: at example #1940000, processed 743532196 words (1201799/s), 8909244 word types, 1938920 tags\n",
      "2018-10-10 02:10:02,644 : INFO : PROGRESS: at example #1950000, processed 746654780 words (1206726/s), 8931509 word types, 1948916 tags\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-63facf856a19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Doc2Vec model initialization and parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbow_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm_concat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#Model Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, documents, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m         \"\"\"\n\u001b[1;32m    728\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[0;32m--> 729\u001b[0;31m             documents, self.docvecs, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         report_values = self.vocabulary.prepare_vocab(\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, documents, docvecs, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0mchecked_string_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdocument_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-80a01e0338b8>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msource\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mcnt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m\"<doc\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m           \u001b[0;31m# Every new document starts with this format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                         \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;34m\"\"\" Return the next decoded line from the input stream.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;34m\"\"\" Return the next decoded line from the input stream.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size, keepends)\u001b[0m\n\u001b[1;32m    595\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# we didn't get anything or this was our only try\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkeepends\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeepends\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#Doc2Vec model initialization and parameters \n",
    "model = Doc2Vec(vector_size=300, window=15, min_count=20, sample=1e-5, workers=cores, hs=0, dm=0, negative=5, dbow_words=1, dm_concat=0)\n",
    "model.build_vocab(sentences)\n",
    "\n",
    "#Model Training \n",
    "for epoch in range(int(args.epochs)):\n",
    "    model.train(sentences, total_examples=2215484)\n",
    "    print(\"Epoch completed: \"+str(epoch+1))\n",
    "\n",
    "model.save(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c5fb1181eea3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(int(args.epochs)):\n",
    "    print(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
