{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dictionary from ../data/preprocessed/LDAmodel/OnlineParticipation_fullset_nouns_bow.dict\n",
      "loading corpus from ../data/preprocessed/LDAmodel/OnlineParticipation_fullset_nouns_bow.mm\n",
      "loading texts from ../data/preprocessed/LDAmodel/OnlineParticipation_fullset_nouns_texts.json\n",
      "Loading model from ../data/preprocessed/LDAmodel/a42/OnlineParticipation_LDAmodel_a42_10\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from itertools import chain, islice\n",
    "from os.path import join\n",
    "#from time import time\n",
    "\n",
    "#import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "#from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "from constants import (\n",
    "    ETL_PATH\n",
    ")\n",
    "from topic_coherence_experiments import TopicsLoader\n",
    "\n",
    "#import warnings\n",
    "#warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n",
    "#from utils import tprint\n",
    "\n",
    "pd.options.display.max_rows = 2001\n",
    "pd.options.display.precision = 3\n",
    "#np.set_printoptions(precision=3, threshold=None, edgeitems=None, linewidth=800, suppress=None)\n",
    "\n",
    "datasets = {\n",
    "    'E': 'Europarl',\n",
    "    'FA': 'FAZ_combined',\n",
    "    'FO': 'FOCUS_cleansed',\n",
    "    'O': 'OnlineParticipation',\n",
    "    'P': 'PoliticalSpeeches',\n",
    "    'dewi': 'dewiki',\n",
    "    'dewa': 'dewac',\n",
    "}\n",
    "dataset=datasets['O']\n",
    "params_list = ['a42', 'b42', 'c42', 'd42']\n",
    "nbs_topics = [10, 25, 50, 100]\n",
    "param_id = params_list[0]\n",
    "nb_topics = nbs_topics[0]\n",
    "\n",
    "topicsloader = TopicsLoader(dataset=dataset, param_ids=[param_id], nbs_topics=[nb_topics])\n",
    "ldamodel = topicsloader.ldamodels[0]\n",
    "corpus = topicsloader.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadin ../data/preprocessed/LDAmodel/a42/OnlineParticipation_LDAmodel_a42_10_stats.json\n",
      "split dataset. size of: train_set=20531, val_set=2566, test_set=2567,\n"
     ]
    }
   ],
   "source": [
    "def split_corpus(corpus, max_test_size_rel=0.1, max_test_size_abs=5000):\n",
    "    length = len(corpus)\n",
    "    corpora = dict()\n",
    "    if length*max_test_size_rel < max_test_size_abs:\n",
    "        split1 = int(length*(1-(2*max_test_size_rel)))\n",
    "        split2 = int(length*(1-max_test_size_rel))\n",
    "    else:\n",
    "        split1 = length-(2*max_test_size_abs)\n",
    "        split2 = length-max_test_size_abs\n",
    "    corpora['training_corpus'] = corpus[:split1]\n",
    "    corpora['holdout_corpus'] = corpus[split1:split2]\n",
    "    corpora['test_corpus'] = corpus[split2:]\n",
    "    print(\n",
    "        f'split dataset. size of:',\n",
    "        f'train_set={split1},',\n",
    "        f'val_set={split2 - split1},',\n",
    "        f'test_set={len(corpus) - split2},'\n",
    "    )\n",
    "    return corpora\n",
    "\n",
    "statsfile = f'{dataset}_LDAmodel_{param_id}_{nb_topics}_stats.json'\n",
    "statspath = join(ETL_PATH, 'LDAmodel', param_id, statsfile)\n",
    "with open(statspath, 'r') as fp:\n",
    "    print('Loadin', statspath)\n",
    "    stats = json.load(fp)\n",
    "    \n",
    "corpora = split_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = {\n",
    "    'a42': None,\n",
    "    'b42': 200,\n",
    "    'c42': 1_000,\n",
    "    'd42': 200,\n",
    "}\n",
    "ldamodel[-1].update(\n",
    "    corpus=corpora['training_corpus'], \n",
    "    chunksize=20_000,\n",
    "    passes=10, \n",
    "    iterations=iterations[param_id],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "metrics = ldamodel[-1].metrics\n",
    "out_dir = join(ETL_PATH, f'LDAmodel/{param_id}')\n",
    "out = join(out_dir, f'{dataset}_LDAmodel_{param_id}_{nb_topics}')\n",
    "with open(out + '_resume_metrics.json', 'w') as fp:\n",
    "    serializable_metrics = {}\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v[0], np.ndarray):\n",
    "            serializable_metrics[k] = [x.tolist() for x in v]\n",
    "        else:\n",
    "            serializable_metrics[k] = [float(x) for x in v]\n",
    "    json.dump(serializable_metrics, fp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
