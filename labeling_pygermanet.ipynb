{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygermanet import load_germanet\n",
    "gn = load_germanet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset(auseinandergehen.v.3),\n",
       " Synset(funktionieren.v.1),\n",
       " Synset(funktionieren.v.2),\n",
       " Synset(gehen.v.1),\n",
       " Synset(gehen.v.4),\n",
       " Synset(gehen.v.5),\n",
       " Synset(gehen.v.6),\n",
       " Synset(gehen.v.7),\n",
       " Synset(gehen.v.9),\n",
       " Synset(gehen.v.10),\n",
       " Synset(gehen.v.11),\n",
       " Synset(gehen.v.12),\n",
       " Synset(gehen.v.13),\n",
       " Synset(gehen.v.14),\n",
       " Synset(handeln.v.1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gn.synsets('gehen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['diejenigen']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gn.lemmatise(u'ginge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset(funktionieren.v.2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funktionieren = gn.synset(u'funktionieren.v.2')\n",
    "funktionieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset(vorgehen.v.1), Synset(leerlaufen.v.2)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funktionieren.hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset(GNROOT.n.1),\n",
       "  Synset(Entität.n.2),\n",
       "  Synset(Objekt.n.4),\n",
       "  Synset(Ding.n.2),\n",
       "  Synset(Teil.n.2),\n",
       "  Synset(Teilmenge.n.2),\n",
       "  Synset(Gruppe.n.1),\n",
       "  Synset(biologische Gruppe.n.1),\n",
       "  Synset(Spezies.n.1),\n",
       "  Synset(Rasse.n.1),\n",
       "  Synset(Tierrasse.n.1),\n",
       "  Synset(Hunderasse.n.1),\n",
       "  Synset(Husky.n.1)],\n",
       " [Synset(GNROOT.n.1),\n",
       "  Synset(Entität.n.2),\n",
       "  Synset(kognitives Objekt.n.1),\n",
       "  Synset(Kategorie.n.1),\n",
       "  Synset(Art.n.1),\n",
       "  Synset(Spezies.n.1),\n",
       "  Synset(Rasse.n.1),\n",
       "  Synset(Tierrasse.n.1),\n",
       "  Synset(Hunderasse.n.1),\n",
       "  Synset(Husky.n.1)],\n",
       " [Synset(GNROOT.n.1),\n",
       "  Synset(Entität.n.2),\n",
       "  Synset(Objekt.n.4),\n",
       "  Synset(natürliches Objekt.n.1),\n",
       "  Synset(Kreatur.n.1),\n",
       "  Synset(Organismus.n.1),\n",
       "  Synset(höheres Lebewesen.n.1),\n",
       "  Synset(Tier.n.1),\n",
       "  Synset(Gewebetier.n.1),\n",
       "  Synset(Chordatier.n.1),\n",
       "  Synset(Wirbeltier.n.1),\n",
       "  Synset(Säugetier.n.1),\n",
       "  Synset(Plazentatier.n.1),\n",
       "  Synset(Raubtier.n.1),\n",
       "  Synset(Landraubtier.n.1),\n",
       "  Synset(hundeartiges Landraubtier.n.1),\n",
       "  Synset(Hund.n.2),\n",
       "  Synset(Husky.n.1)],\n",
       " [Synset(GNROOT.n.1),\n",
       "  Synset(Entität.n.2),\n",
       "  Synset(Objekt.n.4),\n",
       "  Synset(natürliches Objekt.n.1),\n",
       "  Synset(Kreatur.n.1),\n",
       "  Synset(Organismus.n.1),\n",
       "  Synset(höheres Lebewesen.n.1),\n",
       "  Synset(Tier.n.1),\n",
       "  Synset(Haustier.n.1),\n",
       "  Synset(Hund.n.2),\n",
       "  Synset(Husky.n.1)]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gn.synset('Husky.n.1').hypernym_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma(funktionieren.v.2.funktionieren),\n",
       " Lemma(funktionieren.v.2.funzen),\n",
       " Lemma(funktionieren.v.2.gehen),\n",
       " Lemma(funktionieren.v.2.laufen),\n",
       " Lemma(funktionieren.v.2.arbeiten)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funktionieren.lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma(brennen.v.1.brennen),\n",
       " Lemma(verbrennen.v.1.brennen),\n",
       " Lemma(brennen.v.3.brennen),\n",
       " Lemma(brennen.v.4.brennen),\n",
       " Lemma(brennen.v.5.brennen),\n",
       " Lemma(destillieren.v.1.brennen),\n",
       " Lemma(brennen.v.7.brennen),\n",
       " Lemma(brennen.v.8.brennen)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gn.lemmas('brennen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e82f01f26255>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgur65\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mgur65\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_gurevych\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mgn\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mload_germanet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-e82f01f26255>\u001b[0m in \u001b[0;36mload_gurevych\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;31m# fix typo in gur65\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Reis'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Reise'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from pygermanet import load_germanet, Synset\n",
    "from scipy.stats.stats import pearsonr\n",
    "#import codecs\n",
    "import numpy as np\n",
    "\n",
    "GUR65_FILENAME = '../data/GermanRelatednessDatasets/wortpaare350.gold.pos.txt'\n",
    "\n",
    "def load_gurevych():\n",
    "    gur65 = []\n",
    "    with open(GUR65_FILENAME, 'r') as input_file:\n",
    "        for idx, line in enumerate(input_file):\n",
    "            fields = line.strip().split(';')\n",
    "            if idx == 0:\n",
    "                header = fields\n",
    "            else:\n",
    "                # fix typo in gur65\n",
    "                fields[1] = {'Reis': 'Reise'}.get(fields[1], fields[1])\n",
    "                fields[2] = float(fields[2])\n",
    "                fields[3] = float(fields[3])\n",
    "                gur65.append(fields)\n",
    "    gur65 = np.core.records.array(\n",
    "        gur65,\n",
    "        dtype=np.dtype({'formats': ['U30', 'U30', '<f8', '<f8'],\n",
    "                        'names': header}))\n",
    "    return gur65\n",
    "\n",
    "gur65 = load_gurevych()\n",
    "gn    = load_germanet()\n",
    "\n",
    "# select those words which are found in GermaNet; exclude the\n",
    "# adjective \"jung\"\n",
    "pred = lambda w1, w2: bool(gn.synsets(w1) and gn.synsets(w2) and\n",
    "                           w1 != 'jung' and w2 != 'jung')\n",
    "\n",
    "print('Semantic similarity computed on {0} of {1} word pairs'.format(\n",
    "    sum([1 for word1, word2 in zip(gur65['Word1'], gur65['Word2'])\n",
    "         if pred(word1, word2)]),\n",
    "    len(gur65)))\n",
    "\n",
    "sim_funcs = [('lch', Synset.sim_lch,  np.max),\n",
    "             ('res', Synset.sim_res,  np.max),\n",
    "             ('jcn', Synset.dist_jcn, np.min),\n",
    "             ('lin', Synset.sim_lin,  np.max)]\n",
    "\n",
    "print()\n",
    "print('metric   r')\n",
    "print('---------------')\n",
    "for sim_name, sim_func, comb_func in sim_funcs:\n",
    "    scores = []\n",
    "    for word1, word2, human, _hstd in gur65:\n",
    "        if not pred(word1, word2):\n",
    "            continue\n",
    "        score = comb_func(np.array([sim_func(ss1, ss2)\n",
    "                                    for ss1 in gn.synsets(word1)\n",
    "                                    for ss2 in gn.synsets(word2)]))\n",
    "        scores.append([score, human])\n",
    "    scores = np.array(scores)\n",
    "    r, _p = pearsonr(scores[:,0],scores[:,1])\n",
    "    print('{0}      {1:.3f}'.format(sim_name, r))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
