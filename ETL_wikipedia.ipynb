{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "import locale; locale.setlocale(locale.LC_ALL, '')\n",
    "import xml.etree.ElementTree as et\n",
    "from html import unescape\n",
    "from datetime import datetime\n",
    "\n",
    "from constants import DATA_BASE, ETL_PATH, \\\n",
    "    META, DATASET, SUBSET, ID, ID2, TITLE, TAGS, TIME, DESCRIPTION, TEXT, LINKS, DATA, HASH\n",
    "\n",
    "CORPUS = \"dewiki\"\n",
    "LOCAL_PATH = \"dewiki/dewiki-latest-pages-articles.xml\"\n",
    "IN_PATH = join(DATA_BASE, LOCAL_PATH)\n",
    "OUT_PATH = join(ETL_PATH, CORPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matches against: [[Kategorie:Soziologische Systemtheorie]], [[Kategorie:Fiktive Person|Smithee, Alan]]\n",
    "category = r\"\\[\\[Kategorie:(?P<cat>[\\w ]+)(?:\\|.*)?\\]\\]\"\n",
    "re_category = re.compile(category)\n",
    "\n",
    "# remove tags\n",
    "refs = r\"<\\s*(ref|math)[^>.]*?(?:\\/\\s*>|>.*?<\\s*\\/\\s*(ref|math)\\s*>)\"\n",
    "tags = r\"<(.|\\n)*?>\"\n",
    "re_tags = re.compile(r\"(%s|%s)\" % (refs, tags), re.MULTILINE)\n",
    "\n",
    "table = r\"{\\|(?s:.*?)\\|}\"\n",
    "re_table = re.compile(table)\n",
    "\n",
    "# remove meta data: [[Datei:...]]\n",
    "chars = r\"\\xa0\"\n",
    "emph = r\"\\'{2,}\"\n",
    "bullet = r\"^[\\*:] *\"\n",
    "bullet2 = r\"^\\|.*\"\n",
    "meta = r\"\\[\\[\\w+:.*?\\]\\]\"\n",
    "footer = r\"== (Bibliographie|Literatur|Weblinks|Einzelnachweise) ==(?s:.)*\"\n",
    "re_meta = re.compile(r\"(%s|%s|%s|%s|%s|%s)\" % (chars, emph, bullet, bullet2, meta, footer), re.MULTILINE)\n",
    "\n",
    "# => merge ^\n",
    "remove = r'(' + r'|'.join([refs, tags, table, chars, emph, bullet, bullet2, meta, footer]) + r')'\n",
    "re_remove = re.compile(remove, re.MULTILINE)\n",
    "\n",
    "# matches against: [[Aristoteles]], [[Reductio ad absurdum|indirekten Beweis]]\n",
    "wikilink = r\"\\[\\[(.*?)\\]\\]\"\n",
    "re_link = re.compile(wikilink)\n",
    "\n",
    "zitat = r\"{{Zitat(?:\\||-.*?\\|Übersetzung=)(?P<token>.*?)(?:\\|.*?)?}}\"\n",
    "re_zitat = re.compile(zitat)\n",
    "\n",
    "replace = r'(' + r'|'.join([wikilink, zitat]) + r')'\n",
    "re_replace = re.compile(replace)\n",
    "\n",
    "infobox = r\"{{.*?(?:}}|(?={{))\"\n",
    "re_infobox = re.compile(infobox, re.DOTALL)\n",
    "\n",
    "lf = r\"\\n\\n*\\n(?!==)\"\n",
    "re_lf = re.compile(lf)\n",
    "\n",
    "\n",
    "def parse_markdown(text):\n",
    "    \"\"\"\n",
    "    This is actucally not a real parser since it keeps no internal states. Therefore nested structures\n",
    "    are a bit of a problem and a few artifacty may remain. Also the regexes are a bit nasty and need to\n",
    "    read the text multiple times. Maybe I'm doing a new version at some point, but for the time being\n",
    "    it's working sufficiently well.\n",
    "    \"\"\"\n",
    "    # replace html escapings\n",
    "    text = unescape(text)\n",
    "\n",
    "    # extract categories\n",
    "    categories = re_category.findall(text)\n",
    "\n",
    "    # remove formatting tags and ref/math tags with content\n",
    "    # This regex is actually working way better than the w3lib.remove_tags[_with_content] implementations.\n",
    "    # It's ~1.5x faster and keeps all wanted content, while the w3lib methods introduce problems with some\n",
    "    # self-closing xml-tags. Of course lxml/beautifulsoup would be another option.\n",
    "    text = re_tags.sub('', text)\n",
    "\n",
    "    # remove tables\n",
    "    text = re_table.sub('', text)\n",
    "\n",
    "    # remove metadata and formatting\n",
    "    text = re_meta.sub('', text)\n",
    "\n",
    "    # replace citations\n",
    "    text = re_zitat.sub(r\"„\\g<token>”\", text)\n",
    "\n",
    "    # replace WikiLinks\n",
    "    links = []\n",
    "\n",
    "    def replace_links(matchobj):\n",
    "        split = matchobj.group(1).split('|', 1)\n",
    "        links.append(split[0])\n",
    "        return split[1] if len(split) > 1 else split[0]\n",
    "\n",
    "    text = re_link.sub(replace_links, text)\n",
    "\n",
    "    # repeat for nested structures, performancewise not perfect\n",
    "    n = 1\n",
    "    while n > 0:\n",
    "        text, n = re_infobox.subn('', text)\n",
    "\n",
    "    text = re_lf.sub('\\n', text)\n",
    "    return text.strip(' \\n}{'), categories, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(infile, outfile, iterations, write_every=None, print_every=1000):\n",
    "    \"\"\"\n",
    "    :param infile: path to Wikipedia xml dump\n",
    "    :param outfile: path to writable csv file\n",
    "    :param iterations: number all articles to process and write. None for all.\n",
    "    :param write_every: append to csv file every m articles. \n",
    "                        if None: writes a pickled DataFrame *after* processing the entire corpus \n",
    "                                disadvantage: high memory consumption\n",
    "                                advantage: keeps objects as byte streams\n",
    "    :param print_every: print progress to stdout every n articles\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    outfile += '.csv' if write_every else '.pickle'\n",
    "    \n",
    "    with open(infile, 'r') as fr, open(outfile, 'w') as fw:\n",
    "\n",
    "        fields = [HASH] + META + DATA\n",
    "        writer = csv.DictWriter(fw, fields, quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writeheader()\n",
    "\n",
    "        article_count = 0\n",
    "        rows = []\n",
    "        row = None\n",
    "        is_article = is_redirect = False\n",
    "\n",
    "        for event, elem in et.iterparse(fr, events=['start', 'end']):\n",
    "            tag = strip_tag(elem.tag)\n",
    "\n",
    "            if event == 'start':\n",
    "                # start new row\n",
    "                if tag == 'page':\n",
    "                    row = dict()\n",
    "            else:\n",
    "                if tag == 'title':\n",
    "                    row[TITLE], row[DESCRIPTION] = split_title(elem.text)\n",
    "                elif tag == 'ns' and int(elem.text) == 0:\n",
    "                    is_article = True\n",
    "                    row[DATASET] = CORPUS\n",
    "                    row[SUBSET] = \"\"\n",
    "                elif tag == 'id' and tag not in row:\n",
    "                    row[ID] = elem.text\n",
    "                    row[ID2] = 0\n",
    "                elif tag == 'timestamp':\n",
    "                    row[TIME] = datetime.strptime(elem.text.replace('Z', 'UTC'), '%Y-%m-%dT%H:%M:%S%Z')  # 2018-07-29T18:22:20Z\n",
    "                elif tag == 'redirect':\n",
    "                    is_redirect = True\n",
    "                    row[LINKS] = elem.get('title')\n",
    "                elif tag == 'text' and is_article:\n",
    "                    if not is_redirect:\n",
    "                        row[TEXT], row[TAGS], row[LINKS] = parse_markdown(elem.text)\n",
    "                        row[TAGS] = tuple(row[TAGS])\n",
    "                        # dump empty pages\n",
    "                        if not row[TEXT]:\n",
    "                            is_article = False\n",
    "                    else:\n",
    "                        row[TEXT], row[TAGS], row[LINKS] = \"\", tuple(), list()\n",
    "                # write and close row, reset flags\n",
    "                elif tag == 'page':\n",
    "                    if is_article:\n",
    "                        row[HASH] = hash(tuple([row[key] for key in META]))\n",
    "                        article_count += 1\n",
    "                        rows.append(row)\n",
    "                        # print status\n",
    "                        if article_count > 1:\n",
    "                            if (article_count % print_every) == 0:\n",
    "                                print(locale.format(\"%d\", article_count, grouping=True))\n",
    "                            if write_every:\n",
    "                                # if write_every is False we will save everything *after* processing\n",
    "                                if (article_count % write_every) == 0:\n",
    "                                    # write batch of rows and reset list of rows\n",
    "                                    writer.writerows(rows)\n",
    "                                    rows = []\n",
    "                    # reset everything\n",
    "                    row = None\n",
    "                    is_article = is_redirect = False\n",
    "                elem.clear()\n",
    "\n",
    "            if article_count == iterations:\n",
    "                break\n",
    "        \n",
    "    if not write_every:\n",
    "        store(rows, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_tag(tag):\n",
    "    return tag.split('}', 1)[1] if '}' in tag else tag\n",
    "\n",
    "def split_title(title):\n",
    "    split = title.find('(', 1)\n",
    "    split = None if split < 1 else split\n",
    "    return title[:split], (title[split:] if split else '')\n",
    "\n",
    "def store(rows, fname):\n",
    "    df = pd.DataFrame.from_dict(rows)\n",
    "    print('saving to', fname)\n",
    "    df = df.set_index(HASH)[META+DATA]\n",
    "    df.to_pickle(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time parse_xml(IN_PATH, OUT_PATH, iterations=2000, write_every=0, print_every=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
