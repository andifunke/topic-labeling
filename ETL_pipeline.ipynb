{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- default imports ---\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- constants definitions ---\n",
    "\n",
    "DATA_BASE = \"../../master_cloud/corpora\"\n",
    "ETL_BASE = \"preprocessed\"\n",
    "ETL_PATH = join(DATA_BASE, ETL_BASE)\n",
    "NLP_BASE = join(ETL_BASE, 'nlp')\n",
    "NLP_PATH = join(DATA_BASE, NLP_BASE)\n",
    "\n",
    "# standard meta data fields\n",
    "DATASET = 'dataset'\n",
    "SUBSET = 'subset'\n",
    "ID = 'doc_id'\n",
    "ID2 = 'doc_subid'\n",
    "TITLE = 'title'\n",
    "TAGS = 'tags'\n",
    "TIME = 'date_time'\n",
    "#AUTHOR\n",
    "#SUBTITLE\n",
    "#CATEGORY\n",
    "META = [DATASET, SUBSET, ID, ID2, TITLE, TAGS, TIME]\n",
    "TEXT = 'text'\n",
    "HASH = 'hash'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- store meta data and content ---\n",
    "\n",
    "def store(corpus, df):\n",
    "    \"\"\"returns the file name where the dataframe was stores\"\"\"\n",
    "    makedirs(ETL_PATH, exist_ok=True)\n",
    "    fname = join(ETL_PATH, corpus + '.pickle')\n",
    "    print('saving to', fname)\n",
    "    df.to_pickle(fname)\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(f):\n",
    "     return pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just READ DataFrames\n",
    "\n",
    "path = NLP_PATH\n",
    "\n",
    "files = sorted([f for f in listdir(path) if isfile(join(path, f))\n",
    "                # and f[:3] == 'dew'\n",
    "                # and f[:3] == 'Onl'\n",
    "                # and f[:3] == 'Eur'\n",
    "                # and f[:3] == 'FAZ'\n",
    "                # and f[:3] == 'Pol'\n",
    "               ])\n",
    "\n",
    "dfs = [(name, read(join(path, name))) for name in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>#documents</th>\n",
       "      <th>#sentences</th>\n",
       "      <th>#tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FAZ_nlp.pickle</td>\n",
       "      <td>51385</td>\n",
       "      <td>1837668</td>\n",
       "      <td>29527792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OnlineParticipation_nlp.pickle</td>\n",
       "      <td>25981</td>\n",
       "      <td>143384</td>\n",
       "      <td>1751790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PoliticalSpeeches_nlp.pickle</td>\n",
       "      <td>6037</td>\n",
       "      <td>221397</td>\n",
       "      <td>4136463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          dataset  #documents  #sentences   #tokens\n",
       "0                  FAZ_nlp.pickle       51385     1837668  29527792\n",
       "1  OnlineParticipation_nlp.pickle       25981      143384   1751790\n",
       "2    PoliticalSpeeches_nlp.pickle        6037      221397   4136463"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_list = [(name, df.groupby(HASH).ngroups, df.groupby([HASH, 'sent_idx']).ngroups, df.shape[0]) \n",
    "                 for name, df in dfs]\n",
    "overview = pd.DataFrame(overview_list, columns=['dataset', '#documents', '#sentences', '#tokens'])\n",
    "overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>index</th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>text</th>\n",
       "      <th>token</th>\n",
       "      <th>POS</th>\n",
       "      <th>ent_iob</th>\n",
       "      <th>noun_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>„</td>\n",
       "      <td>„</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Balancierte</td>\n",
       "      <td>balancieren</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Partnerschaft</td>\n",
       "      <td>Partnerschaft</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>“</td>\n",
       "      <td>“</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Maas</td>\n",
       "      <td>maa</td>\n",
       "      <td>ADV</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>plädiert</td>\n",
       "      <td>plädieren</td>\n",
       "      <td>VERB</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>für</td>\n",
       "      <td>für</td>\n",
       "      <td>ADP</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>neue</td>\n",
       "      <td>neu</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Amerika-Strategie</td>\n",
       "      <td>Amerika-strategie</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n</td>\n",
       "      <td></td>\n",
       "      <td>SPACE</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>Die</td>\n",
       "      <td>der</td>\n",
       "      <td>DET</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>Amtszeit</td>\n",
       "      <td>Amtszeit</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>von</td>\n",
       "      <td>von</td>\n",
       "      <td>ADP</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>Präsident</td>\n",
       "      <td>Präsident</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>Donald</td>\n",
       "      <td>Donald</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>Trump</td>\n",
       "      <td>Trump</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>I</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>könne</td>\n",
       "      <td>können</td>\n",
       "      <td>VERB</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>man</td>\n",
       "      <td>man</td>\n",
       "      <td>PRON</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>nicht</td>\n",
       "      <td>nicht</td>\n",
       "      <td>PART</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>einfach</td>\n",
       "      <td>einfach</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>aussitzen</td>\n",
       "      <td>aussitzen</td>\n",
       "      <td>VERB</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>erklärt</td>\n",
       "      <td>erklären</td>\n",
       "      <td>VERB</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>Außenminister</td>\n",
       "      <td>Außenminister</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>Maas</td>\n",
       "      <td>Maa</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>B</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>Die</td>\n",
       "      <td>der</td>\n",
       "      <td>DET</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-3019970016641425166</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>EU</td>\n",
       "      <td>EU</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527762</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>240</td>\n",
       "      <td>15</td>\n",
       "      <td>darauffolgenden</td>\n",
       "      <td>darauffolgend</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>O</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527763</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>241</td>\n",
       "      <td>15</td>\n",
       "      <td>Wochen</td>\n",
       "      <td>Woche</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>O</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527764</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>242</td>\n",
       "      <td>15</td>\n",
       "      <td>ließ</td>\n",
       "      <td>lassen</td>\n",
       "      <td>VERB</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527765</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>243</td>\n",
       "      <td>15</td>\n",
       "      <td>der</td>\n",
       "      <td>der</td>\n",
       "      <td>DET</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527766</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>244</td>\n",
       "      <td>15</td>\n",
       "      <td>Erfolg</td>\n",
       "      <td>Erfolg</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527767</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>245</td>\n",
       "      <td>15</td>\n",
       "      <td>der</td>\n",
       "      <td>der</td>\n",
       "      <td>DET</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527768</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>246</td>\n",
       "      <td>15</td>\n",
       "      <td>Kombinationstherapie</td>\n",
       "      <td>Kombinationstherapie</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527769</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>247</td>\n",
       "      <td>15</td>\n",
       "      <td>allerdings</td>\n",
       "      <td>allerdings</td>\n",
       "      <td>ADV</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527770</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>248</td>\n",
       "      <td>15</td>\n",
       "      <td>wieder</td>\n",
       "      <td>wieder</td>\n",
       "      <td>ADV</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527771</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>249</td>\n",
       "      <td>15</td>\n",
       "      <td>nach</td>\n",
       "      <td>nach</td>\n",
       "      <td>PART</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527772</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>250</td>\n",
       "      <td>15</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527773</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>251</td>\n",
       "      <td>16</td>\n",
       "      <td>Ursache</td>\n",
       "      <td>Ursache</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527774</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>252</td>\n",
       "      <td>16</td>\n",
       "      <td>hierfür</td>\n",
       "      <td>hierfür</td>\n",
       "      <td>ADV</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527775</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>253</td>\n",
       "      <td>16</td>\n",
       "      <td>war</td>\n",
       "      <td>sein</td>\n",
       "      <td>AUX</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527776</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>254</td>\n",
       "      <td>16</td>\n",
       "      <td>vermutlich</td>\n",
       "      <td>vermutlich</td>\n",
       "      <td>ADV</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527777</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>255</td>\n",
       "      <td>16</td>\n",
       "      <td>die</td>\n",
       "      <td>der</td>\n",
       "      <td>DET</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527778</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>256</td>\n",
       "      <td>16</td>\n",
       "      <td>Rückkehr</td>\n",
       "      <td>Rückkehr</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527779</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>257</td>\n",
       "      <td>16</td>\n",
       "      <td>zu</td>\n",
       "      <td>zu</td>\n",
       "      <td>ADP</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527780</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>258</td>\n",
       "      <td>16</td>\n",
       "      <td>alten</td>\n",
       "      <td>alt</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>O</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527781</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>259</td>\n",
       "      <td>16</td>\n",
       "      <td>Gewohnheiten</td>\n",
       "      <td>Gewohnheit</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>O</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527782</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>260</td>\n",
       "      <td>16</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527783</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>261</td>\n",
       "      <td>17</td>\n",
       "      <td>Das</td>\n",
       "      <td>das</td>\n",
       "      <td>PRON</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527784</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>262</td>\n",
       "      <td>17</td>\n",
       "      <td>legt</td>\n",
       "      <td>legen</td>\n",
       "      <td>VERB</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527785</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>263</td>\n",
       "      <td>17</td>\n",
       "      <td>ein</td>\n",
       "      <td>einen</td>\n",
       "      <td>DET</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527786</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>264</td>\n",
       "      <td>17</td>\n",
       "      <td>Blick</td>\n",
       "      <td>Blick</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527787</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>265</td>\n",
       "      <td>17</td>\n",
       "      <td>auf</td>\n",
       "      <td>auf</td>\n",
       "      <td>ADP</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527788</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>266</td>\n",
       "      <td>17</td>\n",
       "      <td>die</td>\n",
       "      <td>der</td>\n",
       "      <td>DET</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527789</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>267</td>\n",
       "      <td>17</td>\n",
       "      <td>Gewichtskurven</td>\n",
       "      <td>Gewichtskurve</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527790</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>268</td>\n",
       "      <td>17</td>\n",
       "      <td>nahe</td>\n",
       "      <td>nah</td>\n",
       "      <td>PART</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29527791</th>\n",
       "      <td>5088538977550609339</td>\n",
       "      <td>269</td>\n",
       "      <td>17</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29527792 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         hash  index  sent_idx                  text  \\\n",
       "0        -3019970016641425166      0         0                     „   \n",
       "1        -3019970016641425166      1         0           Balancierte   \n",
       "2        -3019970016641425166      2         0         Partnerschaft   \n",
       "3        -3019970016641425166      3         0                     “   \n",
       "4        -3019970016641425166      4         0                     :   \n",
       "5        -3019970016641425166      5         1                  Maas   \n",
       "6        -3019970016641425166      6         1              plädiert   \n",
       "7        -3019970016641425166      7         1                   für   \n",
       "8        -3019970016641425166      8         1                  neue   \n",
       "9        -3019970016641425166      9         1     Amerika-Strategie   \n",
       "10       -3019970016641425166     10         1                     .   \n",
       "11       -3019970016641425166     11         1                    \\n   \n",
       "12       -3019970016641425166     12         2                   Die   \n",
       "13       -3019970016641425166     13         2              Amtszeit   \n",
       "14       -3019970016641425166     14         2                   von   \n",
       "15       -3019970016641425166     15         2             Präsident   \n",
       "16       -3019970016641425166     16         2                Donald   \n",
       "17       -3019970016641425166     17         2                 Trump   \n",
       "18       -3019970016641425166     18         2                 könne   \n",
       "19       -3019970016641425166     19         2                   man   \n",
       "20       -3019970016641425166     20         2                 nicht   \n",
       "21       -3019970016641425166     21         2               einfach   \n",
       "22       -3019970016641425166     22         2             aussitzen   \n",
       "23       -3019970016641425166     23         2                     ,   \n",
       "24       -3019970016641425166     24         2               erklärt   \n",
       "25       -3019970016641425166     25         2         Außenminister   \n",
       "26       -3019970016641425166     26         2                  Maas   \n",
       "27       -3019970016641425166     27         2                     .   \n",
       "28       -3019970016641425166     28         3                   Die   \n",
       "29       -3019970016641425166     29         3                    EU   \n",
       "...                       ...    ...       ...                   ...   \n",
       "29527762  5088538977550609339    240        15       darauffolgenden   \n",
       "29527763  5088538977550609339    241        15                Wochen   \n",
       "29527764  5088538977550609339    242        15                  ließ   \n",
       "29527765  5088538977550609339    243        15                   der   \n",
       "29527766  5088538977550609339    244        15                Erfolg   \n",
       "29527767  5088538977550609339    245        15                   der   \n",
       "29527768  5088538977550609339    246        15  Kombinationstherapie   \n",
       "29527769  5088538977550609339    247        15            allerdings   \n",
       "29527770  5088538977550609339    248        15                wieder   \n",
       "29527771  5088538977550609339    249        15                  nach   \n",
       "29527772  5088538977550609339    250        15                     .   \n",
       "29527773  5088538977550609339    251        16               Ursache   \n",
       "29527774  5088538977550609339    252        16               hierfür   \n",
       "29527775  5088538977550609339    253        16                   war   \n",
       "29527776  5088538977550609339    254        16            vermutlich   \n",
       "29527777  5088538977550609339    255        16                   die   \n",
       "29527778  5088538977550609339    256        16              Rückkehr   \n",
       "29527779  5088538977550609339    257        16                    zu   \n",
       "29527780  5088538977550609339    258        16                 alten   \n",
       "29527781  5088538977550609339    259        16          Gewohnheiten   \n",
       "29527782  5088538977550609339    260        16                     .   \n",
       "29527783  5088538977550609339    261        17                   Das   \n",
       "29527784  5088538977550609339    262        17                  legt   \n",
       "29527785  5088538977550609339    263        17                   ein   \n",
       "29527786  5088538977550609339    264        17                 Blick   \n",
       "29527787  5088538977550609339    265        17                   auf   \n",
       "29527788  5088538977550609339    266        17                   die   \n",
       "29527789  5088538977550609339    267        17        Gewichtskurven   \n",
       "29527790  5088538977550609339    268        17                  nahe   \n",
       "29527791  5088538977550609339    269        17                     .   \n",
       "\n",
       "                         token    POS ent_iob  noun_phrase  \n",
       "0                            „  PUNCT       O            0  \n",
       "1                  balancieren    ADJ       O            0  \n",
       "2                Partnerschaft   NOUN       O            0  \n",
       "3                            “  PUNCT       O            0  \n",
       "4                            :  PUNCT       O            0  \n",
       "5                          maa    ADV       B            0  \n",
       "6                    plädieren   VERB       O            0  \n",
       "7                          für    ADP       O            0  \n",
       "8                          neu    ADJ       O            0  \n",
       "9            Amerika-strategie   NOUN       O            0  \n",
       "10                           .  PUNCT       O            0  \n",
       "11                              SPACE       B            0  \n",
       "12                         der    DET       O            0  \n",
       "13                    Amtszeit   NOUN       O            0  \n",
       "14                         von    ADP       O            0  \n",
       "15                   Präsident   NOUN       O            1  \n",
       "16                      Donald  PROPN       B            1  \n",
       "17                       Trump   NOUN       I            1  \n",
       "18                      können   VERB       O            0  \n",
       "19                         man   PRON       O            0  \n",
       "20                       nicht   PART       O            0  \n",
       "21                     einfach    ADJ       O            0  \n",
       "22                   aussitzen   VERB       O            0  \n",
       "23                           ,  PUNCT       O            0  \n",
       "24                    erklären   VERB       O            0  \n",
       "25               Außenminister   NOUN       O            2  \n",
       "26                         Maa  PROPN       B            2  \n",
       "27                           .  PUNCT       O            0  \n",
       "28                         der    DET       O            0  \n",
       "29                          EU  PROPN       B            0  \n",
       "...                        ...    ...     ...          ...  \n",
       "29527762         darauffolgend    ADJ       O           19  \n",
       "29527763                 Woche   NOUN       O           19  \n",
       "29527764                lassen   VERB       O            0  \n",
       "29527765                   der    DET       O            0  \n",
       "29527766                Erfolg   NOUN       O            0  \n",
       "29527767                   der    DET       O            0  \n",
       "29527768  Kombinationstherapie   NOUN       O            0  \n",
       "29527769            allerdings    ADV       O            0  \n",
       "29527770                wieder    ADV       O            0  \n",
       "29527771                  nach   PART       O            0  \n",
       "29527772                     .  PUNCT       O            0  \n",
       "29527773               Ursache   NOUN       O            0  \n",
       "29527774               hierfür    ADV       O            0  \n",
       "29527775                  sein    AUX       O            0  \n",
       "29527776            vermutlich    ADV       O            0  \n",
       "29527777                   der    DET       O            0  \n",
       "29527778              Rückkehr   NOUN       O            0  \n",
       "29527779                    zu    ADP       O            0  \n",
       "29527780                   alt    ADJ       O           20  \n",
       "29527781            Gewohnheit   NOUN       O           20  \n",
       "29527782                     .  PUNCT       O            0  \n",
       "29527783                   das   PRON       O            0  \n",
       "29527784                 legen   VERB       O            0  \n",
       "29527785                 einen    DET       O            0  \n",
       "29527786                 Blick   NOUN       O            0  \n",
       "29527787                   auf    ADP       O            0  \n",
       "29527788                   der    DET       O            0  \n",
       "29527789         Gewichtskurve   NOUN       O            0  \n",
       "29527790                   nah   PART       O            0  \n",
       "29527791                     .  PUNCT       O            0  \n",
       "\n",
       "[29527792 rows x 8 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = store(CORPUS, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ, PROCESS and *STORE*\n",
    "\n",
    "LOCAL_PATH = ETL_BASE\n",
    "FULL_PATH = join(DATA_BASE, LOCAL_PATH)\n",
    "\n",
    "files = sorted([f for f in listdir(FULL_PATH) if isfile(join(FULL_PATH, f)) \n",
    "                and f[:3] == 'dew'\n",
    "               ])\n",
    "\n",
    "for name in files:\n",
    "    fname = join(FULL_PATH, name)\n",
    "    df = read(fname)\n",
    "    df['doc_id'] = df['doc_id'].str.strip()\n",
    "    df['title'] = df['title'].str.strip()\n",
    "    print('saving to', fname)\n",
    "    #df.to_pickle(fname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines for certain corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- extract, transform, load (save) the following corpus:\n",
    "\n",
    "CORPUS = \"OnlineParticipation\"\n",
    "LOCAL_PATH = \"OnlineParticipationDatasets/downloads\"\n",
    "FULL_PATH = join(DATA_BASE, LOCAL_PATH)\n",
    "\n",
    "\n",
    "def transform_subset(source, subset_name: str):\n",
    "    \"\"\"\n",
    "    :param source: list of dictionaries in original key/value format\n",
    "    :param subset_name: string identifier of the subset the data belongs to\n",
    "    :return: list of dictionaries in standard key/value format\n",
    "    \"\"\"\n",
    "    category_lookup = {}\n",
    "    print('transform', subset_name)\n",
    "\n",
    "    for doc in source:\n",
    "        target = dict()\n",
    "        target[DATASET] = CORPUS\n",
    "        target[SUBSET] = subset_name\n",
    "        target[ID] = doc['suggestion_id']\n",
    "        target[TITLE] = doc['title']\n",
    "        target[TIME] = doc['date_time']\n",
    "\n",
    "        # wuppertal has a different data scheme\n",
    "        if subset_name == 'wuppertal2017':\n",
    "            if 'tags' in doc:\n",
    "                target[TAGS] = tuple(doc['tags'])\n",
    "                category_lookup[target[ID]] = target[TAGS]\n",
    "            else:\n",
    "                target[TAGS] = category_lookup[target[ID]]\n",
    "            target[ID2] = 0\n",
    "            target[TEXT] = doc['title'] + ' .\\n' \\\n",
    "                      + doc['content'] + ' .\\n' \\\n",
    "                      + doc['Voraussichtliche Rolle für die Stadt Wuppertal'] + ' .\\n' \\\n",
    "                      + doc['Mehrwert der Idee für Wuppertal'] + ' .\\n'\n",
    "                      # + doc['Eigene Rolle bei der Projektidee'] + ' .\\n'\n",
    "                      # + doc['Geschätzte Umsetzungsdauer und Startschuss'] + ' .\\n'\n",
    "                      # + doc['Kostenschätzung der Ideeneinreicher'] + ' .\\n'\n",
    "                    \n",
    "        else:\n",
    "            if 'category' in doc:\n",
    "                target[TAGS] = doc['category']\n",
    "                category_lookup[target[ID]] = target[TAGS]\n",
    "            else:\n",
    "                target[TAGS] = category_lookup[target[ID]]\n",
    "            target[ID2] = doc['comment_id'] if ('comment_id' in doc) else 0\n",
    "            # ignore if no content\n",
    "            if not doc['content']:\n",
    "                continue\n",
    "            target[TEXT] = doc['title'] + ' .\\n' + doc['content'] if doc['title'] else doc['content']\n",
    "        \n",
    "        target[HASH] = hash(tuple([target[key] for key in META]))\n",
    "        yield target\n",
    "\n",
    "\n",
    "def load_data(number_of_subsets: int=None, start: int=0):\n",
    "    \"\"\"\n",
    "    :param number_of_subsets: number of subsets to process in one call (None for no limit)\n",
    "    :param start: index of first subset to process\n",
    "    :yield: data set name, data subset name, data json\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"process\", CORPUS)\n",
    "    \n",
    "    # --- read files ---\n",
    "    files = [f for f in listdir(FULL_PATH) if isfile(join(FULL_PATH, f))]\n",
    "\n",
    "    if number_of_subsets:\n",
    "        number_of_subsets += start\n",
    "        if number_of_subsets > len(files):\n",
    "            number_of_subsets = None\n",
    "\n",
    "    for name in files[start:number_of_subsets]:\n",
    "        if name[-9:-5] != 'flat':\n",
    "            continue\n",
    "        \n",
    "        fpath = os.path.join(FULL_PATH, name)\n",
    "        try: \n",
    "            with open(fpath, 'r') as fp:\n",
    "                print('open:', fpath)\n",
    "                data = json.load(fp)\n",
    "                if not data:\n",
    "                    continue\n",
    "        except IOError:\n",
    "            print(\"Could not open\", fpath)\n",
    "            continue\n",
    "        subset = name[6:-10]\n",
    "\n",
    "        yield transform_subset(data, subset)\n",
    "\n",
    "\n",
    "dfs = [pd.DataFrame(item) for item in load_data()]\n",
    "if dfs:\n",
    "    df = pd.concat(dfs)\n",
    "    df = df.set_index(HASH)[META+[TEXT]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- extract, transform, load (save) the following corpus:\n",
    "\n",
    "CORPUS = \"FAZ\"\n",
    "LOCAL_PATH = \"scrapy/faz\"\n",
    "FULL_PATH = join(DATA_BASE, LOCAL_PATH)\n",
    "\n",
    "\n",
    "def transform_subset(source, subset_name: str):\n",
    "    \"\"\"\n",
    "    :param source: list of dictionaries in original key/value format\n",
    "    :param subset_name: string identifier of the subset the data belongs to\n",
    "    :return: list of dictionaries in standard key/value format\n",
    "    \"\"\"\n",
    "    print('transform', subset_name)\n",
    "\n",
    "    for doc in source:\n",
    "        target = dict()\n",
    "        target[DATASET] = CORPUS\n",
    "        target[SUBSET] = doc['url'].split('/')[4]\n",
    "        target[ID] = doc['url']\n",
    "        target[ID2] = 0\n",
    "        target[TITLE] = doc['title']\n",
    "        target[TAGS] = tuple(doc['keywords'])\n",
    "        if doc['published']:\n",
    "            target[TIME] = datetime.strptime(doc['published'], '%Y-%m-%dT%H:%M:%S%z')  # 2018-08-22T11:11:45+0200\n",
    "            # target[TIME] = datetime.fromisoformat(doc['published'])  # may work in Python 3.7\n",
    "        else: \n",
    "            target[TIME] = None\n",
    "        target[TEXT] = doc['title'] + ' .\\n' \\\n",
    "                     + doc['description'] + ' .\\n' \\\n",
    "                     + doc['text']\n",
    "\n",
    "        target[HASH] = hash(tuple([target[key] for key in META]))\n",
    "        yield target\n",
    "\n",
    "\n",
    "def load_data(number_of_subsets: int=None, start: int=0):\n",
    "    \"\"\"\n",
    "    :param number_of_subsets: number of subsets to process in one call (None for no limit)\n",
    "    :param start: index of first subset to process\n",
    "    :yield: data set name, data subset name, data json\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"process\", CORPUS)\n",
    "    \n",
    "    # --- read files ---\n",
    "    files = [f for f in listdir(FULL_PATH) if isfile(join(FULL_PATH, f))]\n",
    "    print(files)\n",
    "\n",
    "    if number_of_subsets:\n",
    "        number_of_subsets += start\n",
    "        if number_of_subsets > len(files):\n",
    "            number_of_subsets = None\n",
    "\n",
    "    for name in files[start:number_of_subsets]:\n",
    "        fpath =join(FULL_PATH, name)\n",
    "        try: \n",
    "            with open(fpath, 'r') as fp:\n",
    "                print('open:', fpath)\n",
    "                data = [json.loads(d) for d in fp.readlines()]\n",
    "                if not data:\n",
    "                    continue\n",
    "        except IOError:\n",
    "            print(\"Could not open\", fpath)\n",
    "            continue\n",
    "        subset = name[4:-3]\n",
    "\n",
    "        yield transform_subset(data, subset)\n",
    "\n",
    "\n",
    "dfs = [pd.DataFrame(item) for item in load_data()]\n",
    "if dfs:\n",
    "    df = pd.concat(dfs)\n",
    "    df = df.set_index(HASH)[META+[TEXT]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- extract, transform, load (save) the following corpus:\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "CORPUS = \"Europarl\"\n",
    "LOCAL_PATH = \"Europarl/Europarl/xml/de\"\n",
    "FULL_PATH = join(DATA_BASE, LOCAL_PATH)\n",
    "\n",
    "\n",
    "def transform_subset(source, subset_name: str):\n",
    "    \"\"\"\n",
    "    :param source: list of dictionaries in original key/value format\n",
    "    :param subset_name: string identifier of the subset the data belongs to\n",
    "    :return: list of dictionaries in standard key/value format\n",
    "    \"\"\"\n",
    "    print('transform', subset_name)\n",
    "    \n",
    "    soup = BeautifulSoup(source, 'xml')\n",
    "\n",
    "    for chapter in soup.find_all('CHAPTER'):\n",
    "        target = dict()\n",
    "        target[DATASET] = CORPUS\n",
    "        target[SUBSET] = subset_name\n",
    "        target[ID] = subset_name\n",
    "        target[ID2] = chapter.attrs['ID']\n",
    "        target[TITLE] = ' '.join([w.string for w in chapter.find('s').find_all('w')])\n",
    "        target[TAGS] = None\n",
    "        target[TIME] = datetime.strptime(subset_name[3:11], '%y-%m-%d')  # ep-07-01-18-009-07\n",
    "        \n",
    "        text = []\n",
    "        for paragraph in chapter.find_all([\"SPEAKER\", \"P\"]):\n",
    "            if paragraph.name == 'SPEAKER':\n",
    "                text.append(paragraph.attrs['NAME'])\n",
    "            elif paragraph.name == 'P':\n",
    "                text.append(' '.join([w.string for w in paragraph.find_all('w')]))\n",
    "        target[TEXT] = '\\n'.join(text)\n",
    "\n",
    "        target[HASH] = hash(tuple([target[key] for key in META]))\n",
    "        yield target\n",
    "\n",
    "\n",
    "def load_data(number_of_subsets: int=None, start: int=0):\n",
    "    \"\"\"\n",
    "    :param number_of_subsets: number of subsets to process in one call (None for no limit)\n",
    "    :param start: index of first subset to process\n",
    "    :yield: data set name, data subset name, data json\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"process\", CORPUS)\n",
    "    \n",
    "    # --- read files ---\n",
    "    files = [f for f in listdir(FULL_PATH) if isfile(join(FULL_PATH, f))]\n",
    "\n",
    "    if number_of_subsets:\n",
    "        number_of_subsets += start\n",
    "        if number_of_subsets > len(files):\n",
    "            number_of_subsets = None\n",
    "\n",
    "    for name in files[start:number_of_subsets]:\n",
    "        fpath = join(FULL_PATH, name)\n",
    "        try:\n",
    "            with gzip.open(fpath, 'rb') as fp:\n",
    "                data = fp.read()\n",
    "        except IOError:\n",
    "            print(\"Could not open\", fpath)\n",
    "            continue\n",
    "        subset = name[:-7]\n",
    "\n",
    "        yield transform_subset(data, subset)\n",
    "\n",
    "\n",
    "dfs = [pd.DataFrame(item) for item in load_data(number_of_subsets=None)]\n",
    "if dfs:\n",
    "    df = pd.concat(dfs)\n",
    "    df = df.set_index(HASH)[META+[TEXT]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- extract, transform, load (save) the following corpus:\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "CORPUS = \"PoliticalSpeeches\"\n",
    "LOCAL_PATH = \"German-political-speeches-2018-release\"\n",
    "FULL_PATH = join(DATA_BASE, LOCAL_PATH)\n",
    "\n",
    "\n",
    "def transform_subset(source, subset_name: str):\n",
    "    \"\"\"\n",
    "    :param source: list of dictionaries in original key/value format\n",
    "    :param subset_name: string identifier of the subset the data belongs to\n",
    "    :return: list of dictionaries in standard key/value format\n",
    "    \"\"\"\n",
    "    print('transform', subset_name)\n",
    "    \n",
    "    soup = BeautifulSoup(source, 'xml')\n",
    "\n",
    "    months = dict(Januar='01', Februar='02', März='03', April='04', Mai='05', Juni='06',\n",
    "                  Juli='07', August='08', September='09', Oktober='10', November='11', Dezember='12')\n",
    "    pattern = re.compile(r'(' + '|'.join(months.keys()) + r')')\n",
    "\n",
    "    for speech in soup.find_all('text'):\n",
    "        target = dict()\n",
    "        target[DATASET] = CORPUS\n",
    "        target[SUBSET] = subset_name\n",
    "        target[ID] = speech.attrs['url']\n",
    "        target[ID2] = 0\n",
    "        target[TITLE] = speech.attrs['titel']\n",
    "        target[TAGS] = speech.attrs['person']\n",
    "        if speech.attrs['datum']:\n",
    "            match = pattern.search(speech.attrs['datum'])\n",
    "            if match:\n",
    "                datum = speech.attrs['datum'].replace(\" \", \"\")\n",
    "                time = pattern.sub(lambda key: months[key.group()] + '.', datum)\n",
    "                target[TIME] = datetime.strptime(time, '%d.%m.%Y')\n",
    "            else:\n",
    "                target[TIME] = datetime.strptime(speech.attrs['datum'], '%d.%m.%Y')\n",
    "        else:\n",
    "            target[TIME] = None\n",
    "\n",
    "        target[TEXT] = speech.attrs['titel'] + ' .\\n' \\\n",
    "                     + speech.attrs['untertitel'] + ' .\\n' if 'untertitel' in speech.attrs else ''\\\n",
    "                     + speech.find('rohtext').string\n",
    "\n",
    "        target[HASH] = hash(tuple([target[key] for key in META]))\n",
    "        yield target\n",
    "\n",
    "\n",
    "def load_data(number_of_subsets: int=None, start: int=0):\n",
    "    \"\"\"\n",
    "    :param number_of_subsets: number of subsets to process in one call (None for no limit)\n",
    "    :param start: index of first subset to process\n",
    "    :yield: data set name, data subset name, data json\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"process\", CORPUS)\n",
    "    \n",
    "    # --- read files ---\n",
    "    files = [f for f in listdir(FULL_PATH) if isfile(join(FULL_PATH, f))]\n",
    "\n",
    "    if number_of_subsets:\n",
    "        number_of_subsets += start\n",
    "        if number_of_subsets > len(files):\n",
    "            number_of_subsets = None\n",
    "\n",
    "    for name in files[start:number_of_subsets]:\n",
    "        if name[-3:] != 'xml':\n",
    "            continue\n",
    "        \n",
    "        fpath = join(FULL_PATH, name)\n",
    "        try:\n",
    "            with open(fpath, 'r') as fp:\n",
    "                data = fp.read()\n",
    "        except IOError:\n",
    "            print(\"Could not open\", fpath)\n",
    "            continue\n",
    "        subset = name[:-4]\n",
    "\n",
    "        yield transform_subset(data, subset)\n",
    "\n",
    "\n",
    "dfs = [pd.DataFrame(item) for item in load_data(number_of_subsets=None)]\n",
    "if dfs:\n",
    "    df = pd.concat(dfs)\n",
    "    df = df.set_index(HASH)[META+[TEXT]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- extract, transform, load (save) the following corpus:\n",
    "\n",
    "CORPUS = \"dewac\"\n",
    "LOCAL_PATH = \"WaCKy/dewac\"\n",
    "FULL_PATH = join(DATA_BASE, LOCAL_PATH)\n",
    "mn = 12\n",
    "\n",
    "\n",
    "def transform_subset(source, subset_name: str):\n",
    "    \"\"\"\n",
    "    :param source: list of dictionaries in original key/value format\n",
    "    :param subset_name: string identifier of the subset the data belongs to\n",
    "    :return: list of dictionaries in standard key/value format\n",
    "    \"\"\"\n",
    "    # print('transform', source['url'][mn:])\n",
    "    \n",
    "    assert source['url'][:mn] == \"CURRENT URL \"\n",
    "    target = dict()\n",
    "    target[DATASET] = CORPUS\n",
    "    target[SUBSET] = subset_name\n",
    "    target[ID] = source['url'][mn:]\n",
    "    target[ID2] = None\n",
    "    target[TITLE] = target[ID].split('/')[-1]\n",
    "    target[TAGS] = None\n",
    "    target[TIME] = None\n",
    "    target[TEXT] = source['text']\n",
    "    target[HASH] = hash(tuple([target[key] for key in META]))\n",
    "    yield target\n",
    "\n",
    "\n",
    "def load_data(number_of_documents: int=None, start: int=0):\n",
    "    \"\"\"\n",
    "    :param number_of_subsets: number of subsets to process in one call (None for no limit)\n",
    "    :param start: index of first subset to process\n",
    "    :yield: data set name, data subset name, data json\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"process\", CORPUS)\n",
    "    \n",
    "    # --- read files ---\n",
    "    name = 'dewac_preproc'\n",
    "    fpath = join(FULL_PATH, name)\n",
    "    print(fpath)\n",
    "    try:\n",
    "        with open(fpath, 'r', encoding='latin-1') as fp:\n",
    "            i = 0\n",
    "            # not the most efficient with respect to CPU-cycles and I/O operations \n",
    "            # but quite feasible in regards of memory consumption\n",
    "            while True:\n",
    "                data = dict()\n",
    "                if number_of_documents and i >= start+number_of_documents:\n",
    "                    break\n",
    "                data['url'] = fp.readline().strip()\n",
    "                if not data['url']:\n",
    "                    break \n",
    "                data['text'] = fp.readline().strip()\n",
    "                i += 1\n",
    "                if i < start:\n",
    "                    # could be more efficient if we don't close the file pointer\n",
    "                    continue\n",
    "                if data['url'][:mn] != \"CURRENT URL \":\n",
    "                    fp.readline()\n",
    "                    continue\n",
    "                # print(i, ':')\n",
    "                yield transform_subset(data, '')\n",
    "    except IOError:\n",
    "        print(\"Could not open\", fpath)\n",
    "\n",
    "\n",
    "for i in range(0, 20):\n",
    "    dfs = [pd.DataFrame(item) for item in load_data(number_of_documents=100000, start=i*100000)]\n",
    "    if dfs:\n",
    "        df = pd.concat(dfs)\n",
    "        df = df.set_index(HASH)[META+[TEXT]]\n",
    "        store((\"%s_%02d\" % CORPUS, i), df)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print('done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
